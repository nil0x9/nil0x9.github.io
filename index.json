[{"content":"在深度学习中最常用的优化方法是梯度下降方法及其变体。在过去很长一段时间中，Adam优化器是NLP社区的默认选择，在ViT出现之后，CV方面的工作也逐渐开始使用Adam和Adam的变体（在ViT之前，一种常见的观点是Adam不适用于Vision任务）。\n最近Muon优化器在Kimi的新工作带动下又火了一把，相较于Adam优化器需要同时维护一阶、二阶矩估计量，Muon只需要维护一份梯度的动量估计，因此在大规模训练中有很大优势。最近笔者顺着Muon的reference看了Jeremy Bernstein在优化的一些文章，觉得很有意思，因此写这篇文章梳理一下这一系列工作的部分精要。本文的核心论点是：使用诱导范数来约束梯度更新，可以推导出最近的一些新出现的优化方法，这也可能是未来深度学习优化的一个有潜力的探索方向。\n梯度下降（Recap） 当前深度学习优化算法的基石是梯度下降。之前笔者写过一篇拙文（自然梯度（二）：黎曼距离下的最速下降）整理过梯度下降的推导，核心的结论是：当我们假设参数空间是一个欧几里得空间、参数的距离可以用欧几里得距离来衡量时，我们在某个点约束$\\|\\Delta\\theta\\|_2\\le\\epsilon(\\epsilon\u0026gt;0)$时，$\\Delta\\theta$取梯度的反方向时可以让目标函数下降最多（具体的证明请参阅上述引文）。\n使用梯度下降最大的问题是，它实际上忽略了模型的结构。换句话说，梯度下降相当于将模型所有参数展平为1维向量，并且用向量2范数来衡量每次更新的「步长」。这种抽象是实用的，但是也存在一定的问题。两组参数有可能在欧几里得空间中距离很近，但是诱导的模型输出空间距离很远。造成的结果就是更新的方向实际上不是目标函数下降最快的方向。\n这个问题要如何解决呢？在自然梯度（二）：黎曼距离下的最速下降中，我们介绍了自然梯度方法，即使用Fisher信息矩阵的逆作为梯度的pre-conditioner来矫正梯度下降的方向，从原理上是使用参数更新前后引导的概率分布的KL散度作为每次更新的步长约束。但是对于常见的深度神经网络来说，这样做仍然是不切实际的，因为FIM是一个$N\\times N$的大矩阵（其中$N$是参数量），对于这么大的矩阵存储或求逆都是很难做到的。\n诱导范数作为步长约束 是否有一种更「廉价」的方法，可以考虑模型的参数结构，同时将参数的变化对于输出的影响作为约束呢？\n幸运的是，对于当下最流行的神经网络（e.g., Transformer）而言，模型往往可以拆解为很多小模块，其中最常见的是Linear模块（线性映射，这里忽略bias term）\n$$ f(\\boldsymbol{x};\\boldsymbol{W})=\\boldsymbol{Wx},\\ \\boldsymbol{W}\\in\\mathbb{R}^{n\\times m},\\boldsymbol{x}\\in \\mathbb{R}^{m}\\\\ $$\n在标准的Transformer中，Attention、FFN、LM分类器都是由Linear模块组成的，Embedding从数学原理上也是输入为one-hot encoding的线性映射。假设现在对于某个Linear模块的参数$\\boldsymbol{W}$做$\\Delta\\boldsymbol{W}$的更新（$\\boldsymbol{W}'\\leftarrow \\boldsymbol{W}+\\Delta\\boldsymbol{W}$），我们需要衡量这个更新对于最终输出的影响是多少（从而可以约束这个影响）。由于神经网络比较复杂，衡量$\\Delta\\boldsymbol{W}$对于最终目标函数的影响是相对繁琐的，但我们可以退而求其次，衡量$\\Delta\\boldsymbol{W}$对于这个Linear模块的输出$\\boldsymbol{Wx}$的影响。 考虑线性模块的输入与输出空间的距离都使用欧几里得范数$\\|\\cdot\\|_{\\ell_2}$衡量，那么这个约束可以通过如下不等式实现\n$$ \\|\\Delta\\boldsymbol{W}\\boldsymbol{x}\\|_{\\ell_2} \\le {\\color[rgb]{0, 0.5, 0.8}{\\|\\Delta\\boldsymbol{W}\\|_{\\ell_2\\to\\ell_2}}}\\|\\boldsymbol{x}\\|_{\\ell_2} $$ 这里的${\\color[rgb]{0, 0.5, 0.8}{\\|\\Delta\\boldsymbol{W}\\|_{\\ell_2\\to\\ell_2}}}$是矩阵2范数。这个不等式告诉我们，如果约束了参数更新量的谱范数（不等式右侧），也就约束了更新前后这个线性模块输出的变化量。\n假设现在需要优化的神经网络是由一系列的线性模块堆叠组成（e.g., MLP），我们可以参照梯度下降的推导构造如下的更新1\n$$ \\underset{\\Delta\\boldsymbol{W}_1,\\ldots,\\Delta\\boldsymbol{W}_L}{\\text{arg min}}\\left[ \\sum_{l=1}^L {\\langle \\boldsymbol{G}_l, \\Delta\\boldsymbol{W}_l \\rangle}_F + \\frac{\\lambda}{2}\\max_{l=1}^L{\\color[rgb]{0, 0.5, 0.8}{\\|\\Delta\\boldsymbol{W}_l\\|^2_{\\ell_2\\to\\ell_2}}} \\right]\\\\ $$\n这里$\\boldsymbol{G}_l$表示$\\boldsymbol{W}_l$对应的梯度矩阵（布局与原参数矩阵相同），${\\langle \\cdot, \\cdot \\rangle}_F$表示Frobenius内积（对矩阵而言，逐元素相乘求和）。这里之所以使用$\\max_{l=1}^L$（而不是直接求和），是因为我们引入这个约束时希望目标函数在$\\Delta\\boldsymbol{W}_l$变化下，能够保持平滑的性质2，因此需要bound所有参数矩阵更新量的谱范数的最大值。\n我们来逐步推导这个最小值成立时的$\\Delta\\boldsymbol{W}_1,\\ldots,\\Delta\\boldsymbol{W}_L$取值3。为了方便，把每个$\\Delta\\boldsymbol{W}_l$拆解成大小和方向两部分：$\\Delta\\boldsymbol{W}_l=c_l\\boldsymbol{T}_l(c_l\\triangleq\\|\\Delta\\boldsymbol{W}_l\\|_{\\ell_2\\to\\ell_2})$\n（为了可读性，下面的$\\|\\cdots\\|$均表示谱范数$\\|\\cdot\\|_{\\ell_2\\to\\ell_2}$）\n$$ \\begin{align} \u0026amp;\\underset{\\Delta\\boldsymbol{W}_1,\\ldots,\\Delta\\boldsymbol{W}_L}{\\text{min}}\\left[ \\sum_{l=1}^L {\\langle \\boldsymbol{G}_l, \\Delta\\boldsymbol{W}_l \\rangle}_F + \\frac{\\lambda}{2}\\max_{l=1}^L\\|\\Delta\\boldsymbol{W}_l\\|^2 \\right]\\\\ \u0026amp;=\\underset{c_1,\\ldots,c_L\\ge 0}{\\text{min}}\\left[ \\sum_{l=1}^L c_l\\min_{\\|\\boldsymbol{T}_l\\|=1}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F + \\frac{\\lambda}{2}\\max_{l=1}^Lc_l^2 \\right]\\\\ \u0026amp;=\\underset{c_1,\\ldots,c_L\\ge 0}{\\text{min}}\\left[ -\\sum_{l=1}^L c_l\\max_{\\|\\boldsymbol{T}_l\\|=1}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F + \\frac{\\lambda}{2}\\max_{l=1}^Lc_l^2 \\right]\\\\ \u0026amp;=\\underset{c_1,\\ldots,c_L\\ge 0}{\\text{min}}\\left[ -\\sum_{l=1}^L c_l \\|\\boldsymbol{G}_l\\|_* + \\frac{\\lambda}{2}\\max_{l=1}^Lc_l^2 \\right]\\quad\\triangleright\\|\\cdot\\|_*\\text{表示核范数}\\\\ \u0026amp;=\\underset{\\eta\\ge 0}{\\text{min}}\\left[ -\\sum_{l=1}^L \\eta\\|\\boldsymbol{G}_l\\|_* + \\frac{\\lambda}{2}\\max_{l=1}^L \\eta^2 \\right]\\tag{1}\\\\ \\end{align} $$\n上面的每个等号成立的解释如下：\n第一个等号成立，是直接带入$c_l\\triangleq\\|\\Delta\\boldsymbol{W}_l\\|_{\\ell_2\\to\\ell_2}$的结果，注意到$\\min$的条件被拆成了大小和方向两部分。\n第二个等号成立，是因为Frobenius内积具有线性性质， $$ \\begin{align} \\min_{\\|\\boldsymbol{T}_l\\|=1}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F\u0026amp;=-\\max_{\\|\\boldsymbol{T}_l\\|=1}{-\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F\\\\ \u0026amp;=-\\max_{\\|\\boldsymbol{T}_l\\|=1}{\\langle \\boldsymbol{G}_l, -\\boldsymbol{T}_l \\rangle}_F\\\\ \u0026amp;=-\\max_{\\|\\boldsymbol{T}'_l\\|=1}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}'_l \\rangle}_F\\\\ \\end{align} $$\n第三个等号成立，是因为Frobenius内积${\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F$最大值是矩阵$\\boldsymbol{G}_l$的核范数（nuclear norm，即奇异值之和）(求解过程放在文末)。\n第四个等号成立，即所有系数$c_l$都取相同值$\\eta$：假设我们当前有某组$c_1,\\ldots,c_L$取到目标最小值，且满足$\\max_i c_i=\\eta$，此时如果存在$c_j\u0026lt;\\eta$，则只需要增加$c_j$的值，即可在第二项$\\lambda/2\\max_{l=1}^Lc_l^2$不变的情况下，进一步减小第一项$-\\sum_{l=1}^L c_l \\|\\boldsymbol{G}_l\\|_*$的值（注意到核范数是非负的）。因此这里令目标函数取最小的必要条件是所有$c_1,\\ldots,c_L$均取最大值$\\eta$。\n式$(1)$看起来可能有一些复杂，但是本质上关于$\\eta$是一个简单的二次函数，用初等数学4即可求出它的最优值对应的$\\eta$： $$ \\eta = \\frac{1}{\\lambda}\\sum_l^L \\|\\boldsymbol{G}_l\\|_* $$\n现在我们已经求出了$\\Delta \\boldsymbol{W}_l$的最优范数$c^*_l=\\eta$了，还需要知道它的方向$\\boldsymbol{T}^*_l$\n$$ \\boldsymbol{T}^*_l=\\underset{\\boldsymbol{T}_l:\\|\\boldsymbol{T}_l\\|=1}{\\text{arg min}}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F=-\\underset{\\boldsymbol{T}_l:\\|\\boldsymbol{T}_l\\|=1}{\\text{arg max}}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F $$\n这里直接给出$\\boldsymbol{T}_l$的解，求解的过程放在文末。假设$\\boldsymbol{G}_l\\in\\mathbb{R}^{m\\times n}$的最简SVD分解为$\\boldsymbol{U}_{m\\times r}\\boldsymbol{\\Sigma}_{r\\times r}\\boldsymbol{V}_{r\\times n}^\\top, r=\\text{rank}(\\boldsymbol{G}_l)$，则\n$$ \\boldsymbol{T}^*_l=-\\boldsymbol{U}\\boldsymbol{V}^\\top $$\n也就是说，上述带谱范数约束下的最速下降的解是 $$ \\begin{align} \\Delta \\boldsymbol{W}_l\u0026amp;=-\\eta\\ {\\color[rgb]{0, 0.5, 0.8}{\\boldsymbol{U}\\boldsymbol{V}^\\top}}\\\\ \\end{align} $$\n到这一步，如果读者熟悉Muon优化器的话，会发现$-{\\color[rgb]{0, 0.5, 0.8}{\\boldsymbol{U}\\boldsymbol{V}^\\top}}$正是Muon优化器的更新方向（如果忽略动量估计的话）。另外，对于Shampoo优化器，如果忽略掉左右conditioner的累加的话，也可以得出同样的更新方向。 $$ \\begin{align} \u0026amp;-(\\boldsymbol{G}\\boldsymbol{G}^\\top)^{-\\frac{1}{4}}\\boldsymbol{G}(\\boldsymbol{G}^\\top\\boldsymbol{G})^{-\\frac{1}{4}}\\\\ =\u0026amp;- (\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^\\top\\boldsymbol{V}\\boldsymbol{\\Sigma}\\boldsymbol{U}^\\top)^{-\\frac{1}{4}}\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^\\top(\\boldsymbol{V}\\boldsymbol{\\Sigma}\\boldsymbol{U}^\\top\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^\\top)^{-\\frac{1}{4}}\\\\ =\u0026amp;-(\\boldsymbol{U}\\boldsymbol{\\Sigma}^2\\boldsymbol{U}^\\top)^{-\\frac{1}{4}}\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^\\top(\\boldsymbol{V}\\boldsymbol{\\Sigma}^2\\boldsymbol{V}^\\top)^{-\\frac{1}{4}}\\\\ =\u0026amp;-\\boldsymbol{U}\\boldsymbol{\\Sigma}^{-\\frac{1}{2}}\\boldsymbol{\\Sigma}\\boldsymbol{\\Sigma}^{-\\frac{1}{2}}\\boldsymbol{V}^\\top=-\\boldsymbol{U}\\boldsymbol{V}^\\top\\\\ \\end{align} $$\n总结上面的这一小节内容：Muon、Shampoo优化器本质上是约束每个线性模块的算子诱导范数（在上面的例子中，是谱范数$\\|\\cdot\\|_{\\ell_2\\to\\ell_2}$）衡量的参数更新量，从而推导出使用$\\boldsymbol{U}\\boldsymbol{V}^\\top$替代梯度矩阵$\\boldsymbol{G}$作为更新方向。这种约束的优势是，考虑了每次参数更新对于线性模块的输出的影响，更新的方向相比于标准梯度下降（使用欧氏距离衡量参数更新量）会更稳定。\n在矩阵约束中考虑语义信息 故事到这里还没有结束。在神经网络中，同样是线性模块，根据输入输出语义的不同，表达的含义是不一样的。因此我们可以使用更泛化的诱导范数来约束每个模块参数的更新。\n考虑一个矩阵$\\boldsymbol{W}\\in\\mathbb{R}^{m\\times n}$，它链接着赋范空间$(\\mathbb{R}^n, \\|\\cdot\\|_\\alpha)$和$(\\mathbb{R}^m, \\|\\cdot\\|_\\beta)$，矩阵$\\boldsymbol{W}$的算子诱导范数定义如下 $$ \\|W\\|_{\\alpha\\to\\beta}=\\underset{\\boldsymbol{x}\\in\\mathbb{R}^n}{\\max} \\frac{\\|W\\boldsymbol{x}\\|_\\beta}{\\|\\boldsymbol{x}\\|_\\alpha} $$\n在前面的例子中，我们假设了输入与输出都是使用$\\ell_2$范数来衡量的，得到的最优更新方向是$\\boldsymbol{U}\\boldsymbol{V}^\\top$。现在，我们考虑另一个例子——Embedding矩阵，它的输入是one-hot向量(仅token对应的位置是1，其他位置都是0的向量)，此时我们可以使用$\\ell_1\\to\\text{RMS}$的诱导范数（输入空间是用$\\ell_1$范数，输出空间用RMS范数） $$ \\|W\\|_{\\ell_1\\to\\text{RMS}}=\\underset{\\boldsymbol{x}\\in\\mathbb{R}^n}{\\max} \\frac{\\|W\\boldsymbol{x}\\|_\\text{RMS}}{\\|\\boldsymbol{x}\\|_{\\ell_1}} $$ 考虑到$\\boldsymbol{x}$总是one-hot向量，则上述诱导范数退化为Embedding RMS Norm的最大值： $$ \\|W\\|_{\\ell_1\\to\\text{RMS}}=\\underset{1\\le j\\le n}{\\max} \\|\\boldsymbol{W}_{:,j}\\|_\\text{RMS} $$ 参照上一小节的推导，假设这个Embedding矩阵的梯度是$\\boldsymbol{G}$，则对应的最速更新方向由如下给出： $$ -\\underset{\\boldsymbol{T}}{\\text{arg max}}{\\langle \\boldsymbol{G}, \\boldsymbol{T} \\rangle}_F \\quad\\text{s.t. }\\underset{1\\le j\\le n}{\\max} \\|\\boldsymbol{T}_{:,j}\\|_\\text{RMS}=1 $$\n上述问题的解由如下构造（即对梯度的每列做RMS标准化）： $$ \\boldsymbol{T}_{:,j}=\\boldsymbol{G}_{:,j}/\\|\\boldsymbol{G}_{:,j}\\|_\\text{RMS} $$\n在Bernstein \u0026amp; Newhouse 2024.中，作者还针对Conv2D算子给出了更新方向（在Bernstein的框架中，这个更新方向就是给定范数下的对偶映射（duality map））。最近Pethick 2025.把这个框架进一步拓展，给出了$\\ell_1\\to\\infty$诱导范数下的最优更新方向$\\text{sign}(\\boldsymbol{G})$，他们将这个更新规则用在了分类模型的输出分类器上（最终他们的优化器实现类似一个Muon与SignSGD的混合）。总而言之，根据参数矩阵的输入、输出的语义不同，应当选取不同的诱导范数约束，对应的更新规则也不同。\n结语 本篇文章分享了笔者阅读Bernstein一系列文章之后的一些重点摘录。对于深度学习中常见的矩阵参数，可以考虑根据输入与输出的不同语义，对梯度矩阵赋予不同的诱导范数约束，从而得到不同的更新规则。\n这一系列的工作还有很多值得挖掘的地方，例如最优的学习率如何分配（对应到上述优化问题中，我们可以为每个诱导范数的约束项增加一个加权系数）？在Large 2024.中作者提到适当地设计模块之间的约束加权，则不同模型宽度（隐藏层维度）下的最优学习率是一致的（learning rate transfer）。如果这点在更多模型上成立的话，那么大规模神经网络的训练就可以有更加第一性的调参路径了。对于这部分相关工作，笔者还没有做深入的阅读，今后有机会可以再写文章分享。\n附录 求解$\\underset{\\boldsymbol{T}_l:\\|\\boldsymbol{T}_l\\|=1}{\\text{max}}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F$和$\\underset{\\boldsymbol{T}_l:\\|\\boldsymbol{T}_l\\|=1}{\\text{arg max}}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F$（下面的矩阵范数均为谱范数$\\|\\cdot\\|_{\\ell_2\\to\\ell_2}$）：\n假设$\\boldsymbol{G}_l$的SVD分解为$\\boldsymbol{U\\Sigma V}^\\top=\\sum_i \\sigma_i\\boldsymbol{u}_i\\boldsymbol{v}_i^\\top$，则 $$ \\begin{align} {\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F \u0026amp;= \\text{Tr}\\left(\\boldsymbol{G}_l^\\top \\boldsymbol{T}_l\\right)\\\\ \u0026amp;=\\text{Tr}\\left(\\left(\\sum_i \\sigma_i\\boldsymbol{u}_i\\boldsymbol{v}_i^\\top\\right)^\\top \\boldsymbol{T}_l\\right)\\\\ \u0026amp;=\\text{Tr}\\left(\\sum_i \\sigma_i\\boldsymbol{v}_i\\boldsymbol{u}_i^\\top \\boldsymbol{T}_l\\right)\\\\ \u0026amp;=\\text{Tr}\\left(\\sum_i \\sigma_i\\boldsymbol{u}_i^\\top \\boldsymbol{T}_l\\boldsymbol{v}_i\\right)\\\\ \u0026amp;=\\sum_i \\sigma_i\\boldsymbol{u}_i^\\top \\boldsymbol{T}_l\\boldsymbol{v}_i\\quad\\triangleright\\text{括号内是标量}\\\\ \u0026amp;\\le\\sum_i \\sigma_i\\\\ \\end{align} $$\n最后一个不等式成立是由于Cauchy-Schwarz不等式（$\\{\\boldsymbol{u}_i\\},\\{\\boldsymbol{u}_i\\}$是正交规范向量组，范数是1）： $$ |\\boldsymbol{u}_i^\\top \\boldsymbol{T}_l\\boldsymbol{v}_i|\\le \\|\\boldsymbol{u}_i\\| \\|\\boldsymbol{T}_l\\boldsymbol{v}_i\\| \\le \\|\\boldsymbol{u}_i\\| \\|\\boldsymbol{T}_l\\|\\|\\boldsymbol{v}_i\\|=1 $$\n注意到，如果带入$\\boldsymbol{T}_l=\\boldsymbol{U}\\boldsymbol{V}^\\top=\\sum_j \\boldsymbol{u}_j\\boldsymbol{v}_j^\\top$（可以验证此时$\\|\\boldsymbol{T}_l\\|=1$）\n$$ \\begin{align} \\sum_i \\sigma_i\\boldsymbol{u}_i^\\top \\boldsymbol{T}_l\\boldsymbol{v}_i \u0026amp;=\\sum_i \\sigma_i\\boldsymbol{u}_i^\\top (\\sum_j \\boldsymbol{u}_j\\boldsymbol{v}_j^\\top)\\boldsymbol{v}_i\\\\ \u0026amp;=\\sum_i \\sigma_i \\sum_j \\boldsymbol{u}_i^\\top\\boldsymbol{u}_j\\boldsymbol{v}_j^\\top \\boldsymbol{v}_i\\\\ \u0026amp;=\\sum_i \\sigma_i \\end{align} $$ 因此$\\sum_i\\sigma_i$这个上界是可取到的，则 $$ \\begin{split} \\underset{\\boldsymbol{T}_l:\\|\\boldsymbol{T}_l\\|=1}{\\text{max}}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F=\\sum_i\\sigma_i\\|\\boldsymbol{G}_l\\|_*\\\\ \\boldsymbol{U}\\boldsymbol{V}^\\top\\in \\underset{\\boldsymbol{T}_l:\\|\\boldsymbol{T}_l\\|=1}{\\text{arg max}}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F\\\\ \\end{split} $$\n这里如果$\\boldsymbol{G}_l$是满秩的，则argmax问题的解是唯一的（可以用反证法来证明），如果非满秩，则$\\boldsymbol{U}\\boldsymbol{V}^\\top$不是唯一解（不影响主要结论）。\n参考阅读 Jordan 2024. Muon: An optimizer for hidden layers in neural networks Gupta 2018. Shampoo: Preconditioned Stochastic Tensor Optimization Bernstein \u0026amp; Newhouse 2024. Old Optimizer, New Norm: An Anthology Large 2024. Scalable Optimization in the Modular Norm Bernstein \u0026amp; Newhouse 2024. Modular Duality in Deep Learning Pethick 2025. Training Deep Learning Models with Norm-Constrained LMOs 这里直接按照Lagrange乘子展开。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n考虑极端情况，如果直接求和作为约束，那么得到的解可能会让某个参数矩阵变化极大，其他矩阵保持不变，那么目标函数的变化可能是非常剧烈的。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n参考了Bernstein \u0026amp; Newhouse 2024. Old Optimizer, New Norm: An Anthology\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n$ax^2+bx(a\u0026gt;0)$的最小值在$b/2a$取到。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://nil9.net/posts/gd-norm-const/","summary":"\u003cp\u003e在深度学习中最常用的优化方法是梯度下降方法及其变体。在过去很长一段时间中，Adam优化器是NLP社区的默认选择，在ViT出现之后，CV方面的工作也逐渐开始使用Adam和Adam的变体（在ViT之前，一种常见的观点是Adam不适用于Vision任务）。\u003c/p\u003e\n\u003cp\u003e最近Muon优化器在Kimi的新工作带动下又火了一把，相较于Adam优化器需要同时维护一阶、二阶矩估计量，Muon只需要维护一份梯度的动量估计，因此在大规模训练中有很大优势。最近笔者顺着Muon的reference看了Jeremy Bernstein在优化的一些文章，觉得很有意思，因此写这篇文章梳理一下这一系列工作的部分精要。本文的核心论点是：使用诱导范数来约束梯度更新，可以推导出最近的一些新出现的优化方法，这也可能是未来深度学习优化的一个有潜力的探索方向。\u003c/p\u003e\n\u003ch1 id=\"梯度下降recap\"\u003e梯度下降（Recap）\u003c/h1\u003e\n\u003cp\u003e当前深度学习优化算法的基石是梯度下降。之前笔者写过一篇拙文（\u003ca href=\"https://nil9.net/posts/natural-gradient-descent/\"\u003e自然梯度（二）：黎曼距离下的最速下降\u003c/a\u003e）整理过梯度下降的推导，核心的结论是：当我们假设参数空间是一个欧几里得空间、参数的距离可以用欧几里得距离来衡量时，我们在某个点约束\u003ccode\u003e$\\|\\Delta\\theta\\|_2\\le\\epsilon(\\epsilon\u0026gt;0)$\u003c/code\u003e时，\u003ccode\u003e$\\Delta\\theta$\u003c/code\u003e取梯度的反方向时可以让目标函数下降最多（具体的证明请参阅上述引文）。\u003c/p\u003e\n\u003cp\u003e使用梯度下降最大的问题是，它实际上\u003cstrong\u003e忽略了模型的结构\u003c/strong\u003e。换句话说，梯度下降相当于将模型所有参数展平为1维向量，并且用向量2范数来衡量每次更新的「步长」。这种抽象是实用的，但是也存在一定的问题。两组参数有可能在欧几里得空间中距离很近，但是诱导的模型输出空间距离很远。造成的结果就是更新的方向实际上不是目标函数下降最快的方向。\u003c/p\u003e\n\u003cp\u003e这个问题要如何解决呢？在\u003ca href=\"https://nil9.net/posts/natural-gradient-descent/\"\u003e自然梯度（二）：黎曼距离下的最速下降\u003c/a\u003e中，我们介绍了自然梯度方法，即使用Fisher信息矩阵的逆作为梯度的pre-conditioner来矫正梯度下降的方向，从原理上是使用参数更新前后引导的概率分布的KL散度作为每次更新的步长约束。但是对于常见的深度神经网络来说，这样做仍然是不切实际的，因为FIM是一个\u003ccode\u003e$N\\times N$\u003c/code\u003e的大矩阵（其中\u003ccode\u003e$N$\u003c/code\u003e是参数量），对于这么大的矩阵存储或求逆都是很难做到的。\u003c/p\u003e\n\u003ch1 id=\"诱导范数作为步长约束\"\u003e诱导范数作为步长约束\u003c/h1\u003e\n\u003cp\u003e是否有一种更「廉价」的方法，可以考虑\u003cstrong\u003e模型的参数结构\u003c/strong\u003e，同时将\u003cstrong\u003e参数的变化对于输出的影响\u003c/strong\u003e作为约束呢？\u003c/p\u003e\n\u003cp\u003e幸运的是，对于当下最流行的神经网络（e.g., Transformer）而言，模型往往可以拆解为很多小模块，其中最常见的是Linear模块（线性映射，这里忽略bias term）\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ f(\\boldsymbol{x};\\boldsymbol{W})=\\boldsymbol{Wx},\\ \\boldsymbol{W}\\in\\mathbb{R}^{n\\times m},\\boldsymbol{x}\\in \\mathbb{R}^{m}\\\\ $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e在标准的Transformer中，Attention、FFN、LM分类器都是由Linear模块组成的，Embedding从数学原理上也是输入为one-hot encoding的线性映射。假设现在对于某个Linear模块的参数\u003ccode\u003e$\\boldsymbol{W}$\u003c/code\u003e做\u003ccode\u003e$\\Delta\\boldsymbol{W}$\u003c/code\u003e的更新（\u003ccode\u003e$\\boldsymbol{W}'\\leftarrow \\boldsymbol{W}+\\Delta\\boldsymbol{W}$\u003c/code\u003e），我们需要衡量这个更新对于最终输出的影响是多少（从而可以约束这个影响）。由于神经网络比较复杂，衡量\u003ccode\u003e$\\Delta\\boldsymbol{W}$\u003c/code\u003e对于最终目标函数的影响是相对繁琐的，但我们可以退而求其次，衡量\u003ccode\u003e$\\Delta\\boldsymbol{W}$\u003c/code\u003e对于这个Linear模块的输出\u003ccode\u003e$\\boldsymbol{Wx}$\u003c/code\u003e的影响。\n考虑线性模块的输入与输出空间的距离都使用欧几里得范数\u003ccode\u003e$\\|\\cdot\\|_{\\ell_2}$\u003c/code\u003e衡量，那么这个约束可以通过如下不等式实现\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\|\\Delta\\boldsymbol{W}\\boldsymbol{x}\\|_{\\ell_2} \\le {\\color[rgb]{0, 0.5, 0.8}{\\|\\Delta\\boldsymbol{W}\\|_{\\ell_2\\to\\ell_2}}}\\|\\boldsymbol{x}\\|_{\\ell_2} $$\u003c/code\u003e\n这里的\u003ccode\u003e${\\color[rgb]{0, 0.5, 0.8}{\\|\\Delta\\boldsymbol{W}\\|_{\\ell_2\\to\\ell_2}}}$\u003c/code\u003e是矩阵2范数。这个不等式告诉我们，如果约束了参数更新量的谱范数（不等式右侧），也就约束了更新前后这个线性模块输出的变化量。\u003c/p\u003e\n\u003cp\u003e假设现在需要优化的神经网络是由一系列的线性模块堆叠组成（e.g., MLP），我们可以参照梯度下降的推导构造如下的更新\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\underset{\\Delta\\boldsymbol{W}_1,\\ldots,\\Delta\\boldsymbol{W}_L}{\\text{arg min}}\\left[ \\sum_{l=1}^L {\\langle \\boldsymbol{G}_l, \\Delta\\boldsymbol{W}_l \\rangle}_F + \\frac{\\lambda}{2}\\max_{l=1}^L{\\color[rgb]{0, 0.5, 0.8}{\\|\\Delta\\boldsymbol{W}_l\\|^2_{\\ell_2\\to\\ell_2}}} \\right]\\\\ $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这里\u003ccode\u003e$\\boldsymbol{G}_l$\u003c/code\u003e表示\u003ccode\u003e$\\boldsymbol{W}_l$\u003c/code\u003e对应的梯度矩阵（布局与原参数矩阵相同），\u003ccode\u003e${\\langle \\cdot, \\cdot \\rangle}_F$\u003c/code\u003e表示Frobenius内积（对矩阵而言，逐元素相乘求和）。这里之所以使用\u003ccode\u003e$\\max_{l=1}^L$\u003c/code\u003e（而不是直接求和），是因为我们引入这个约束时希望目标函数在\u003ccode\u003e$\\Delta\\boldsymbol{W}_l$\u003c/code\u003e变化下，能够保持平滑的性质\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e，因此需要bound所有参数矩阵更新量的谱范数的最大值。\u003c/p\u003e\n\u003cp\u003e我们来逐步推导这个最小值成立时的\u003ccode\u003e$\\Delta\\boldsymbol{W}_1,\\ldots,\\Delta\\boldsymbol{W}_L$\u003c/code\u003e取值\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e。为了方便，把每个\u003ccode\u003e$\\Delta\\boldsymbol{W}_l$\u003c/code\u003e拆解成大小和方向两部分：\u003ccode\u003e$\\Delta\\boldsymbol{W}_l=c_l\\boldsymbol{T}_l(c_l\\triangleq\\|\\Delta\\boldsymbol{W}_l\\|_{\\ell_2\\to\\ell_2})$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e（为了可读性，下面的\u003ccode\u003e$\\|\\cdots\\|$\u003c/code\u003e均表示谱范数\u003ccode\u003e$\\|\\cdot\\|_{\\ell_2\\to\\ell_2}$\u003c/code\u003e）\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{align} \u0026amp;\\underset{\\Delta\\boldsymbol{W}_1,\\ldots,\\Delta\\boldsymbol{W}_L}{\\text{min}}\\left[ \\sum_{l=1}^L {\\langle \\boldsymbol{G}_l, \\Delta\\boldsymbol{W}_l \\rangle}_F + \\frac{\\lambda}{2}\\max_{l=1}^L\\|\\Delta\\boldsymbol{W}_l\\|^2 \\right]\\\\ \u0026amp;=\\underset{c_1,\\ldots,c_L\\ge 0}{\\text{min}}\\left[ \\sum_{l=1}^L c_l\\min_{\\|\\boldsymbol{T}_l\\|=1}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F + \\frac{\\lambda}{2}\\max_{l=1}^Lc_l^2 \\right]\\\\ \u0026amp;=\\underset{c_1,\\ldots,c_L\\ge 0}{\\text{min}}\\left[ -\\sum_{l=1}^L c_l\\max_{\\|\\boldsymbol{T}_l\\|=1}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F + \\frac{\\lambda}{2}\\max_{l=1}^Lc_l^2 \\right]\\\\ \u0026amp;=\\underset{c_1,\\ldots,c_L\\ge 0}{\\text{min}}\\left[ -\\sum_{l=1}^L c_l \\|\\boldsymbol{G}_l\\|_* + \\frac{\\lambda}{2}\\max_{l=1}^Lc_l^2 \\right]\\quad\\triangleright\\|\\cdot\\|_*\\text{表示核范数}\\\\ \u0026amp;=\\underset{\\eta\\ge 0}{\\text{min}}\\left[ -\\sum_{l=1}^L \\eta\\|\\boldsymbol{G}_l\\|_* + \\frac{\\lambda}{2}\\max_{l=1}^L \\eta^2 \\right]\\tag{1}\\\\ \\end{align} $$\u003c/code\u003e\u003c/p\u003e","title":"从约束视角看深度学习优化若干新进展"},{"content":"最近在阅读Muon is Scalable for LLM Training这篇文章的时候注意到他们使用无权重衰减（weight decay）版本的Muon优化LLM的时候，优化器的收敛优势会随着训练过程逐渐消失，又看到@小明同学在评论区提到的一个细节，很多开源的LLM在技术报告中都提到了使用0.1作为权重衰减的系数，觉得是个比较有意思的发现。结合Kimi的文章中关于bf16的简单陈述，笔者在本文中稍微展开讲下，权重衰减对于LLM的低精度训练中有什么作用。\n首先把结论放在前面：除了一般认知中的正则化作用，权重衰减也可能降低精度损失的风险——对于计算机的浮点数而言，绝对值越大，精度越低。对于低精度/混合精度训练而言，使用权重衰减可以控制参数的绝对值范围，从而保证模型参数不落入低精度的数值区间。\n浮点数的存储与精度 上述结论主要与浮点数在计算机内的存储形式有关。学过计算机的一些基本课程的读者可能有印象，浮点数的存储是二进制的形式，分为符号位、指数位和尾数位三段。深度学习中常见的浮点数协议（fp32、fp16、bf16、tf32）的区别在于指数位和尾数位的比特数量不同。由于浮点数是一个「指数」的形式，因此它在实数空间的分布是不均匀的。\n这里我们考虑规范数的情形（指数位非全0），做一点分析。假设符号位、指数位、尾数位（mantissa）的二进制编码分别是$S$、$E$、$M$，那么对应的浮点数为： $$ \\text{value}=(-1)^S\\times 1.M\\times 2^{E-\\text{bias}} $$\n在单精度fp32标准中，$\\text{bias}$取${01111111}_{2}=127_{10}$ .\nfp32浮点数的一个例子\n例如在图中的例子中，$S=0$，$E=\\underbrace{00\\cdots0}_{7\\ 0's}1$，$M=\\underbrace{00\\cdots0}_{22\\ 0's}1$，相应的值为 $$ \\begin{align} \u0026amp;{-1}^0\\times 1.\\underbrace{00\\cdots0}_{22\\ 0's}1_2\\times 2^{00000001_2-{01111111}_{2}}\\\\ \u0026amp;\\approx [1.175494490952134\\times 10^{-38}]_{10} \\end{align} $$\n现在我们来考虑不同的数值范围内的浮点数精度。对于区间范围$[2^{x}, 2^{x+1}],\\forall -126\\le x\\le127$（这里的x已经是经过-bias之后得到的最终指数），我们希望在给定任意浮点数$y\\in[2^{x}, 2^{x+1}]$的基础上增加一个最小量$\\varepsilon$（即区间内两个浮点数的最小间隔），这个增加的过程是通过操纵二进制编码实现的，那么最小间隔只能是通过在$y$的尾数部分加上\n$$ 0.\\underbrace{00\\cdots0}_{22\\ 0's}1 $$\n来实现，对应的最小间隔是 $$ \\begin{align} \\varepsilon \u0026amp;= 0.\\underbrace{00\\cdots0}_{22\\text{ 0's}}1_2\\times 2^{x}\\\\ \u0026amp;=2^{-23}\\times2^{x}=2^{x-23}\\\\ \\end{align} $$\n如果你将上面的公式带入不同的指数位，可以验证与Wikipedia中给出的这张表的Gap是吻合的：\n不同指数位下的最小精度，来源：Wikipedia\n这个计算可以拓展到其他的精度格式：\n对于半精度格式fp16而言，其尾数位有10位，对应的最小间隔是$2^{x-10}$； 对于半精度格式bf16而言，其尾数位有7位，对应的最小间隔是$2^{x-7}$； 从这里也可以看到，bf16相比fp16虽然拓宽了表示范围，但是减少了精度（同样数值范围内的最小间隔更宽了）。\n从这里我们得到了一个结论：计算机存储的浮点数之间的最小间隔随着浮点数绝对值数值增加，指数级地增大，换言之，浮点数（绝对值）数值越大，精度越低。并且这个问题对于fp16或bf16格式的浮点数，问题要更加显著。\n这个结论的另一个引申的问题是舍入误差，假如一个较大的浮点数和一个较小的浮点数相加，由于浮点数的加法（减法过程相当于取补码后相加，结论是类似的）过程需要先将两个数的指数位对齐，因此绝对值较小的数字的尾数的最后几位数字可能会在加法中丢失。这里我们举一个极端的例子来说明。\n假设浮点数存储为fp32格式（8位指数、23位尾数）。 $$ \\begin{align} x=(-1)^0\\times 1.0_2\\times 2^{-1}\\\\ y=(-1)^0\\times 1.0_2\\times 2^{-25}\\\\ \\end{align} $$\n在执行浮点数加法的时候，指数位首先按照大的一个operand的指数对齐，也就是$y$的指数变为$2^{-25}\\to 2^{-1}$，相应地需要将尾数（包括前面隐含的1）向右移动$-1-(-25)=24$位。由于fp32的尾数位只有23位，因此在移动之后$y$变成了$0.000\\cdots0\\times2^{-1}$。也就是说$y$在对齐的过程中直接变成了0！\n读者可以运行如下的代码验证这一结论：\nimport numpy as np x = np.float32(2**-1) # 0.5 y = np.float32(2**-25) # 0.0000001192092896 result = x + y print(f\u0026#34;x = {x}, y = {y}\u0026#34;) print(f\u0026#34;x+y = {result}\u0026#34;) print(f\u0026#34;x == x+y? {x==result}\u0026#34;) # x = 0.5, y = 2.9802322387695312e-08 # x+y = 0.5 # x == x+y? True 对深度模型训练的影响 上面我们从浮点数的存储格式建立了「计算机浮点数的数值绝对值越大，则精度越低」的结论，并且引申到舍入误差的问题。接下来我们把这个现象带入到一个神经网络的训练过程中来看可能引发什么样的问题。\n在神经网络的训练过程中，一个经典的训练过程是（以监督学习为例）：\n（forward）给定当前参数$\\boldsymbol{\\theta}$和小批量数据$\\boldsymbol{x},\\boldsymbol{y}$，计算损失函数$\\mathcal{L}(\\boldsymbol{\\theta};\\boldsymbol{x},\\boldsymbol{y})$； （backward）反向传播，得到每个参数的梯度； （update）更新优化器状态（梯度的统计量，例如梯度动量，Adam中的一、二阶矩统计量），更新模型参数$\\boldsymbol{\\theta}$. 对于这个过程而言，前一节中的结论会造成两方面的结果：\n使用低精度浮点数保存和更新模型参数时，如果模型参数绝对值比较大，而更新的步幅比较小，那么更新会由于舍入误差而失效； 从一个高精度的模型转化为低精度模型的时候，参数的绝对值越大，则丢失的精度越多。 如果读者有训练一些LM或者其他神经网络的经验，可能会发现Transformer这类深度模型在训练过程中，参数的范数会随着训练过程中逐渐增大。Merrill 2020指出对于T5模型而言，其参数范数的增长正比于$\\sqrt{t}$（$t$是更新次数）. 因此，对于训练后期，随着参数的量级逐渐变大，精度变差的风险也会增加。\n值得指出的是，当下的混合精度训练范式一般会在低精度的权重之外，维护一份fp32的权重，优化器的states一般也会使用高精度版本，防止累加的过程中出现严重的舍入误差。而且在低精度的GEMM中，也会使用高精度的accumulator来存储分块内的内积。但是这些操作仍然不能完全杜绝由浮点数存储带来的精度问题。\n混合精度训练的图示，图片来源：https://zhuanlan.zhihu.com/p/678116738\n例如，在模型更新了fp32的备份之后，还需要将fp32的权重转化为低精度的版本，参与后续的forward过程。由于浮点数的精度随着绝对值的增加而降低，因此参数的绝对值越大，在精度的转化中损失的精度也越多。此外，在前向和反向计算的过程中，激活值夜会存在类似的精度损失问题。\n如果我们在训练过程中引入权重衰减：\n$$ \\theta^+\\leftarrow \\theta - \\eta(\\tilde{\\Delta}+{\\color[rgb]{0, 0.5, 0.8}{\\lambda\\theta}}) $$ 其中$\\tilde{\\Delta}$是优化器计算出来的权重更新量，$\\lambda$是权重衰减的系数，那么模型的权重的绝对值就可以得到一定的控制。除了提供一定的正则化效应之外，也能够降低由于模型的参数范数增长而导致的精度损失的风险。\n总结 本文从浮点数的存储原理出发，建立了「数值越大，精度越低」的结论，从而（ad-hoc地）解释了LLM的训练对权重衰减的依赖。但是也需要指出的是，权重的范数增长与模型的结构是有一定关系的，这个规律不一定对所有模型成立。\n参考 Merrill 2020. Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent\nhttps://en.wikipedia.org/wiki/IEEE_754-1985\nMoonshot AI 2025. Muon is Scalable for LLM Training\nDeepseek AI 2024. DeepSeek-V3 Technical Report\n","permalink":"https://nil9.net/posts/wd-model-precision/","summary":"\u003cp\u003e最近在阅读\u003ca href=\"https://github.com/MoonshotAI/Moonlight/blob/master/Moonlight.pdf\"\u003eMuon is Scalable for LLM Training\u003c/a\u003e这篇文章的时候注意到他们使用无权重衰减（weight decay）版本的Muon优化LLM的时候，优化器的收敛优势会随着训练过程逐渐消失，又看到\u003ca href=\"https://www.zhihu.com/people/xiao-ming-tong-xue-86-81\"\u003e@小明同学\u003c/a\u003e在评论区提到的一个细节，很多开源的LLM在技术报告中都提到了使用0.1作为权重衰减的系数，觉得是个比较有意思的发现。结合Kimi的文章中关于bf16的简单陈述，笔者在本文中稍微展开讲下，权重衰减对于LLM的低精度训练中有什么作用。\u003c/p\u003e\n\u003cp\u003e首先把结论放在前面：除了一般认知中的正则化作用，权重衰减也可能降低精度损失的风险——对于计算机的浮点数而言，\u003cstrong\u003e绝对值越大，精度越低\u003c/strong\u003e。对于低精度/混合精度训练而言，使用权重衰减可以控制参数的绝对值范围，从而保证模型参数不落入低精度的数值区间。\u003c/p\u003e\n\u003ch1 id=\"浮点数的存储与精度\"\u003e浮点数的存储与精度\u003c/h1\u003e\n\u003cp\u003e上述结论主要与浮点数在计算机内的存储形式有关。学过计算机的一些基本课程的读者可能有印象，浮点数的存储是二进制的形式，分为符号位、指数位和尾数位三段。深度学习中常见的浮点数协议（fp32、fp16、bf16、tf32）的区别在于指数位和尾数位的比特数量不同。由于浮点数是一个「指数」的形式，\u003cstrong\u003e因此它在实数空间的分布是不均匀的\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e这里我们考虑规范数的情形（指数位非全0），做一点分析。假设符号位、指数位、尾数位（mantissa）的二进制编码分别是\u003ccode\u003e$S$\u003c/code\u003e、\u003ccode\u003e$E$\u003c/code\u003e、\u003ccode\u003e$M$\u003c/code\u003e，那么对应的浮点数为：\n\u003ccode\u003e$$ \\text{value}=(-1)^S\\times 1.M\\times 2^{E-\\text{bias}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e在单精度fp32标准中，\u003ccode\u003e$\\text{bias}$\u003c/code\u003e取\u003ccode\u003e${01111111}_{2}=127_{10}$\u003c/code\u003e .\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\n  \u003cimg alt=\"fp32浮点数的一个例子\" loading=\"lazy\" src=\"/images/wd-model-precision/ieee-fp32-example.png\" title=\"fp32浮点数的一个例子\"\u003e\u003cfigcaption class=\"image-caption\"\u003efp32浮点数的一个例子\u003c/figcaption\u003e\u003c/figure\u003e\u003c/p\u003e\n\u003cp\u003e例如在图中的例子中，\u003ccode\u003e$S=0$\u003c/code\u003e，\u003ccode\u003e$E=\\underbrace{00\\cdots0}_{7\\ 0's}1$\u003c/code\u003e，\u003ccode\u003e$M=\\underbrace{00\\cdots0}_{22\\ 0's}1$\u003c/code\u003e，相应的值为\n\u003ccode\u003e$$ \\begin{align} \u0026amp;{-1}^0\\times 1.\\underbrace{00\\cdots0}_{22\\ 0's}1_2\\times 2^{00000001_2-{01111111}_{2}}\\\\ \u0026amp;\\approx [1.175494490952134\\times 10^{-38}]_{10} \\end{align} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e现在我们来考虑不同的数值范围内的浮点数精度。对于区间范围\u003ccode\u003e$[2^{x}, 2^{x+1}],\\forall -126\\le x\\le127$\u003c/code\u003e（这里的x已经是经过-bias之后得到的最终指数），我们希望在给定任意浮点数\u003ccode\u003e$y\\in[2^{x}, 2^{x+1}]$\u003c/code\u003e的基础上增加一个最小量\u003ccode\u003e$\\varepsilon$\u003c/code\u003e（即区间内两个浮点数的最小间隔），这个增加的过程是通过操纵二进制编码实现的，那么最小间隔只能是通过在\u003ccode\u003e$y$\u003c/code\u003e的尾数部分加上\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ 0.\\underbrace{00\\cdots0}_{22\\ 0's}1 $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e来实现，对应的最小间隔是\n\u003ccode\u003e$$ \\begin{align} \\varepsilon \u0026amp;= 0.\\underbrace{00\\cdots0}_{22\\text{ 0's}}1_2\\times 2^{x}\\\\ \u0026amp;=2^{-23}\\times2^{x}=2^{x-23}\\\\ \\end{align} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e如果你将上面的公式带入不同的指数位，可以验证与\u003ca href=\"https://en.wikipedia.org/wiki/IEEE_754-1985\"\u003eWikipedia\u003c/a\u003e中给出的这张表的Gap是吻合的：\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\n  \u003cimg alt=\"不同指数位下的最小精度\" loading=\"lazy\" src=\"/images/wd-model-precision/ieee-fp32-prec.png\" title=\"不同指数位下的最小精度，来源：Wikipedia\"\u003e\u003cfigcaption class=\"image-caption\"\u003e不同指数位下的最小精度，来源：Wikipedia\u003c/figcaption\u003e\u003c/figure\u003e\u003c/p\u003e\n\u003cp\u003e这个计算可以拓展到其他的精度格式：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e对于半精度格式fp16而言，其尾数位有10位，对应的最小间隔是\u003ccode\u003e$2^{x-10}$\u003c/code\u003e；\u003c/li\u003e\n\u003cli\u003e对于半精度格式bf16而言，其尾数位有7位，对应的最小间隔是\u003ccode\u003e$2^{x-7}$\u003c/code\u003e；\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e从这里也可以看到，bf16相比fp16虽然拓宽了表示范围，但是减少了精度（同样数值范围内的最小间隔更宽了）。\u003c/p\u003e\n\u003cp\u003e从这里我们得到了一个结论：计算机存储的浮点数之间的最小间隔随着浮点数绝对值数值增加，指数级地增大，换言之，\u003cstrong\u003e浮点数（绝对值）数值越大，精度越低\u003c/strong\u003e。并且这个问题对于fp16或bf16格式的浮点数，问题要更加显著。\u003c/p\u003e\n\u003cp\u003e这个结论的另一个引申的问题是\u003cstrong\u003e舍入误差\u003c/strong\u003e，假如一个较大的浮点数和一个较小的浮点数相加，由于浮点数的加法（减法过程相当于取补码后相加，结论是类似的）过程需要先将两个数的指数位对齐，因此绝对值较小的数字的尾数的最后几位数字可能会在加法中丢失。这里我们举一个极端的例子来说明。\u003c/p\u003e\n\u003cp\u003e假设浮点数存储为fp32格式（8位指数、23位尾数）。\n\u003ccode\u003e$$ \\begin{align} x=(-1)^0\\times 1.0_2\\times 2^{-1}\\\\ y=(-1)^0\\times 1.0_2\\times 2^{-25}\\\\ \\end{align} $$\u003c/code\u003e\u003c/p\u003e","title":"为什么LLM一般使用较大的权重衰减系数？"},{"content":"最近Deepseek比较出圈，连带着里面用到的MLA也被讨论得更多了。MLA无疑是一个非常出色的attention改进，但是由于它的KV cache设计，不能很好地兼容RoPE，因此作者们使用了decoupled RoPE这样的「补丁」来引入位置关系，这无疑也增加了实现的复杂度。\n最近，修改注意力KV Cache这一线工作又增添了TPA这个新成员，笔者觉得这篇文章比较有趣，因此希望写一篇简短的导读。之所以叫「导读」，是因为本文不打算太深入文章的formulation和实验，而是从笔者自己的视角出发介绍文章的一些重点贡献。。\nMHA的拆解 最标准的多头注意力（Vaswani的版本），大致可以拆解成3个步骤（这里默认讨论自注意力）1： $$ \\begin{aligned} \\text{step 1 }\u0026amp; \\begin{cases} \\boldsymbol{q}_i^{(h)} = \\boldsymbol{x}_i\\boldsymbol{W}_q^{(h)}\\in\\mathbb{R}^{d_k},\\quad \\boldsymbol{W}_q^{(h)}\\in\\mathbb{R}^{d\\times d_k} \\\\ \\boldsymbol{k}_i^{(h)} = \\boldsymbol{x}_i\\boldsymbol{W}_k^{(h)}\\in\\mathbb{R}^{d_k},\\quad \\boldsymbol{W}_k^{(h)}\\in\\mathbb{R}^{d\\times d_k}\\\\ \\boldsymbol{v}_i^{(h)} = \\boldsymbol{x}_i\\boldsymbol{W}_v^{(h)}\\in\\mathbb{R}^{d_v},\\quad \\boldsymbol{W}_v^{(h)}\\in\\mathbb{R}^{d\\times d_v} \\end{cases} \\\\ \\text{step 2 }\u0026amp; \\begin{cases} \\boldsymbol{o}_t^{(h)} = \\text{Attention}\\left(\\boldsymbol{q}_t^{(h)}, \\boldsymbol{k}_{\\leq t}^{(h)}, \\boldsymbol{v}_{\\leq t}^{(h)}\\right)\\triangleq\\frac{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(h)} \\boldsymbol{k}_i^{(h)}{}^{\\top}\\right)\\boldsymbol{v}_i^{(h)}}{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(h)} \\boldsymbol{k}_i^{(h)}{}^{\\top}\\right)} \\\\ \\end{cases} \\\\ \\text{step 3 }\u0026amp; \\begin{cases} \\boldsymbol{o}_t = \\left[\\boldsymbol{o}_t^{(1)}, \\boldsymbol{o}_t^{(2)}, \\cdots, \\boldsymbol{o}_t^{(H)}\\right] \\end{cases} \\\\ \\end{aligned}\\\\ $$\n这里$\\boldsymbol{x}_1,\\boldsymbol{x}_2,\\cdots,\\boldsymbol{x}_l$是输入向量，上标$h\\in \\{1,\\ldots,H\\}$表示注意力头，$d,d_k,d_v$分别表示输入维度、key维度、value维度。在第一步中，我们将每个token的表征独立地线性投射到不同的自空间上，第二步是标准的点积注意力，第三步是拼接（后续再做一步线性变换）。\n我们重点来审视一下第一个步骤，这里由于每个token的表征是独立操作的（类似FFN），因此我们可以将每个token的表征看做一个样本点，这里做的事情就是将一个$d$维度的向量，转化成3个矩阵$\\tilde{\\boldsymbol{Q}}\\in\\mathbb{R}^{H\\times d_k},\\tilde{\\boldsymbol{K}}\\in\\mathbb{R}^{H\\times d_k},\\tilde{\\boldsymbol{V}}\\in\\mathbb{R}^{H\\times d_v}$，每一个头对应这个矩阵中的一行。接着我们逐行计算每行对应的三组向量的点积注意力，就构成了标准的多头注意力。在标准实现中，这个变换是通过参数矩阵的线性变换+ reshape实现的。\n在自回归模型推理的阶段，这里涉及到的$\\boldsymbol{k}_i^{(h)},\\boldsymbol{v}_i^{(h)}$会被后续的token使用到，因此可以将其缓存起来，避免重复计算，这就是KV cache的思想。序列中每个位置则需要缓存$2Hd_{kv}$个值（实现中一般$d_k=d_v$）。\nTPA：一种低秩重参化技巧 上面我们描述了在标准注意力中，在计算点积注意力之前，需要通过一系列形如$f:\\mathbb{R}^d\\to\\mathbb{R}^{H\\times d'}$的映射，把输入投射到query-key-value子空间。在TPA这个文章中，作者介绍了一Contextual Factorization (CF)技巧来构造这个映射。笔者将这个技巧描述为「低秩重参化」。\n以$\\tilde{\\boldsymbol{Q}}$的构造为例，对于输入$\\boldsymbol{x}_i$，引入两个参数矩阵$\\boldsymbol{W}^A\\in\\mathbb{R}^{d\\times (r\\cdot H)},\\boldsymbol{W}^B\\in\\mathbb{R}^{d\\times (r\\cdot d_k)}$，得到两个向量2 $$ \\begin{align} \\boldsymbol{a}_i \u0026amp;= \\boldsymbol{x}_i \\boldsymbol{W}^A\\in\\mathbb{R}^{r\\cdot H}\\\\ \\boldsymbol{b}_i \u0026amp;= \\boldsymbol{x}_i \\boldsymbol{W}^B\\in\\mathbb{R}^{r\\cdot d_k}\\\\ \\end{align}\\\\ $$\n这里$R$是我们指定的最大的秩，是一个超参数，将这两个向量reshape成矩阵形式： $$ \\begin{align} \\tilde{\\boldsymbol{A}}_i \u0026amp;\\in\\mathbb{R}^{r\\times H}\\\\ \\tilde{\\boldsymbol{B}}_i \u0026amp;\\in\\mathbb{R}^{r\\times d_k}\\\\ \\end{align}\\tag{1}\\\\ $$\n这样我们就可以构造出一个$\\tilde{\\boldsymbol{Q}}$\n$$ \\tilde{\\boldsymbol{Q}}_i = \\frac{1}{R}\\tilde{\\boldsymbol{A}}_i^\\top \\tilde{\\boldsymbol{B}}_i\\in\\mathbb{R}^{H\\times d_k}\\\\ $$\n同样的方法可以构造出$\\tilde{\\boldsymbol{K}}_i\\in\\mathbb{R}^{H\\times d_k}$和$\\tilde{\\boldsymbol{V}}_i\\in\\mathbb{R}^{H\\times d_v}$。按照上一节中的介绍，后面需要做的就是将三组矩阵的每一行分别当做每个注意力头的query-key-value做点积注意力。与标准MHA不同的是，这里的映射是带有非线性的。\n顺便一提，原作中这个重参化的引入是用的外积和的形式，笔者觉得有点冗余了，因为外积和与矩阵的乘法是等价的，相信多数读者对于矩阵的乘法是更加熟悉的。\n更少的KV Cache 使用这种重参化的形式的一个好处是，在推理的时候，只需要缓存$\\{\\tilde{\\boldsymbol{A}}^K_j,\\tilde{\\boldsymbol{B}}^K_j,\\tilde{\\boldsymbol{A}}^V_j,\\tilde{\\boldsymbol{B}}^V_j\\}_{j\\le t}$即可，对应每个token位置的KV Cache量在 $$ r_k(H+d_k)+r_v(H+d_v)\\\\ $$\n如果代入原作的设定$r_k=r_v=2$，TPA的KV Cache可以大致计算为$4(H+d_{kv})$，比起标准MHA的$2Hd_{kv}$要低不少。根据知乎@寒月灼华的计算，Medium大小的模型上，TPA每token的KV Cache为444，相比MHA的2048和MLA的1056，都是更有优势的。\n兼容旋转位置编码 众所周知，现在最广泛使用的位置编码方式RoPE，可以通过在点积注意力的query-key上分别乘上分块对角旋转矩阵来实现高效的相对位置表征3。而在MLA中，由于KV Cache保存的压缩向量并不是点积注意力最终的key，因此不能直接兼容RoPE。而TPA的形式恰好可以直接兼容RoPE，原作中有比较完整的证明过程，这里笔者按照上一节的符号做一个简短的sketch proof。\nRoPE的基本思想是，在第$i$个token位置引入旋转编码矩阵$\\boldsymbol{\\mathcal{R}}_i$，从而 $$ \\left(\\boldsymbol{q}_i\\boldsymbol{\\mathcal{R}}_i\\right)\\left(\\boldsymbol{k}_j\\boldsymbol{\\mathcal{R}}_j\\right)^\\top = \\boldsymbol{q}_i\\boldsymbol{\\mathcal{R}}_{j-i}\\boldsymbol{k}_j^\\top\\tag{2}\\\\ $$\n在KV Cache的框架下，问题的关键在于令key的旋转位置编码被包含在KV Cache中，刚好TPA能够满足这个要求。考虑上一节构造的$\\tilde{\\boldsymbol{Q}},\\tilde{\\boldsymbol{K}}$\n$$ \\begin{align} \\tilde{\\boldsymbol{Q}}_i \u0026amp;= \\frac{1}{R}(\\tilde{\\boldsymbol{A}}_i^Q)^\\top \\tilde{\\boldsymbol{B}}_i^Q\\in\\mathbb{R}^{H\\times d_k}\\\\ \\tilde{\\boldsymbol{K}}_j \u0026amp;= \\frac{1}{R}(\\tilde{\\boldsymbol{A}}_j^K)^\\top \\tilde{\\boldsymbol{B}}_j^K\\in\\mathbb{R}^{H\\times d_k}\\\\ \\end{align}\\\\ $$\n我们已经提到，第$h$个注意力头就是在$\\tilde{\\boldsymbol{Q}},\\tilde{\\boldsymbol{K}},\\tilde{\\boldsymbol{V}}$矩阵的第$h$行向量基础上做点积注意力。我们假设$\\boldsymbol{a}_i^Q$是$\\tilde{\\boldsymbol{A}}_i^Q$的第$h$列，$\\boldsymbol{a}_j^K$是$\\tilde{\\boldsymbol{A}}_j^K$的第$h$列，则在TPA中，第$h$个注意力头的点积注意力的输入分别是 $$ \\begin{align} \\boldsymbol{q}_i \u0026amp;= \\frac{1}{R}\\boldsymbol{a}_j^Q \\tilde{\\boldsymbol{B}}_i^Q\\in\\mathbb{R}^{d_k}\\\\ \\boldsymbol{k}_j \u0026amp;= \\frac{1}{R}\\boldsymbol{a}_j^K \\tilde{\\boldsymbol{B}}_j^K\\in\\mathbb{R}^{d_k}\\\\ \\end{align}\\\\ $$\n按照公式$(2)$的原理，只需要将旋转位置编码乘在上述两项的右侧即可 $$ \\begin{align} \\boldsymbol{q}_i{\\color[rgb]{0, 0.5, 0.8}{\\boldsymbol{\\mathcal{R}}_i}} \u0026amp;= \\frac{1}{R}\\boldsymbol{a}_j^Q \\tilde{\\boldsymbol{B}}_i^Q{\\color[rgb]{0, 0.5, 0.8}{\\boldsymbol{\\mathcal{R}}_i}}=\\frac{1}{R}\\boldsymbol{a}_j^Q \\left(\\tilde{\\boldsymbol{B}}_i^Q{\\color[rgb]{0, 0.5, 0.8}{\\boldsymbol{\\mathcal{R}}_i}}\\right)\\\\ \\boldsymbol{k}_j{\\color[rgb]{0, 0.5, 0.8}{\\boldsymbol{\\mathcal{R}}_j}} \u0026amp;= \\frac{1}{R}\\boldsymbol{a}_j^K \\tilde{\\boldsymbol{B}}_j^K{\\color[rgb]{0, 0.5, 0.8}{\\boldsymbol{\\mathcal{R}}_j}}=\\frac{1}{R}\\boldsymbol{a}_j^K \\left(\\tilde{\\boldsymbol{B}}_j^K{\\color[rgb]{0, 0.5, 0.8}{\\boldsymbol{\\mathcal{R}}_j}}\\right)\\\\ \\end{align}\\\\ $$\n在实现中，这相当于在公式$(1)$的变换之后，分别对$\\tilde{\\boldsymbol{B}}^Q, \\tilde{\\boldsymbol{B}}^K$应用RoPE编码4\nB_q, B_k = apply_rotary_emb(B_q, cos, sin), apply_rotary_emb(B_k, cos, sin) 在推理的时候，将$\\{\\tilde{\\boldsymbol{A}}^K_j,\\tilde{\\boldsymbol{B}}^K_j{\\color[rgb]{0, 0.5, 0.8}{\\boldsymbol{\\mathcal{R}}_j}},\\tilde{\\boldsymbol{A}}^V_j,\\tilde{\\boldsymbol{B}}^V_j\\}_{j\\le t}$缓存，剩余部分正常计算。\n总结 本文简单介绍了Tensor-Product Attention（TPA）的基本方法和两个性质：较少的KV Cache缓存量和RoPE的兼容性。更细节的描述和详尽的实验请阅读Zhang 2025. Tensor Product Attention Is All You Need以及作者维护的仓库tensorgi/T6。\n参考阅读 Zhang 2025. Tensor Product Attention Is All You Need 缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA Transformer升级之路：2、博采众长的旋转式位置编码 本篇中的向量是行向量。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n为了显示简洁，公式中去掉了Q的标注。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n这是逻辑上的描述，实现上不会实例化一整个矩阵。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/tensorgi/T6/blob/bd6dd4ab682a9955d256d395fa9bf0d5da8a804b/model/T6.py#L122C9-L122C84\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://nil9.net/posts/tensor-product-attention/","summary":"\u003cp\u003e最近Deepseek比较出圈，连带着里面用到的MLA也被讨论得更多了。MLA无疑是一个非常出色的attention改进，但是由于它的KV cache设计，不能很好地兼容\u003ca href=\"https://kexue.fm/archives/8265\"\u003eRoPE\u003c/a\u003e，因此作者们使用了decoupled RoPE这样的「补丁」来引入位置关系，这无疑也增加了实现的复杂度。\u003c/p\u003e\n\u003cp\u003e最近，修改注意力KV Cache这一线工作又增添了\u003ca href=\"https://arxiv.org/pdf/2501.06425\"\u003eTPA\u003c/a\u003e这个新成员，笔者觉得这篇文章比较有趣，因此希望写一篇简短的导读。之所以叫「导读」，是因为本文不打算太深入文章的formulation和实验，而是从笔者自己的视角出发介绍文章的一些重点贡献。。\u003c/p\u003e\n\u003ch1 id=\"mha的拆解\"\u003eMHA的拆解\u003c/h1\u003e\n\u003cp\u003e最标准的多头注意力（Vaswani的版本），大致可以拆解成3个步骤（这里默认讨论自注意力）\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e：\n\u003ccode\u003e$$ \\begin{aligned} \\text{step 1 }\u0026amp; \\begin{cases} \\boldsymbol{q}_i^{(h)} = \\boldsymbol{x}_i\\boldsymbol{W}_q^{(h)}\\in\\mathbb{R}^{d_k},\\quad \\boldsymbol{W}_q^{(h)}\\in\\mathbb{R}^{d\\times d_k} \\\\  \\boldsymbol{k}_i^{(h)} = \\boldsymbol{x}_i\\boldsymbol{W}_k^{(h)}\\in\\mathbb{R}^{d_k},\\quad \\boldsymbol{W}_k^{(h)}\\in\\mathbb{R}^{d\\times d_k}\\\\  \\boldsymbol{v}_i^{(h)} = \\boldsymbol{x}_i\\boldsymbol{W}_v^{(h)}\\in\\mathbb{R}^{d_v},\\quad \\boldsymbol{W}_v^{(h)}\\in\\mathbb{R}^{d\\times d_v}  \\end{cases} \\\\ \\text{step 2 }\u0026amp; \\begin{cases} \\boldsymbol{o}_t^{(h)} = \\text{Attention}\\left(\\boldsymbol{q}_t^{(h)}, \\boldsymbol{k}_{\\leq t}^{(h)}, \\boldsymbol{v}_{\\leq t}^{(h)}\\right)\\triangleq\\frac{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(h)} \\boldsymbol{k}_i^{(h)}{}^{\\top}\\right)\\boldsymbol{v}_i^{(h)}}{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(h)} \\boldsymbol{k}_i^{(h)}{}^{\\top}\\right)} \\\\  \\end{cases} \\\\ \\text{step 3 }\u0026amp; \\begin{cases} \\boldsymbol{o}_t = \\left[\\boldsymbol{o}_t^{(1)}, \\boldsymbol{o}_t^{(2)}, \\cdots, \\boldsymbol{o}_t^{(H)}\\right]  \\end{cases} \\\\ \\end{aligned}\\\\ $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这里\u003ccode\u003e$\\boldsymbol{x}_1,\\boldsymbol{x}_2,\\cdots,\\boldsymbol{x}_l$\u003c/code\u003e是输入向量，上标\u003ccode\u003e$h\\in \\{1,\\ldots,H\\}$\u003c/code\u003e表示注意力头，\u003ccode\u003e$d,d_k,d_v$\u003c/code\u003e分别表示输入维度、key维度、value维度。在第一步中，我们将每个token的表征独立地线性投射到不同的自空间上，第二步是标准的点积注意力，第三步是拼接（后续再做一步线性变换）。\u003c/p\u003e\n\u003cp\u003e我们重点来审视一下第一个步骤，这里由于每个token的表征是独立操作的（类似FFN），因此我们可以将每个token的表征看做一个样本点，这里做的事情就是将一个\u003ccode\u003e$d$\u003c/code\u003e维度的向量，转化成3个矩阵\u003ccode\u003e$\\tilde{\\boldsymbol{Q}}\\in\\mathbb{R}^{H\\times d_k},\\tilde{\\boldsymbol{K}}\\in\\mathbb{R}^{H\\times d_k},\\tilde{\\boldsymbol{V}}\\in\\mathbb{R}^{H\\times d_v}$\u003c/code\u003e，每一个头对应这个矩阵中的一行。接着我们逐行计算每行对应的三组向量的点积注意力，就构成了标准的多头注意力。在标准实现中，这个变换是通过参数矩阵的线性变换+ reshape实现的。\u003c/p\u003e","title":"Tensor Product Attention (TPA) 导读"},{"content":"上篇文章中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在本篇文章中，我们利用上篇得到的结论，推导自然梯度中为何引入FIM来修正梯度方向，并介绍自然梯度的一些性质。\n梯度下降：欧氏距离下的最速下降 考虑一个最优化任务（$f:\\Theta\\to\\mathbb{R}$）： $$ \\underset{\\theta}{\\operatorname{min}} f(\\theta) $$\n最常见的一阶优化方法是梯度下降/steepest descent： $$ \\theta^+=\\theta-\\eta\\nabla_\\theta f $$\n其中$\\eta$是学习率。这里的「steepest」指的是在约束欧氏距离定义下的步长在极小范围内时，选取梯度的负方向能最大化一步之内目标函数下降的程度。 $$ \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta:\\|\\delta\\|\\le \\epsilon}{\\operatorname{argmin}} f(\\theta+\\delta)\\right)=-\\frac{\\nabla_\\theta f}{\\|\\nabla_\\theta f\\|}\\tag{1} $$\nproof\n对极限内的目标函数做一阶泰勒展开： $$ \\begin{align} \\underset{\\delta:\\|\\delta\\|\\le \\epsilon}{\\operatorname{argmin}} f(\\theta+\\delta) \u0026amp;\\approx \\underset{\\delta:\\|\\delta\\|\\le \\epsilon}{\\operatorname{argmin}} f(\\theta) + \\nabla_\\theta f^\\top\\delta\\\\ \u0026amp;=\\underset{\\delta:\\|\\delta\\|\\le \\epsilon}{\\operatorname{argmin}} \\nabla_\\theta f^\\top\\delta \\end{align} $$\n我们将约束条件稍加改写： $$ \\underset{\\delta}{\\min} \\nabla_\\theta f^\\top\\delta\\quad\\text{s.t. }\\|\\delta\\|^2\\le \\epsilon^2 $$\n定义拉格朗日函数 $$ \\mathcal{L}(\\delta, \\lambda):= \\nabla_\\theta f^\\top\\delta + \\lambda (\\|\\delta\\|^2-\\epsilon^2) $$\n根据KKT条件： $$ \\begin{align} \u0026amp;\\nabla_\\delta\\mathcal{L}(\\delta, \\lambda) = 0 \u0026amp;\\triangleright\\text{Stationarity}\\\\ \u0026amp;\\lambda(\\|\\delta\\|^2-\\epsilon^2)=0 \u0026amp;\\triangleright\\text{Complementary slackness}\\\\ \u0026amp;\\|\\delta\\|^2-\\epsilon^2 \\le0\u0026amp;\\triangleright\\text{Primal feasibility}\\\\ \u0026amp;\\lambda\\ge 0\u0026amp;\\triangleright\\text{Dual feasibility}\\\\ \\end{align} $$\n根据驻点条件得到 $$ \\begin{align} \u0026amp;\\nabla_\\theta f+2\\lambda\\delta=0\\\\ \u0026amp;\\delta = -\\frac{1}{2\\lambda}\\nabla_\\theta f\\\\ \\end{align} $$\n代入互补松弛条件（这里$\\lambda=0$可以排除，因为会造成$\\delta$为unbounded）： $$ \\begin{align} \\lambda\u0026amp;\\left(\\left\\|-\\frac{1}{2\\lambda}\\nabla_\\theta f\\right\\|^2-\\epsilon^2\\right)=0\\\\ \u0026amp;\\lambda = \\frac{1}{2\\epsilon}\\|\\nabla_\\theta f\\|\\\\ \\end{align} $$\n带回驻点条件： $$ \\delta^* = -\\epsilon \\frac{\\nabla_\\theta f}{\\|\\nabla_\\theta f\\|} $$\n将$\\delta^*$代入原极限表达式：\n$$ \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta:\\|\\delta\\|\\le \\epsilon}{\\operatorname{argmin}} f(\\theta+\\delta)\\right)=-\\frac{\\nabla_\\theta f}{\\|\\nabla_\\theta f\\|} $$\n注意到在上述的「steepest」的求解中，对步长的约束条件$\\|\\delta\\|\\le\\epsilon$基于欧几里得距离，这里隐含了如下假设：（1）参数空间是标准欧几里得空间；（2）参数构成一组正交归一（orthonormal）的坐标系统。当这个假设不能很好满足的时候，梯度下降的最速性质可能会大打折扣。\n为了说明这个问题，我们考虑一个二维的二次型函数$f(\\boldsymbol{x})=\\boldsymbol{x}^\\top\\boldsymbol{Ax}$，我们令 $$ \\boldsymbol{A} = \\left[\\begin{matrix}1 \u0026amp; 0.5\\\\0.5 \u0026amp; 2\\end{matrix}\\right] $$\n这个函数的等高线图是一个典型的椭圆型，在这个例子中，参数空间不是标准欧氏空间，而是被$\\boldsymbol{A}$定义的椭球几何所支配。如下图所示，使用标准的梯度下降时，由于梯度方向并不指向最低点，因此优化路径是一条曲线，或者（当学习率过大时）呈Z字形。图片右侧是使用自然梯度的优化路径，后面我们会推导自然梯度的形式。\n使用梯度下降与自然梯度在一个「椭球型」二次型函数上的优化路径\n自然梯度：黎曼距离下的最速下降 在揭示了梯度下降的可能问题之后，我们将公式$(1)$中的约束做如下的泛化（差异处我们用蓝色做了区分）：\n$$ \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} f(\\theta+\\delta)\\right)=?\\tag{2} $$\n这里我们引入了局部的度量张量$G(\\theta)$，上篇文章已经简要介绍过黎曼度量和其定义的线元（局部微小变化的长度）\n$$ |\\delta|^2 = \\delta^\\top G(\\theta) \\delta $$\n对$(2)$式做推导，得到的结果就是自然梯度的方向. $$ \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} f(\\theta+\\delta)\\right)=-CG(\\theta)^{-1}\\nabla_\\theta f \\tag{3} $$\n其中$C$是某个常数，可以被吸收到学习率中。这里的证明框架与梯度下降是基本一致的，只不过对约束条件做了一定修改（高亮为蓝色）：\nproof\n对极限内的目标函数做一阶泰勒展开： $$ \\begin{align} \\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} f(\\theta+\\delta) \u0026amp;\\approx \\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} f(\\theta) + \\nabla_\\theta f^\\top\\delta\\\\ \u0026amp;=\\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} \\nabla_\\theta f^\\top\\delta \\end{align} $$\n定义拉格朗日函数 $$ \\mathcal{L}(\\delta, \\lambda):= \\nabla_\\theta f^\\top\\delta + \\lambda ({\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta} -\\epsilon^2) $$\n根据KKT条件： $$ \\begin{align} \u0026amp;\\nabla_\\delta\\mathcal{L}(\\delta, \\lambda) = 0 \u0026amp;\\triangleright\\text{Stationarity}\\\\ \u0026amp;\\lambda ({\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta} -\\epsilon^2)=0 \u0026amp;\\triangleright\\text{Complementary slackness}\\\\ \u0026amp;{\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta} -\\epsilon^2 \\le0\u0026amp;\\triangleright\\text{Primal feasibility}\\\\ \u0026amp;\\lambda\\ge 0\u0026amp;\\triangleright\\text{Dual feasibility}\\\\ \\end{align} $$\n根据驻点条件得到 $$ \\begin{align} \u0026amp;\\nabla_\\theta f+2\\lambda {\\color[rgb]{0, 0.5, 0.8}G(\\theta)}\\delta=0\\\\ \u0026amp;\\delta = -\\frac{1}{2\\lambda} {\\color[rgb]{0, 0.5, 0.8}G(\\theta)^{-1}}\\nabla_\\theta f\\\\ \\end{align} $$\n代入互补松弛条件（$\\lambda=0$可以排除，因为会造成$\\delta$为unbounded）： $$ \\begin{align} {\\color[rgb]{0, 0.5, 0.8}\\left(-\\frac{1}{2\\lambda} G(\\theta)^{-1}\\nabla_\\theta f\\right)^\\top }\u0026amp;{\\color[rgb]{0, 0.5, 0.8}G(\\theta)\\left(-\\frac{1}{2\\lambda} G(\\theta)^{-1}\\nabla_\\theta f\\right)}-\\epsilon^2=0\\\\ \\lambda \u0026amp;= \\frac{1}{2\\epsilon}{\\color[rgb]{0, 0.5, 0.8}\\sqrt{ \\nabla_\\theta f^\\top G(\\theta)^{-1}\\nabla_\\theta f }}\\\\ \\end{align} $$\n带回驻点条件： $$ \\delta^* = -\\epsilon\\frac{{\\color[rgb]{0, 0.5, 0.8}G(\\theta)^{-1}}\\nabla_\\theta f}{{\\color[rgb]{0, 0.5, 0.8}\\sqrt{ \\nabla_\\theta f^\\top G(\\theta)^{-1}\\nabla_\\theta f }}} $$\n将$\\delta^*$代入原极限表达式：\n$$ \\begin{align} \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} f(\\theta+\\delta)\\right)\u0026amp;=-\\frac{{\\color[rgb]{0, 0.5, 0.8}G(\\theta)^{-1}}\\nabla_\\theta f}{{\\color[rgb]{0, 0.5, 0.8}\\sqrt{ \\nabla_\\theta f^\\top G(\\theta)^{-1}\\nabla_\\theta f }}}\\\\ \u0026amp;=-C{\\color[rgb]{0, 0.5, 0.8}G(\\theta)^{-1}}\\nabla_\\theta f\\\\ \\end{align} $$\n在上述结论中，我们实质上是对标准的梯度下降方向应用了度量张量的逆$G(\\theta)^{-1}$，从而修正了梯度的方向（可以将这个矩阵叫做pre-conditioner）。\n在机器学习中，我们常关注的是概率模型的最大似然优化问题，在自然梯度（一）：Fisher信息矩阵作为黎曼度量中，我们已经建立了Fisher信息矩阵是给定概率分布族的参数空间的黎曼度量张量这一结论。如果我们需要优化的函数是一个概率似然函数$\\ell(\\theta):=\\log p(x|\\theta)$，则自然梯度可以直接由Fisher信息矩阵作为pre-conditioner $$ \\begin{align} \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta+\\delta)\\right) \\le \\epsilon^2}}{\\operatorname{argmin}} \\ell(\\theta+\\delta)\\right)\u0026amp;\\approx\\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top F(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} \\ell(\\theta+\\delta)\\right)\\\\ \u0026amp;=-CF(\\theta)^{-1}\\nabla_\\theta \\ell \\end{align} $$\n对应的参数更新公式为\n$$ \\theta^+=\\theta-\\eta F(\\theta)^{-1}\\nabla_\\theta \\ell(\\theta|x) $$\n拓展到判别模型 在常见的监督学习设定下，我们学习的是一个判别模型，优化目标是一系列条件概率的联合对数似然函数，其中每个输入$\\boldsymbol{x}^{(i)}$对应一个条件概率分布 $$ \\ell(\\theta)=-\\frac{1}{n} \\sum_i^n \\left[\\log p_\\theta(y^{(i)}|\\boldsymbol{x}^{(i)})\\right] $$\n相应地，约束条件需要更改为在每个条件概率的KL散度的期望 $$ \\mathbb{E}_{x\\sim\\tilde{q}(x)}\\left[D_{\\text{KL}}\\left(p(y|x;\\theta)\\|p(y|x;\\theta+\\delta)\\right)\\right] \\le \\epsilon^2 $$\n这里的$\\tilde{q}(x)$是输入数据的真实分布或替代分布（与真实分布接近）。这个约束条件对应的Fisher信息矩阵的形式为\n$$ F(\\theta)=\\mathbb{E}_{x\\sim\\tilde{q}(x)}\\left[ \\mathbb{E}_{y\\sim p(y|x;\\theta)}\\left[ \\nabla_\\theta \\log p(y|x;\\theta)\\nabla_\\theta \\log p(y|x;\\theta)^\\top \\right] \\right]\\tag{4} $$\n自然梯度的特性 与二阶优化的联系与区别\n自然梯度的一般形式中，使用$G(\\theta)^{-1}$作为梯度的pre-conditioner。如果把自然梯度看做一个一般的框架（而不仅仅考虑概率模型），那么当优化目标满足一定条件（e.g.,凸函数）时，二阶优化可以看做是选取Hessian作为自然梯度的度量张量。\n对于常见的概率模型框架（优化对数似然损失），我们选取FIM作为度量张量，可以带来与二阶优化类似的性质，例如，在函数流形的局部曲率比较小的时候（plateau），自然梯度会将更新步长拉得比较大，从而可能有助于快速离开plateau。不过也需要注意，这里的曲率定义在模型的函数流形上，而不是最终的损失函数定义的函数流形上。\nFIM相比Hessian具有一些比较好的特性。一方面，FIM是一个协方差矩阵，它总是半正定的，而Hessian则不然（非正定矩阵的逆是不稳定的）。另一方面，我们观察$(4)$中定义的FIM，注意到内层的期望是定义在模型分布$p(y|x;\\theta)$上的，也就是说，估计一个FIM只需要输入分布和模型分布，而不需要知道标签的真实分布，这在mini-batch特别小（e.g., online learning）的时候非常方便——我们可以在一个无标注的数据集上估计FIM，然后将得到的统计量与一个有标注的batch数据计算得到的梯度结合更新模型参数。\n另外，在特定条件下，自然梯度可以等价于广义-高斯牛顿方法，而后者一般被认为是一个二阶优化方法，可以参考Martens 2020.。\n模型KL约束\n使用FIM的自然梯度通过约束模型在一步更新前后的KL散度得到的「最优」方向，这种约束与模型的参数化方式无关——无论什么样的模型，一步更新的结果都是恒定的KL散度变化约束。在模型的分布距离约束下，优化过程中每一步更新后，模型的分布都不会有非常剧烈的变化，这构成了一种「平滑」的效应，Pascanu \u0026amp; Bengio 2014.认为这一定程度上可以防止过拟合。\n应用限制：复杂度考虑\n到目前为止，自然梯度仍然没有在深度学习中得到广泛应用。自然梯度需要计算FIM，对于包含$M$个参数的模型而言，FIM的空间复杂度为$\\mathcal{O}(M^2)$，对于现在的神经网络而言，这是一个不小的负担——一阶优化方法只需要$\\mathcal{O}(M)$的优化器状态。另外，对一个大矩阵求逆也需要比较大的计算复杂度。将自然梯度推广到大模型中需要引入FIM的结构假设（e.g., 分块对角）。\n总结 在自然梯度的两篇文章中，我们从Fisher信息矩阵（FIM）的定义出发，将FIM与概率模型的参数空间的黎曼度量建立联系。在此基础上，我们推导了自然梯度中为何引入FIM来修正梯度方向，并讨论了自然梯度的特性、与二阶优化的联系与区别、以及应用的限制。\n参考阅读 Amari 1998. Natural Gradient Works Efficiently in Learning Amari. Information Geometry of Neural Networks Pascanu \u0026amp; Bengio 2014. Revisiting Natural Gradient for Deep Networks Martens 2020. New Insights and Perspectives on the Natural Gradient Method ","permalink":"https://nil9.net/posts/natural-gradient-descent/","summary":"\u003cp\u003e\u003ca href=\"https://nil9.net/posts/fisher-info-matrix/\"\u003e上篇文章\u003c/a\u003e中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在本篇文章中，我们利用上篇得到的结论，推导自然梯度中为何引入FIM来修正梯度方向，并介绍自然梯度的一些性质。\u003c/p\u003e\n\u003ch1 id=\"梯度下降欧氏距离下的最速下降\"\u003e梯度下降：欧氏距离下的最速下降\u003c/h1\u003e\n\u003cp\u003e考虑一个最优化任务（\u003ccode\u003e$f:\\Theta\\to\\mathbb{R}$\u003c/code\u003e）：\n\u003ccode\u003e$$ \\underset{\\theta}{\\operatorname{min}} f(\\theta) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e最常见的一阶优化方法是梯度下降/steepest descent：\n\u003ccode\u003e$$ \\theta^+=\\theta-\\eta\\nabla_\\theta f $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中\u003ccode\u003e$\\eta$\u003c/code\u003e是学习率。这里的「steepest」指的是在约束欧氏距离定义下的步长在极小范围内时，选取梯度的负方向能最大化一步之内目标函数下降的程度。\n\u003ccode\u003e$$ \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta:\\|\\delta\\|\\le \\epsilon}{\\operatorname{argmin}}  f(\\theta+\\delta)\\right)=-\\frac{\\nabla_\\theta f}{\\|\\nabla_\\theta f\\|}\\tag{1} $$\u003c/code\u003e\u003c/p\u003e\n\u003cstyle type=\"text/css\"\u003e\n     \n    .notice {\n        --title-color: #fff;\n        --title-background-color: #6be;\n        --content-color: #444;\n        --content-background-color: #e7f2fa;\n    }\n\n    .notice.proof {\n        --title-background-color: rgb(130, 130, 130);\n        --content-background-color: #f7f7f7;\n    }\n\n    .notice.info {\n        --title-background-color: #fb7;\n        --content-background-color: #fec;\n    }\n\n    .notice.tip {\n        --title-background-color: #5a5;\n        --content-background-color: #efe;\n    }\n\n    .notice.warning {\n        --title-background-color: #c33;\n        --content-background-color: #fee;\n    }\n\n     \n\n    body.dark .notice {\n        --title-color: #fff;\n        --title-background-color: #069;\n        --content-color: #ddd;\n        --content-background-color: #023;\n    }\n\n    body.dark .notice.proof {\n        --title-background-color: rgb(129, 129, 129);\n        --content-background-color: rgb(41, 41, 41);\n    }\n\n    body.dark .notice.info {\n        --title-background-color: #a50;\n        --content-background-color: #420;\n    }\n\n    body.dark .notice.tip {\n        --title-background-color: #363;\n        --content-background-color: #121;\n    }\n\n    body.dark .notice.warning {\n        --title-background-color: #800;\n        --content-background-color: #400;\n    }\n\n     \n    .notice {\n        padding: 18px;\n        line-height: 24px;\n        margin-bottom: 24px;\n        border-radius: 4px;\n        color: var(--content-color);\n        background: var(--content-background-color);\n    }\n\n    .notice p:last-child {\n        margin-bottom: 0\n    }\n\n     \n    .notice-title {\n        margin: -18px -18px 12px;\n        padding: 4px 18px;\n        border-radius: 4px 4px 0 0;\n        font-weight: 700;\n        color: var(--title-color);\n        background: var(--title-background-color);\n    }\n\n     \n    .icon-notice {\n        display: inline-flex;\n        align-self: center;\n        margin-right: 8px;\n    }\n\n    .icon-notice img,\n    .icon-notice svg {\n        height: 1em;\n        width: 1em;\n        fill: currentColor;\n    }\n\n    .icon-notice img,\n    .icon-notice.baseline svg {\n        top: .125em;\n        position: relative;\n    }\n\u003c/style\u003e\u003cdiv class=\"notice proof\" \u003e\n    \u003cp class=\"notice-title\"\u003e\n        \u003cspan class=\"icon-notice baseline\"\u003e\n            \n        \u003c/span\u003eproof\u003c/p\u003e","title":"自然梯度（二）：黎曼距离下的最速下降"},{"content":"在一般的梯度下降中，我们认为目标函数梯度的负方向可以最小化一步更新后的目标函数值，这里隐含地假设了参数空间是欧氏空间，且参数构成了一组正交归一的坐标系统。在很多情况下，这一假设是不成立的，作为结果，优化过程的收敛效率可能受到影响。\n作为解决这一问题的一种思路，自然梯度使用Fisher信息矩阵（的逆）作为梯度的pre-conditioner来矫正梯度的方向。本文将分为两篇，在第一篇中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在第二篇中，我们推导自然梯度中为何引入FIM来修正梯度方向，以及自然梯度的一些性质。\nScore function与FIM 假设我们有一个由$\\theta$参数化的概率模型，模型分布为$p(x|\\theta)$，记对数似然函数为$\\ell(\\theta|x):=\\log p(x|\\theta)$。与对数似然函数相关的有两个定义，score function和fisher information。\n定义1（score function）：score function $s(\\theta|x)$被定义为对数似然函数关于参数$\\theta$的梯度\n$$ s(\\theta|x)=\\nabla_\\theta \\ell(\\theta|x) $$\n一些文章会提到score function是用来为参数的好坏打分（score），这是不严谨的。score function中的「score」其实不是为参数打分，而是在Fisher研究的遗传统计问题中给基因异常家庭的「打分」(参见：Interpretation of \u0026ldquo;score\u0026rdquo;)。因此，score function只是约定俗成的一种名称，其实质就是似然函数的梯度，描述的是似然函数对于参数变化的敏感程度。\n性质1：Score function期望为0 $$ \\mathbb{E}_{p(x|\\theta)}[s(\\theta|x)]=\\boldsymbol{0} $$\nproof\n$$ \\begin{align} \\mathbb{E}_{p(x|\\theta)}[s(\\theta|x)] \u0026amp;=\\mathbb{E}_{p(x|\\theta)}[\\nabla_\\theta \\ell(\\theta|x)]\\\\ \u0026amp;=\\int_x p(x|\\theta)\\nabla_\\theta \\log p(x|\\theta) dx\\\\ \u0026amp;=\\int_x p(x|\\theta)\\frac{\\nabla_\\theta p(x|\\theta)}{p(x|\\theta)} dx\\\\ \u0026amp;=\\int_x \\nabla_\\theta p(x|\\theta) dx\\\\ \u0026amp;=\\nabla_\\theta\\int_x p(x|\\theta) dx\\\\ \u0026amp;=\\nabla_\\theta 1 = \\boldsymbol{0}\\\\ \\end{align} $$\n通过性质1可以很顺利地引出Fisher信息矩阵的定义，由于score function是「零均值」的，因此协方差可以直接定义为score function的外积的期望。\n定义2（Fisher information matrix）：Fisher矩阵是score function的方差-协方差矩阵： $$ \\begin{align} \\boldsymbol{F}(\\theta) \u0026amp;:= \\mathbb{E}_{p(x|\\theta)}\\left[ \\left(s(\\theta|x)-\\mathbb{E}_{p(x|\\theta)}[s(\\theta|x)]\\right)\\left(s(\\theta|x)-\\mathbb{E}_{p(x|\\theta)}[s(\\theta|x)]\\right)^\\top \\right]\\\\ \u0026amp;=\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla_\\theta \\ell(\\theta|x)\\nabla_\\theta \\ell(\\theta|x)^\\top \\right]\\quad\\triangleright\\text{性质1} \\end{align} $$\nFisher信息的应用有很多，但我们主要关心它在优化方法中的应用。我们下面会证明，Fisher矩阵是分布$p(x|\\theta)$和$p(x|\\theta')$的KL散度的Hessian在$\\theta'=\\theta$处的取值。这点结论之所以重要，是因为这提供了衡量概率模型参数在函数流形上的距离的一种方法，也就是自然梯度方法的基础。\nFIM与KL散度近似 下面我们尝试通过如下路径建立FIM与KL散度的关系：首先我们证明，FIM与似然函数的Hessian的期望的负值相等（性质2），接着我们利用这一性质，进一步得到：FIM是分布$p(x|\\theta)$和$p(x|\\theta')$的KL散度的Hessian在$\\theta'=\\theta$处的取值（性质3）。通过对KL散度做二阶泰勒展开，可以使用FIM局部近似KL散度，衡量参数在概率模型函数流形上的距离，后面我们会用这个近似关系来推导自然梯度。\n性质2：FIM与似然函数的Hessian的期望的负值相等\n$$ \\boldsymbol{F}(\\theta) = -\\mathbb{E}_{p(x|\\theta)}[\\boldsymbol{H}(\\ell(\\theta|x))] $$\nproof\n$$ \\begin{align} \\boldsymbol{H}(\\ell(\\theta|x)) \u0026amp;=\\nabla^2_\\theta\\left(\\log p(x|\\theta)\\right)\\\\ \u0026amp;=\\nabla_\\theta\\left(\\frac{\\nabla_\\theta p(x|\\theta)}{p(x|\\theta)}\\right)\\\\ \u0026amp;=\\frac{p(x|\\theta)\\nabla^2_\\theta p(x|\\theta)-\\nabla_\\theta p(x|\\theta)\\nabla_\\theta p(x|\\theta)^\\top}{p(x|\\theta)^2}\\\\ \u0026amp;=\\frac{\\nabla^2_\\theta p(x|\\theta)}{p(x|\\theta)}-\\frac{\\nabla_\\theta p(x|\\theta)\\nabla_\\theta p(x|\\theta)^\\top}{p(x|\\theta)^2}\\\\ \u0026amp;=\\frac{\\boldsymbol{H}(p(x|\\theta))}{p(x|\\theta)}-\\nabla_\\theta\\log p(x|\\theta)\\nabla_\\theta\\log p(x|\\theta)^\\top\\\\ \u0026amp;=\\frac{\\boldsymbol{H}(p(x|\\theta))}{p(x|\\theta)}-\\nabla_\\theta\\ell(\\theta|x)\\nabla_\\theta\\ell(\\theta|x)^\\top\\\\ \\end{align} $$\n$$ \\begin{align} \\mathbb{E}_{p(x|\\theta)}[\\boldsymbol{H}(\\ell(\\theta|x))] \u0026amp;=\\mathbb{E}_{p(x|\\theta)}\\left[\\frac{\\boldsymbol{H}(p(x|\\theta))}{p(x|\\theta)}\\right]-\\mathbb{E}_{p(x|\\theta)}\\left[\\nabla_\\theta\\ell(\\theta|x)\\nabla_\\theta\\ell(\\theta|x)^\\top\\right]\\\\ \u0026amp;=\\int_x p(x|\\theta)\\frac{\\boldsymbol{H}(p(x|\\theta))}{p(x|\\theta)}dx-\\boldsymbol{F}(\\theta)\\\\ \u0026amp;=\\int_x \\boldsymbol{H}(p(x|\\theta))dx-\\boldsymbol{F}(\\theta)\\\\ \u0026amp;=\\boldsymbol{H}\\left(\\int_xp(x|\\theta)dx\\right)-\\boldsymbol{F}(\\theta)\\\\ \u0026amp;=\\boldsymbol{H}(1)-\\boldsymbol{F}(\\theta)\\\\ \u0026amp;=0-\\boldsymbol{F}(\\theta)\\\\ \u0026amp;=-\\boldsymbol{F}(\\theta)\\\\ \\end{align} $$\n性质3：FIM是分布$p(x|\\theta)$和$p(x|\\theta')$的KL散度的Hessian在$\\theta'=\\theta$处的取值。\n$$ \\boldsymbol{F}(\\theta)=\\nabla_{\\theta'}^2 D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)|_{\\theta'=\\theta} $$\nproof\n首先将KL散度展开： $$ \\begin{align} D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right) \u0026amp;=\\mathbb{E}_{p(x|\\theta)}\\left[ \\log\\left( \\frac{p(x|\\theta)}{p(x|\\theta')} \\right) \\right]\\\\ \u0026amp;=\\mathbb{E}_{p(x|\\theta)}\\left[ \\log\\left(p(x|\\theta) \\right) \\right]-\\mathbb{E}_{p(x|\\theta)}\\left[ \\log\\left(p(x|\\theta') \\right) \\right]\\\\ \\end{align} $$\n接着，求一阶梯度：\n$$ \\begin{align} \\nabla_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right) \u0026amp;=\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla_{\\theta'}\\log\\left(p(x|\\theta) \\right) \\right]-\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla_{\\theta'}\\log\\left(p(x|\\theta') \\right) \\right]\\\\ \u0026amp;=0-\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla_{\\theta'}\\log\\left(p(x|\\theta') \\right) \\right]\\\\ \u0026amp;=-\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla_{\\theta'}\\log\\left(p(x|\\theta') \\right) \\right]\\\\ \\end{align} $$\n继续求导：\n$$ \\begin{align} \\nabla^2_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right) \u0026amp;=-\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla^2_{\\theta'}\\log\\left(p(x|\\theta') \\right) \\right]\\\\ \u0026amp;=-\\mathbb{E}_{p(x|\\theta)}\\left[ \\boldsymbol{H}(\\ell(\\theta'|x)) \\right]\\\\ \\end{align} $$\n代入$\\theta'=\\theta$： $$ \\begin{align} \\nabla^2_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)|_{\\theta'=\\theta}\u0026amp;=-\\mathbb{E}_{p(x|\\theta)}\\left[ \\boldsymbol{H}(\\ell(\\theta|x)) \\right]\\\\ \u0026amp;=-\\boldsymbol{F}(\\theta) \\quad\\triangleright\\text{性质2} \\end{align} $$\n由于FIM是KL散度的Hessian，可以将其用于KL的近似。\n性质4：FIM可以用于KL散度的局部二阶近似： $$ D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta+\\Delta_\\theta)\\right)\\approx \\frac{1}{2}\\Delta_\\theta^\\top\\boldsymbol{F}(\\theta)\\Delta_\\theta $$\nproof\n我们记$\\theta'=\\theta+\\Delta_\\theta$ $$ \\begin{align} D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)\u0026amp;\\approx D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)|_{\\theta'=\\theta} + \\Delta_\\theta^\\top\\left(\\nabla_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)\\right)|_{\\theta'=\\theta}\\\\ \u0026amp;+\\frac{1}{2}\\Delta_\\theta^\\top\\left(\\nabla^2_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)\\right)|_{\\theta'=\\theta}\\Delta_\\theta\\\\ \u0026amp;=0 + 0 + \\frac{1}{2}\\Delta_\\theta^\\top\\boldsymbol{F}(\\theta)\\Delta_\\theta\\\\ \u0026amp;=\\frac{1}{2}\\Delta_\\theta^\\top\\boldsymbol{F}(\\theta)\\Delta_\\theta\\\\ \\end{align} $$\n其中二阶导到FIM的转化直接利用了性质3；一阶项为0是因为：\n$$ \\begin{align} \\Delta_\\theta^\\top\\left(\\nabla_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)\\right)|_{\\theta'=\\theta} \u0026amp;= \\Delta_\\theta^\\top\\left.\\nabla_{\\theta'}\\mathbb{E}_{p(x|\\theta)}\\left[\\log{\\frac{p(x|\\theta)}{p(x|\\theta')}}\\right]\\right|_{\\theta'=\\theta} \\\\ \u0026amp;= \\Delta_\\theta^\\top\\left.\\mathbb{E}_{p(x|\\theta)}\\left[\\nabla_{\\theta'}{\\log\\frac{p(x|\\theta)}{p(x|\\theta')}}\\right]\\right|_{\\theta'=\\theta}\\\\ \u0026amp;= \\Delta_\\theta^\\top\\left.\\mathbb{E}_{p(x|\\theta)}\\left[\\nabla_{\\theta'}{-\\log p(x|\\theta')}\\right]\\right|_{\\theta'=\\theta}\\\\ \u0026amp;= -\\Delta_\\theta^\\top\\left.\\mathbb{E}_{p(x|\\theta)}\\left[\\nabla_{\\theta'}{\\log p(x|\\theta')}\\right]\\right|_{\\theta'=\\theta} \\\\ \u0026amp;= 0\\quad\\triangleright\\text{性质1} \\end{align} $$\nFIM作为黎曼度量 在建立了FIM可用于KL散度近似的结论之后，我们将其放到一个新的框架里来审视——这种近似关系实际上定义了一种黎曼度量。\n在黎曼几何中，度量张量（metric tensor）提供了一种将坐标值转化为距离（或内积）的工具。假设在流形上的给定点$p$的切空间$T_pM$上定义了一组基向量$\\{e_1,\\cdots,e_n\\}$，黎曼度量可以定义为基向量两两之间的内积： $$ g_{ij} = \\langle \\boldsymbol{e}_i, \\boldsymbol{e}_j\\rangle $$\n对于任意切向量$\\boldsymbol{u},\\boldsymbol{v}\\in T_pM$，可以利用这个黎曼度量来定义切向量的内积：\n$$ \\begin{align} \\langle \\boldsymbol{u}, \\boldsymbol{v}\\rangle _p \u0026amp;= \\langle \\sum_i u_i\\boldsymbol{e}_i, \\sum_j v_j\\boldsymbol{e}_j\\rangle _p\\\\ \u0026amp;=\\sum_{ij}u_i v_j \\langle \\boldsymbol{e}_i, \\boldsymbol{e}_j\\rangle\\\\ \u0026amp;=\\sum_{ij}u_i v_j g_{ij}\\\\ \u0026amp;= \\boldsymbol{u}^\\top \\boldsymbol{G}\\boldsymbol{v} \\end{align} $$\n其中$(\\boldsymbol{G})_{ij}=g_{ij}$，即将度量组织成一个矩阵的形式。基于这个内积可以衍生出$p$点附近的微小距离（线元）的计算，考虑$p$点上的一个微小位移$\\boldsymbol{\\delta}$，则对应的线元为： $$ |\\boldsymbol{\\delta}|^2 = \\boldsymbol{\\delta}^\\top\\boldsymbol{G}\\boldsymbol{\\delta} $$\n作为一个特例，考虑常见的欧氏空间，由于基向量都是规范正交的，任意两个不同基向量的内积是0，且基向量与自身的内积为1，因此对应的度量张量是$\\boldsymbol{G}=\\boldsymbol{I}$，这一点对于任意点都成立。因此欧氏空间对应的内积总是$\\boldsymbol{u}^\\top \\boldsymbol{v}$，线元总是$|\\boldsymbol{\\delta}|^2 = \\boldsymbol{\\delta}^\\top\\boldsymbol{\\delta}$。\n回顾上一节推导得到的性质4（$D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta+\\Delta_\\theta)\\right)\\approx \\frac{1}{2}\\Delta_\\theta^\\top\\boldsymbol{F}(\\theta)\\Delta_\\theta$），通过对比上述的线元的计算，可以发现二者的形式是一致的。\n现在，如果我们将不同参数$\\theta$实例化的分布族$\\mathcal{F}=\\{p_\\theta:\\theta\\in \\Theta\\}$拓展为一个流形$\\mathcal{M}$，使得流形$\\mathcal{M}$上的点与分布族$\\mathcal{F}$中的分布构成双射（一一对应关系）。这样，分布中的参数$\\theta$可以看做流形上的坐标。如果我们将参数的距离定义为参数引导的概率分布的KL散度1，则由性质4的近似，$\\boldsymbol{F}(\\theta)$可以看做点$\\theta$的黎曼度量张量。\n在下篇文章中，我们会通过泛化标准梯度下降的距离约束条件为黎曼距离，从而得到自然梯度方法的更新公式，并给出这种新的约束下对应的一些性质。\n这里的距离是广义而言的，因为KL散度不满足距离的约定。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://nil9.net/posts/fisher-info-matrix/","summary":"\u003cp\u003e在一般的梯度下降中，我们认为目标函数梯度的负方向可以最小化一步更新后的目标函数值，这里隐含地假设了参数空间是欧氏空间，且参数构成了一组正交归一的坐标系统。在很多情况下，这一假设是不成立的，作为结果，优化过程的收敛效率可能受到影响。\u003c/p\u003e\n\u003cp\u003e作为解决这一问题的一种思路，自然梯度使用Fisher信息矩阵（的逆）作为梯度的pre-conditioner来矫正梯度的方向。本文将分为两篇，在第一篇中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在第二篇中，我们推导自然梯度中为何引入FIM来修正梯度方向，以及自然梯度的一些性质。\u003c/p\u003e\n\u003ch1 id=\"score-function与fim\"\u003eScore function与FIM\u003c/h1\u003e\n\u003cp\u003e假设我们有一个由\u003ccode\u003e$\\theta$\u003c/code\u003e参数化的概率模型，模型分布为\u003ccode\u003e$p(x|\\theta)$\u003c/code\u003e，记对数似然函数为\u003ccode\u003e$\\ell(\\theta|x):=\\log p(x|\\theta)$\u003c/code\u003e。与对数似然函数相关的有两个定义，score function和fisher information。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e定义1（score function）\u003c/strong\u003e：score function \u003ccode\u003e$s(\\theta|x)$\u003c/code\u003e被定义为对数似然函数关于参数\u003ccode\u003e$\\theta$\u003c/code\u003e的梯度\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ s(\\theta|x)=\\nabla_\\theta \\ell(\\theta|x) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e一些文章会提到score function是用来为参数的好坏打分（score），这是不严谨的。score function中的「score」其实不是为参数打分，而是在Fisher研究的遗传统计问题中给基因异常家庭的「打分」(参见：\u003ca href=\"https://stats.stackexchange.com/questions/326091/interpretation-of-score\"\u003eInterpretation of \u0026ldquo;score\u0026rdquo;\u003c/a\u003e)。因此，score function只是约定俗成的一种名称，其实质就是似然函数的梯度，描述的是似然函数对于参数变化的敏感程度。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e性质1\u003c/strong\u003e：Score function期望为0\n\u003ccode\u003e$$ \\mathbb{E}_{p(x|\\theta)}[s(\\theta|x)]=\\boldsymbol{0} $$\u003c/code\u003e\u003c/p\u003e\n\u003cstyle type=\"text/css\"\u003e\n     \n    .notice {\n        --title-color: #fff;\n        --title-background-color: #6be;\n        --content-color: #444;\n        --content-background-color: #e7f2fa;\n    }\n\n    .notice.proof {\n        --title-background-color: rgb(130, 130, 130);\n        --content-background-color: #f7f7f7;\n    }\n\n    .notice.info {\n        --title-background-color: #fb7;\n        --content-background-color: #fec;\n    }\n\n    .notice.tip {\n        --title-background-color: #5a5;\n        --content-background-color: #efe;\n    }\n\n    .notice.warning {\n        --title-background-color: #c33;\n        --content-background-color: #fee;\n    }\n\n     \n\n    body.dark .notice {\n        --title-color: #fff;\n        --title-background-color: #069;\n        --content-color: #ddd;\n        --content-background-color: #023;\n    }\n\n    body.dark .notice.proof {\n        --title-background-color: rgb(129, 129, 129);\n        --content-background-color: rgb(41, 41, 41);\n    }\n\n    body.dark .notice.info {\n        --title-background-color: #a50;\n        --content-background-color: #420;\n    }\n\n    body.dark .notice.tip {\n        --title-background-color: #363;\n        --content-background-color: #121;\n    }\n\n    body.dark .notice.warning {\n        --title-background-color: #800;\n        --content-background-color: #400;\n    }\n\n     \n    .notice {\n        padding: 18px;\n        line-height: 24px;\n        margin-bottom: 24px;\n        border-radius: 4px;\n        color: var(--content-color);\n        background: var(--content-background-color);\n    }\n\n    .notice p:last-child {\n        margin-bottom: 0\n    }\n\n     \n    .notice-title {\n        margin: -18px -18px 12px;\n        padding: 4px 18px;\n        border-radius: 4px 4px 0 0;\n        font-weight: 700;\n        color: var(--title-color);\n        background: var(--title-background-color);\n    }\n\n     \n    .icon-notice {\n        display: inline-flex;\n        align-self: center;\n        margin-right: 8px;\n    }\n\n    .icon-notice img,\n    .icon-notice svg {\n        height: 1em;\n        width: 1em;\n        fill: currentColor;\n    }\n\n    .icon-notice img,\n    .icon-notice.baseline svg {\n        top: .125em;\n        position: relative;\n    }\n\u003c/style\u003e\u003cdiv class=\"notice proof\" \u003e\n    \u003cp class=\"notice-title\"\u003e\n        \u003cspan class=\"icon-notice baseline\"\u003e\n            \n        \u003c/span\u003eproof\u003c/p\u003e","title":"自然梯度（一）：Fisher信息矩阵作为黎曼度量"},{"content":"Tied embeddings，即将语言模型中的输入Embeddings权重与输出分类器的权重两组参数共享的操作，一度是语言建模和机器翻译任务的标准配置。在语言模型大规模化之后，这种设计在开源模型中愈发少见了。前几天看到@苏剑林 之前的一篇博客语言模型输出端共享Embedding的重新探索，为tied embeddings的消失提供了一种视角，但也还有值得商榷的地方，本文想从这篇文章出发做一点探讨。\n初始Loss的视角 这里先简要概括一下苏老师文章中的阐述框架1。在使用Transformer做语言建模的时候，可能会使用类似DeepNorm等初始化手段，从而使每一个Transformer Block接近于一个恒等映射2，同时由于词元表征是0均值的，因此LayerNorm可以看做与RMSNorm等价3。所以，假设每个残差分支都初始化为0，假设输入中某个位置的初始embedding是$\\boldsymbol{w}_i$（对应词表中的第$i$个词，维度是$d$），那么最终得到的表征满足\n$$ \\frac{\\boldsymbol{w}_i}{\\Vert\\boldsymbol{w}_i\\Vert \\big/\\sqrt{d}} \\approx \\frac{\\boldsymbol{w}_i}{\\sigma} $$\n假设在该位置的真实标签是词元$j$，则损失函数可以由如下逼近\n$$ \\begin{align}\\mathcal{L}\\triangleq -\\log p(j|i) \u0026amp;= \\log \\sum\\limits_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma} - \\boldsymbol{w}_i\\cdot \\boldsymbol{w}_j \\big/ \\sigma \\\\ \u0026amp;\\approx \\log \\sum_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma}\\\\ \u0026amp;=\\log \\left(e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_i / \\sigma} + \\sum\\limits_{k|k\\neq i} e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma}\\right)\\\\ \u0026amp;\\approx\\log \\left({\\color[rgb]{0, 0.5, 0.8}e^{d \\sigma}} + (n-1)\\right) \\end{align} $$\n其中$|n|$是词表大小。在常见的模型维度下，这里的第一项${\\color[rgb]{0, 0.5, 0.8}e^{d \\sigma}}$是比较大的。我们可以代入几个维度值看下第一项的大小，这里我们假设词表大小是32k，并考虑两种$\\sigma$取法，一种是比较常见的初始化超参数$\\sigma=0.02$，一种是取$\\sigma=1/\\sqrt{d}$。可以看到无论是哪种初始化方法，对应的${\\color[rgb]{0, 0.5, 0.8}e^{d \\sigma}}$都已经远远超过词表大小，响应地初始损失值也处于比较高的水平（按均匀分布的交叉熵是$\\log(n)\\approx 10.37$）。\n不同设定下的「初始损失值」\n以上是苏文中给出的关于语言建模中不再共享embedding的一个视角——tied embeddings会使语言模型的初始损失值很大。\n但这个问题实际上可以用一个rescale来解决，我们可以简单地将输出端乘以$1/\\sqrt{d}$，则损失函数可以做如下近似 $$ \\begin{align}\\mathcal{L} \u0026amp;= \\log \\sum\\limits_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / (\\sigma\\sqrt{d})} - \\boldsymbol{w}_i\\cdot \\boldsymbol{w}_j \\big/(\\sigma\\sqrt{d}) \\\\ \u0026amp;\\approx\\log \\left({\\color[rgb]{0, 0.5, 0.8}e^{\\sigma\\sqrt{d}}} + (n-1)\\right) \\end{align} $$\n这时候，对于常见的$\\sigma=1/\\sqrt{d}$或者$\\sigma=0.02$，${\\color[rgb]{0, 0.5, 0.8}e^{\\sigma\\sqrt{d}}}$这一项相对于词表大小都可以忽略不计了。\n事实上，早期共享embeddings的预训练模型T5的实现4中就使用了这个技巧：\nif self.shared_embedding_and_softmax_weights: logits = mtf.einsum( [x * (self.model_dim.size ** -0.5), embedding_weights], reduced_dims=[self.model_dim]) 另外，在一些公开的预训练实现中，一般残差项的初始化不会使用特别小的值，例如在OLMo-2中，就是对残差分支中的各个参数矩阵就是直接用了标准差为0.02的truncated normal初始化。\n我们可以使用Llama的模型结构做一个简单的实验，我们使用常见的正态分布初始化，在固定层数为12的情况下，测试不同embedding维度下的初始loss值，结果如下表所示。\n初始loss 768 1024 2048 4096 $\\log(n)$ 10.37 - - - untied 10.52 10.56 10.78 11.19 tied 10.53 10.58 10.77 11.24 untied+rescale 10.37 10.37 10.37 10.37 tied+rescale 10.37 10.37 10.37 10.37 可以看到，\n无论是否应用tied embeddings，初始loss都有略高于$\\log(n)$的情况； 在输出端应用rescale技巧，可以将初始loss控制在$\\log(n)$左右。 寻根溯源 笔者认为，初始Loss虽然是一个非常好的视角，但是不能解释当前tied embeddings的式微。讨论tied embeddings的应用，还得稍微追溯学术史，先看看他们是为何被提出的。\n在语言建模中引入tied embeddings技巧可以追溯到LSTM-LM时代的两篇工作：Inan 2016.和Press and Wolf 2017.。其中，Inan 2016.通过类似KD的框架构造出一种soft label\n$$ \\begin{aligned} \\boldsymbol{u}_t \u0026amp;= \\boldsymbol{L}\\boldsymbol{y}^{*}_t \\\\ \\tilde{\\boldsymbol{y}}_t \u0026amp;= \\text{softmax}(\\frac{\\boldsymbol{L}^\\top \\boldsymbol{u}_t}{\\tau}) \\end{aligned} $$\n这里$\\boldsymbol{L}, \\boldsymbol{y}^{*}_t$分别表示embedding权重和第$t$个位置的目标词元。作者论证了在一定的假设下，tied embeddings设定的语言模型相当于在隐式地学习这个soft label（而不是一般的one-hot目标）。\nPress and Wolf 2017.则是通过一系列实验论证了如下几个结论5：\nRNNLM使用tied embeddings时，embeddings的演进方式更接近与untied版本中输出端的embeddings； 使用tied embeddings可以有效降低语言建模中的PPL（PTB数据集），无论是否使用dropout均成立； 在不使用dropout的情况下，在输出embedding之前添加一个额外的投影$P$，并对$P$添加正则化loss，可以进一步降低PPL指标。 这篇文章还提出在机器翻译模型中，对于en-fr这样比较相似的语言，可以在两个语言的语料合集上联合训练一个tokenizer，共享encoder与decoder的embeddings（即encoder的输入、decoder的输入与输出共享一个参数矩阵），后来的Transformer(Vaswani 2017.)也沿用了这一做法。笔者认为这就是初期的很多预训练模型都不约而同地沿用tied embeddings的设定的原因。\n但是如今回顾这两篇文章的时候，我们注意到几点：\n当时的语言模型一般基于浅层的RNN，输入与输出的embeddings参数在模型中占比很大； 当时的实验基于PTB和WikiText数据集，相对于如今的预训练语料规模，可谓是非常小了，尤其是前者。 笔者认为，tied embeddings的有效性与数据和模型规模离不开关系。当数据与模型的规模比较有限时，tied embeddings可以作为一种很好的正则化手段（显著降低参数数量），从Press and Wolf 2017.的实验来看，在PTB这样的小数据集上，tied embeddings的语言模型在训练集上的PPL并不占优势，这表明它的作用可能有部分来自于过拟合风险的降低。\n现在的LLM模做规模化主要是通过加大隐藏层维度和模型层数，non-embedding部分的参数量按$\\mathcal{O}(Ld^2)$的级别增长，而embeddings的参数量只随着隐藏层维度线性增长，因此现有的LLM的embeddings所占参数比例已经非常小了，通过tied embeddings减少参数量的作用非常有限。另外，现在的预训练语料的词元规模也通常在万亿这个量级，与PTB这种训练集不到一百万词的数据集已经不能同日而语了。\n训练的不稳定、工程的限制 前面我们提到，tied embeddings是源于数据与模型规模都较小的LSTMLM时期的一种正则化方法，逐渐成为一项标准设定，在预训练的早期也被沿用了下来。如今在数据与模型规模化的趋势下，正则化的强问题意识已经逐渐不成立了，这种强正则甚至可能成为训练的负担。例如，在OLMo的talk中作者提到，tied embeddings在7B的模型中会造成训练的不稳定。\n除此之外，在语言模型规模化以后，模型的训练越来越依赖于各种跨节点并行计算方法。而使用tied embeddings实际上对并行方法的选择也有一定的限制。例如，使用流水线并行（Pipeline Parallelism）要求将模型纵向拆分部署在多个节点上，那么此时如果将输入与输出层看做两个不同的层，部署在不同的节点上，则首先这两部分参数共享不会节约任何的存储，还需要付出额外的通信成本来同步两个层的梯度。不过笔者觉得这个原因是次要的，如果收益是正向的，那么额外的同步步骤也是值得的。\n结语 本文从语言模型输出端共享Embedding的重新探索中的初始loss视角出发，拓展讨论了在语言建模规模化之后，tied embeddings操作不再作为标准设定的原因：模型与数据规模的变化使得正则化的问题意识不再，且从一些公开的实验来看，tied embeddings可能引发训练的不稳定6，此外tied embeddings也对并行方法的选型有一定限制。\n拓展阅读 Inan 2016. Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling Press and Wolf 2017. Using the Output Embedding to Improve Language Models 语言模型输出端共享Embedding的重新探索 详细内容请查看原文。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n除了DeepNorm，ReZero等优化也有类似的思想。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n在常见实现中，LayerNorm在初始化时，$\\gamma,\\beta$参数分别被初始化为1和0.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n这里略过关于embedding similarity测验的结论。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n在深度学习领域，经验结论很重要，尤其是对于LLM这样试错成本较高的应用中。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://nil9.net/posts/tied-embeddings-in-lm/","summary":"\u003cp\u003eTied embeddings，即将语言模型中的输入Embeddings权重与输出分类器的权重两组参数共享的操作，一度是语言建模和机器翻译任务的标准配置。在语言模型大规模化之后，这种设计在开源模型中愈发少见了。前几天看到\u003ca href=\"https://www.zhihu.com/people/su-jian-lin-22\"\u003e@苏剑林\u003c/a\u003e 之前的一篇博客\u003ca href=\"https://kexue.fm/archives/9698\"\u003e语言模型输出端共享Embedding的重新探索\u003c/a\u003e，为tied embeddings的消失提供了一种视角，但也还有值得商榷的地方，本文想从这篇文章出发做一点探讨。\u003c/p\u003e\n\u003ch1 id=\"初始loss的视角\"\u003e初始Loss的视角\u003c/h1\u003e\n\u003cp\u003e这里先简要概括一下苏老师文章中的阐述框架\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e。在使用Transformer做语言建模的时候，可能会使用类似DeepNorm等初始化手段，从而使每一个Transformer Block接近于一个恒等映射\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e，同时由于词元表征是0均值的，因此LayerNorm可以看做与RMSNorm等价\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e。所以，假设每个残差分支都初始化为0，假设输入中某个位置的初始embedding是\u003ccode\u003e$\\boldsymbol{w}_i$\u003c/code\u003e（对应词表中的第\u003ccode\u003e$i$\u003c/code\u003e个词，维度是\u003ccode\u003e$d$\u003c/code\u003e），那么最终得到的表征满足\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\frac{\\boldsymbol{w}_i}{\\Vert\\boldsymbol{w}_i\\Vert \\big/\\sqrt{d}} \\approx \\frac{\\boldsymbol{w}_i}{\\sigma} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e假设在该位置的真实标签是词元\u003ccode\u003e$j$\u003c/code\u003e，则损失函数可以由如下逼近\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{align}\\mathcal{L}\\triangleq -\\log p(j|i) \u0026amp;= \\log \\sum\\limits_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma} - \\boldsymbol{w}_i\\cdot \\boldsymbol{w}_j \\big/ \\sigma \\\\ \u0026amp;\\approx \\log \\sum_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma}\\\\ \u0026amp;=\\log \\left(e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_i / \\sigma} + \\sum\\limits_{k|k\\neq i} e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma}\\right)\\\\ \u0026amp;\\approx\\log \\left({\\color[rgb]{0, 0.5, 0.8}e^{d \\sigma}} + (n-1)\\right) \\end{align} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中\u003ccode\u003e$|n|$\u003c/code\u003e是词表大小。在常见的模型维度下，这里的第一项\u003ccode\u003e${\\color[rgb]{0, 0.5, 0.8}e^{d \\sigma}}$\u003c/code\u003e是比较大的。我们可以代入几个维度值看下第一项的大小，这里我们假设词表大小是32k，并考虑两种\u003ccode\u003e$\\sigma$\u003c/code\u003e取法，一种是比较常见的初始化超参数\u003ccode\u003e$\\sigma=0.02$\u003c/code\u003e，一种是取\u003ccode\u003e$\\sigma=1/\\sqrt{d}$\u003c/code\u003e。可以看到无论是哪种初始化方法，对应的\u003ccode\u003e${\\color[rgb]{0, 0.5, 0.8}e^{d \\sigma}}$\u003c/code\u003e都已经远远超过词表大小，响应地初始损失值也处于比较高的水平（按均匀分布的交叉熵是\u003ccode\u003e$\\log(n)\\approx 10.37$\u003c/code\u003e）。\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\n  \u003cimg loading=\"lazy\" src=\"/images/tied-embeddings-in-lm/init_loss.png\" title=\"不同设定下的「初始损失值」\"\u003e\u003cfigcaption class=\"image-caption\"\u003e不同设定下的「初始损失值」\u003c/figcaption\u003e\u003c/figure\u003e\u003c/p\u003e\n\u003cp\u003e以上是苏文中给出的关于语言建模中不再共享embedding的一个视角——tied embeddings会使语言模型的初始损失值很大。\u003c/p\u003e","title":"关于语言建模中的Tied Embeddings的一点探讨"}]