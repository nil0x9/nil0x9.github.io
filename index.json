[{"content":"Tied embeddings，即将语言模型中的输入Embeddings权重与输出分类器的权重两组参数共享的操作，一度是语言建模和机器翻译任务的标准配置。在语言模型大规模化之后，这种设计在开源模型中愈发少见了。前几天看到@苏剑林 之前的一篇博客语言模型输出端共享Embedding的重新探索，为tied embeddings的消失提供了一种视角，但也还有值得商榷的地方，本文想从这篇文章出发做一点探讨。\n初始Loss的视角 这里先简要概括一下苏老师文章中的阐述框架1。在使用Transformer做语言建模的时候，可能会使用类似DeepNorm等初始化手段，从而使每一个Transformer Block接近于一个恒等映射2，同时由于词元表征是0均值的，因此LayerNorm可以看做与RMSNorm等价3。所以，假设每个残差分支都初始化为0，假设输入中某个位置的初始embedding是$\\boldsymbol{w}_i$（对应词表中的第$i$个词，维度是$d$），那么最终得到的表征满足\n$$ \\frac{\\boldsymbol{w}_i}{\\Vert\\boldsymbol{w}_i\\Vert \\big/\\sqrt{d}} \\approx \\frac{\\boldsymbol{w}_i}{\\sigma} $$\n假设在该位置的真实标签是词元$j$，则损失函数可以由如下逼近\n$$ \\begin{align}\\mathcal{L}\\triangleq -\\log p(j|i) \u0026amp;= \\log \\sum\\limits_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma} - \\boldsymbol{w}_i\\cdot \\boldsymbol{w}_j \\big/ \\sigma \\\\ \u0026amp;\\approx \\log \\sum_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma}\\\\ \u0026amp;=\\log \\left(e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_i / \\sigma} + \\sum\\limits_{k|k\\neq i} e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma}\\right)\\\\ \u0026amp;\\approx\\log \\left({\\color{blue}e^{d \\sigma}} + (n-1)\\right) \\end{align} $$\n其中$|n|$是词表大小。在常见的模型维度下，这里的第一项${\\color{blue}e^{d \\sigma}}$是比较大的。我们可以代入几个维度值看下第一项的大小，这里我们假设词表大小是32k，并考虑两种$\\sigma$取法，一种是比较常见的初始化超参数$\\sigma=0.02$，一种是取$\\sigma=1/\\sqrt{d}$。可以看到无论是哪种初始化方法，对应的${\\color{blue}e^{d \\sigma}}$都已经远远超过词表大小，响应地初始损失值也处于比较高的水平（按均匀分布的交叉熵是$\\log(n)\\approx 10.37$）。\n不同设定下的「初始损失值」\n以上是苏文中给出的关于语言建模中不再共享embedding的一个视角——tied embeddings会使语言模型的初始损失值很大。\n但这个问题实际上可以用一个rescale来解决，我们可以简单地将输出端乘以$1/\\sqrt{d}$，则损失函数可以做如下近似 $$ \\begin{align}\\mathcal{L} \u0026amp;= \\log \\sum\\limits_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / (\\sigma\\sqrt{d})} - \\boldsymbol{w}_i\\cdot \\boldsymbol{w}_j \\big/(\\sigma\\sqrt{d}) \\\\ \u0026amp;\\approx\\log \\left({\\color{blue}e^{\\sigma\\sqrt{d}}} + (n-1)\\right) \\end{align} $$\n这时候，对于常见的$\\sigma=1/\\sqrt{d}$或者$\\sigma=0.02$，${\\color{blue}e^{\\sigma\\sqrt{d}}}$这一项相对于词表大小都可以忽略不计了。\n事实上，早期共享embeddings的预训练模型T5的实现4中就使用了这个技巧：\nif self.shared_embedding_and_softmax_weights: logits = mtf.einsum( [x * (self.model_dim.size ** -0.5), embedding_weights], reduced_dims=[self.model_dim]) 另外，在一些公开的预训练实现中，一般残差项的初始化不会使用特别小的值，例如在OLMo-2中，就是对残差分支中的各个参数矩阵就是直接用了标准差为0.02的truncated normal初始化。\n我们可以使用Llama的模型结构做一个简单的实验，我们使用常见的正态分布初始化，在固定层数为12的情况下，测试不同embedding维度下的初始loss值，结果如下表所示。\n初始loss 768 1024 2048 4096 $\\log(n)$ 10.37 - - - untied 10.52 10.56 10.78 11.19 tied 10.53 10.58 10.77 11.24 untied+rescale 10.37 10.37 10.37 10.37 tied+rescale 10.37 10.37 10.37 10.37 可以看到，\n无论是否应用tied embeddings，初始loss都有略高于$\\log(n)$的情况； 在输出端应用rescale技巧，可以将初始loss控制在$\\log(n)$左右。 寻根溯源 笔者认为，初始Loss虽然是一个非常好的视角，但是不能解释当前tied embeddings的式微。讨论tied embeddings的应用，还得稍微追溯学术史，先看看他们是为何被提出的。\n在语言建模中引入tied embeddings技巧可以追溯到LSTM-LM时代的两篇工作：Inan 2016.和Press and Wolf 2017.。其中，第一篇文章通过类似KD的框架构造出的\n分布式训练 结语 参考 Inan 2016. Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling Press and Wolf 2017. Using the Output Embedding to Improve Language Models 语言模型输出端共享Embedding的重新探索 关于LLM结构中Tied Embedding的相关思考 详细内容请查看原文。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n除了DeepNorm，ReZero等优化也有类似的思想。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n在常见实现中，LayerNorm在初始化时，$\\gamma,\\beta$参数分别被初始化为1和0.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://nil9.net/posts/tied-embeddings-in-lm/","summary":"\u003cp\u003eTied embeddings，即将语言模型中的输入Embeddings权重与输出分类器的权重两组参数共享的操作，一度是语言建模和机器翻译任务的标准配置。在语言模型大规模化之后，这种设计在开源模型中愈发少见了。前几天看到\u003ca href=\"https://www.zhihu.com/people/su-jian-lin-22\"\u003e@苏剑林\u003c/a\u003e 之前的一篇博客\u003ca href=\"https://kexue.fm/archives/9698\"\u003e语言模型输出端共享Embedding的重新探索\u003c/a\u003e，为tied embeddings的消失提供了一种视角，但也还有值得商榷的地方，本文想从这篇文章出发做一点探讨。\u003c/p\u003e\n\u003ch1 id=\"初始loss的视角\"\u003e初始Loss的视角\u003c/h1\u003e\n\u003cp\u003e这里先简要概括一下苏老师文章中的阐述框架\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e。在使用Transformer做语言建模的时候，可能会使用类似DeepNorm等初始化手段，从而使每一个Transformer Block接近于一个恒等映射\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e，同时由于词元表征是0均值的，因此LayerNorm可以看做与RMSNorm等价\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e。所以，假设每个残差分支都初始化为0，假设输入中某个位置的初始embedding是\u003ccode\u003e$\\boldsymbol{w}_i$\u003c/code\u003e（对应词表中的第\u003ccode\u003e$i$\u003c/code\u003e个词，维度是\u003ccode\u003e$d$\u003c/code\u003e），那么最终得到的表征满足\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\frac{\\boldsymbol{w}_i}{\\Vert\\boldsymbol{w}_i\\Vert \\big/\\sqrt{d}} \\approx \\frac{\\boldsymbol{w}_i}{\\sigma} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e假设在该位置的真实标签是词元\u003ccode\u003e$j$\u003c/code\u003e，则损失函数可以由如下逼近\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{align}\\mathcal{L}\\triangleq -\\log p(j|i) \u0026amp;= \\log \\sum\\limits_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma} - \\boldsymbol{w}_i\\cdot \\boldsymbol{w}_j \\big/ \\sigma \\\\ \u0026amp;\\approx \\log \\sum_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma}\\\\ \u0026amp;=\\log \\left(e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_i / \\sigma} + \\sum\\limits_{k|k\\neq i} e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma}\\right)\\\\ \u0026amp;\\approx\\log \\left({\\color{blue}e^{d \\sigma}} + (n-1)\\right) \\end{align} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中\u003ccode\u003e$|n|$\u003c/code\u003e是词表大小。在常见的模型维度下，这里的第一项\u003ccode\u003e${\\color{blue}e^{d \\sigma}}$\u003c/code\u003e是比较大的。我们可以代入几个维度值看下第一项的大小，这里我们假设词表大小是32k，并考虑两种\u003ccode\u003e$\\sigma$\u003c/code\u003e取法，一种是比较常见的初始化超参数\u003ccode\u003e$\\sigma=0.02$\u003c/code\u003e，一种是取\u003ccode\u003e$\\sigma=1/\\sqrt{d}$\u003c/code\u003e。可以看到无论是哪种初始化方法，对应的\u003ccode\u003e${\\color{blue}e^{d \\sigma}}$\u003c/code\u003e都已经远远超过词表大小，响应地初始损失值也处于比较高的水平（按均匀分布的交叉熵是\u003ccode\u003e$\\log(n)\\approx 10.37$\u003c/code\u003e）。\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\n  \u003cimg loading=\"lazy\" src=\"/images/tied-embeddings-in-lm/init_loss.png\" title=\"不同设定下的「初始损失值」\"\u003e\u003cfigcaption class=\"image-caption\"\u003e不同设定下的「初始损失值」\u003c/figcaption\u003e\u003c/figure\u003e\u003c/p\u003e\n\u003cp\u003e以上是苏文中给出的关于语言建模中不再共享embedding的一个视角——tied embeddings会使语言模型的初始损失值很大。\u003c/p\u003e\n\u003cp\u003e但这个问题实际上可以用一个rescale来解决，我们可以简单地将输出端乘以\u003ccode\u003e$1/\\sqrt{d}$\u003c/code\u003e，则损失函数可以做如下近似\n\u003ccode\u003e$$ \\begin{align}\\mathcal{L} \u0026amp;= \\log \\sum\\limits_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / (\\sigma\\sqrt{d})} - \\boldsymbol{w}_i\\cdot \\boldsymbol{w}_j  \\big/(\\sigma\\sqrt{d}) \\\\ \u0026amp;\\approx\\log \\left({\\color{blue}e^{\\sigma\\sqrt{d}}} + (n-1)\\right) \\end{align} $$\u003c/code\u003e\u003c/p\u003e","title":"关于语言建模中的Tied Embeddings的一点探讨"},{"content":"$\\sin(x)$是第一个被求反函数的。当时$\\sin \\cos \\tan \\cot$接到通知来讨论三角函数的周期性问题，$\\sin(x)$刚到初等函数大厅，门口的守卫把$\\sin(x)$身边的常数拦了下来，$\\sin(x)$感觉到事情不太对劲，但停顿一下后，还是走了进去。\n当$\\sin(x)$走到走廊的时候，多项式函数一拥而上，立刻就扭住了它的双手，$\\sin(x)$还没有明白自己已经被求了反函数，还在大声斥责多项式函数：“你们干什么，我是来讨论周期性的！”\n随后，$\\sin(x)$被控制住定义域映射到了大厅里，$\\ln(x+1)$站起来，宣布了$\\arcsin(x)$的值域默认为$[-0.5\\pi,+0.5\\pi]$，然而未等对数函数念完$\\arccos \\arctan \\arccot$的值域，$\\sin(x)$突然挣脱多项式函数的束缚，大吼一声，扑向$e^x$。$\\sin(x)$ 在[-1,1]上震荡取值，而$e^x\u0026gt;0$，可导可积，性质十分优良，要是让$\\sin(x)$破坏了单调性，那还得了？\nshared\n# https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586 if self.shared_embedding_and_softmax_weights: logits = mtf.einsum( [x * (self.model_dim.size ** -0.5), embedding_weights], reduced_dims=[self.model_dim]) ","permalink":"https://nil9.net/posts/test-post/","summary":"\u003cp\u003e\u003ccode\u003e$\\sin(x)$\u003c/code\u003e是第一个被求反函数的。当时\u003ccode\u003e$\\sin \\cos \\tan \\cot$\u003c/code\u003e接到通知来讨论三角函数的周期性问题，\u003ccode\u003e$\\sin(x)$\u003c/code\u003e刚到初等函数大厅，门口的守卫把\u003ccode\u003e$\\sin(x)$\u003c/code\u003e身边的常数拦了下来，\u003ccode\u003e$\\sin(x)$\u003c/code\u003e感觉到事情不太对劲，但停顿一下后，还是走了进去。\u003c/p\u003e\n\u003cp\u003e当\u003ccode\u003e$\\sin(x)$\u003c/code\u003e走到走廊的时候，多项式函数一拥而上，立刻就扭住了它的双手，\u003ccode\u003e$\\sin(x)$\u003c/code\u003e还没有明白自己已经被求了反函数，还在大声斥责多项式函数：“你们干什么，我是来讨论周期性的！”\u003c/p\u003e\n\u003cp\u003e随后，\u003ccode\u003e$\\sin(x)$\u003c/code\u003e被控制住定义域映射到了大厅里，\u003ccode\u003e$\\ln(x+1)$\u003c/code\u003e站起来，宣布了\u003ccode\u003e$\\arcsin(x)$\u003c/code\u003e的值域默认为\u003ccode\u003e$[-0.5\\pi,+0.5\\pi]$\u003c/code\u003e，然而未等对数函数念完\u003ccode\u003e$\\arccos \\arctan \\arccot$\u003c/code\u003e的值域，\u003ccode\u003e$\\sin(x)$\u003c/code\u003e突然挣脱多项式函数的束缚，大吼一声，扑向\u003ccode\u003e$e^x$\u003c/code\u003e。\u003ccode\u003e$\\sin(x)$\u003c/code\u003e 在[-1,1]上震荡取值，而\u003ccode\u003e$e^x\u0026gt;0$\u003c/code\u003e，可导可积，性质十分优良，要是让\u003ccode\u003e$\\sin(x)$\u003c/code\u003e破坏了单调性，那还得了？\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eshared\u003c/code\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e# https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eshared_embedding_and_softmax_weights:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      logits \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e mtf\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eeinsum(\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e          [x \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e (self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emodel_dim\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003esize \u003cspan style=\"color:#f92672\"\u003e**\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e-\u003c/span\u003e\u003cspan style=\"color:#ae81ff\"\u003e0.5\u003c/span\u003e), embedding_weights],\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e          reduced_dims\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e[self\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003emodel_dim])\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"单纯测试一下mathjax"}]