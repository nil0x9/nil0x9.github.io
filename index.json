[{"content":"最近在细读Tensor Programs的时候，发现一个有意思的细节，Greg 2022.在将mup应用到Transformer的时候，对Attention Logits使用$1/d_k$进行放缩，而不是传统上的$1/\\sqrt{d_k}$。\n我不禁回想起很多年前在知乎的一个问题：\ntransformer中的attention为什么scaled? - Nil-9的回答\n当时笔者沿着Vaswani 2017.的一个脚注，展开写了这样一篇回答，其数学原理并不复杂，浓缩版本是这样的：\n假设$\\boldsymbol{q}$,$\\boldsymbol{k}\\in\\mathbb{R}^{d_k}$是相互独立的随机变量，均值和方差分别是0、1，则点积$\\langle\\boldsymbol{q},\\boldsymbol{k}\\rangle$的量级是$\\Theta(\\sqrt{d_k})$。为了防止点积过大导致softmax出现梯度消失问题，因此需要再点积基础上乘上$1/\\sqrt{d_k}$，将点积稳定在$\\Theta(1)$量级。\n这个问题后来也成为了某些公司的所谓「机器学习八股文」之一。但这个放缩是不是严谨的呢？我们来仔细思考一下。\n这里比较成问题的是「相互独立」的假设。我们先建立如下的基本的结论。\n假设$X,Y$是两个随机变量，已知： $$ \\mathbb{E}[X]= \\mathbb{E}[Y] = 0\\\\ \\text{Var}[X]=\\text{Var}[Y] = 1 $$\n那么乘积$XY$满足： $$ \\mathbb{E}[XY]= \\rho $$ 如果进一步地，$X,Y$均为正态分布随机变量，则有 $$ \\text{Var}[XY]= 1 + \\rho^2 $$ 其中$\\rho$是$X$和$Y$的相关系数。\nproof\n按照相关系数定义： $$ \\rho=\\frac{\\text{Cov}(X, Y)}{\\sqrt{\\text{Var}[X]\\cdot \\text{Var}[Y]}}\\\\ \\text{Cov}(X, Y)=\\mathbb{E}[XY]-\\mathbb{E}[X]\\mathbb{E}[Y] $$ 反过来可以得到 $$ \\begin{align} \\mathbb{E}[XY] \u0026amp;= \\text{Cov}(X, Y)+\\mathbb{E}[X]\\mathbb{E}[Y]\\\\ \u0026amp;=\\rho\\cdot \\sqrt{\\text{Var}[X]\\cdot \\text{Var}[Y]}+\\mathbb{E}[X]\\mathbb{E}[Y]\\\\ \u0026amp;=\\rho \\end{align} $$\n如果$X,Y$均为正态分布随机变量，按照方差的定义： $$ \\begin{align} \\text{Var}[XY]\u0026amp;=\\mathbb{E}[(XY)^2]-(\\mathbb{E}[XY])^2\\\\ \u0026amp;= \\mathbb{E}[X^2Y^2]-\\rho^2\\tag{1}\\\\ \\end{align} $$\n对$\\mathbb{E}[X^2Y^2]$应用Isserlis定理（显然$\\mathbb{E}[X^2]=\\mathbb{E}[Y^2]=1$）：\n$$ \\begin{align} \\mathbb{E}[X^2Y^2] \u0026amp;= \\mathbb{E}[X^2]\\mathbb{E}[Y^2] + \\mathbb{E}[XY]\\mathbb{E}[XY] \\cdot 2\\\\ \u0026amp;=1 + 2\\rho^2 \\end{align} $$\n带入$(1)$，则可以得到 $$ \\begin{align} \\text{Var}[XY]\u0026amp;=1 + 2\\rho^2-\\rho^2\\\\ \u0026amp;=1+\\rho^2 \\end{align} $$\n$\\mathbb{E}[XY]$和$\\text{Var}[XY]$为我们分析内积的量级随向量维度的变化关系提供了很好的工具。对于两个随机向量$\\boldsymbol{x},\\boldsymbol{y}\\in\\mathbb{R}^{n}$，而言，假设每个分量$x_i,y_i$都是独立地由某个联合正态分布中采样得到的。由于内积可以写成一系列乘积的和 $$ \\langle\\boldsymbol{x},\\boldsymbol{y}\\rangle = \\sum_i x_i y_i $$\n由大数定理我们有，当$n\\to\\infty$， $$ \\frac{1}{n}\\sum_{i}^n x_i y_i \\overset{P}{\\to} \\mathbb{E}[XY] $$\n由中心极限定理，有 $$ \\frac{1}{\\sqrt{n}}\\sum_{i}^n \\left(x_i y_i-\\mathbb{E}[XY]\\right) \\overset{\\mathcal{D}}{\\to} \\mathcal{N}\\left(0, \\text{Var}[XY]\\right) $$\n这里讨论两种典型的情况：\n（1）当$x_i,y_i$完全独立时，对应$\\rho=0$，$\\mathbb{E}[XY]=0,\\text{Var}[XY]=1$.\n此时$\\frac{1}{\\sqrt{n}}\\sum_i x_i y_i\\overset{\\mathcal{D}}{\\to}\\mathcal{N}(0, 1)$，内积的量级被中心极限定理主导，$\\langle\\boldsymbol{x},\\boldsymbol{y}\\rangle$具有$\\Theta(\\sqrt{n})$的典型值。\n（2）当$x_i,y_i$具有一定相关性时，例如在极端情况下，二者完全正相关对应$\\rho=1$，$\\mathbb{E}[XY]=1,\\text{Var}[XY]=2$.\n此时我们主要关注大数定理下的极限$\\frac{1}{n}\\sum_{i}^n x_i y_i \\overset{P}{\\to} 1$，$\\langle\\boldsymbol{x},\\boldsymbol{y}\\rangle$具有$\\Theta(n)$的典型值1。\n在典型的Attention模型（e.g., Transformer）中，$\\boldsymbol{q},\\boldsymbol{k}$更接近于哪一种情况呢？\n这里可以借用Dao 2022.的梯度推导来获取一个直观的印象（P19页）（请注意下面公式中$q_i$是指第$i$个query向量，而不是query的某个分量，其他符号类似）:\n$$ dq_i = \\sum_{j} dS_{ij} k_j = \\sum_{j} P_{ij} (dP_{ij} - D_i) k_j = \\sum_{j} \\frac{e^{q_i^T k_j}}{L_i} (do_i^T v_j - D_i) k_j. $$ $$ dk_j = \\sum_{i} dS_{ij} q_i = \\sum_{i} P_{ij} (dP_{ij} - D_i) q_i = \\sum_{i} \\frac{e^{q_i^T k_j}}{L_i} (do_i^T v_j - D_i) q_i. $$\n可以看到对于$q_i$而言，其梯度是$\\{k_j\\}$的线性组合，对$k_j$亦然。由于神经网络训练一般是由一系列梯度累积形成最终的参数\u0026amp;函数，因此我们可以很合理地假设$\\boldsymbol{q},\\boldsymbol{k}$具有一定相关性（但无法达到$\\rho=1$）。\n因此也就有了mup中对Attention Logits使用$1/d_k$的放缩系数的做法。\n不过这里需要注意，上面的分析是在$n\\to\\infty$下推导出来的，在mup这样的无穷宽理论框架下使用这种假设是相对自然的。 但在典型的head维度设定下，使用$1/\\sqrt{d_k}$自然也算不上不好（在初始化状态下，$\\boldsymbol{q},\\boldsymbol{k}$分量互相独立也是合理的假设）。\n大数定理vs.中心极限定理的思维框架在TP中也被用于理解其他量纲。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://nil9.net/posts/attn-logits-scale/","summary":"\u003cp\u003e最近在细读Tensor Programs的时候，发现一个有意思的细节，\u003ca href=\"https://arxiv.org/abs/2203.03466\"\u003eGreg 2022.\u003c/a\u003e在将mup应用到Transformer的时候，对Attention Logits使用\u003ccode\u003e$1/d_k$\u003c/code\u003e进行放缩，而不是传统上的\u003ccode\u003e$1/\\sqrt{d_k}$\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e我不禁回想起很多年前在知乎的一个问题：\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.zhihu.com/question/339723385/answer/782509914\"\u003etransformer中的attention为什么scaled? - Nil-9的回答\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e当时笔者沿着\u003ca href=\"https://arxiv.org/abs/1706.03762\"\u003eVaswani 2017.\u003c/a\u003e的一个脚注，展开写了这样一篇回答，其数学原理并不复杂，浓缩版本是这样的：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e假设\u003ccode\u003e$\\boldsymbol{q}$\u003c/code\u003e,\u003ccode\u003e$\\boldsymbol{k}\\in\\mathbb{R}^{d_k}$\u003c/code\u003e是\u003cstrong\u003e相互独立\u003c/strong\u003e的随机变量，均值和方差分别是0、1，则点积\u003ccode\u003e$\\langle\\boldsymbol{q},\\boldsymbol{k}\\rangle$\u003c/code\u003e的量级是\u003ccode\u003e$\\Theta(\\sqrt{d_k})$\u003c/code\u003e。为了防止点积过大导致softmax出现梯度消失问题，因此需要再点积基础上乘上\u003ccode\u003e$1/\\sqrt{d_k}$\u003c/code\u003e，将点积稳定在\u003ccode\u003e$\\Theta(1)$\u003c/code\u003e量级。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e这个问题后来也成为了某些公司的所谓「机器学习八股文」之一。但这个放缩是不是严谨的呢？我们来仔细思考一下。\u003c/p\u003e\n\u003cp\u003e这里比较成问题的是「相互独立」的假设。我们先建立如下的基本的结论。\u003c/p\u003e\n\u003cp\u003e假设\u003ccode\u003e$X,Y$\u003c/code\u003e是两个随机变量，已知：\n\u003ccode\u003e$$ \\mathbb{E}[X]= \\mathbb{E}[Y] = 0\\\\ \\text{Var}[X]=\\text{Var}[Y] = 1 $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e那么乘积\u003ccode\u003e$XY$\u003c/code\u003e满足：\n\u003ccode\u003e$$ \\mathbb{E}[XY]= \\rho $$\u003c/code\u003e\n如果进一步地，\u003ccode\u003e$X,Y$\u003c/code\u003e均为正态分布随机变量，则有\n\u003ccode\u003e$$ \\text{Var}[XY]= 1 + \\rho^2 $$\u003c/code\u003e\n其中\u003ccode\u003e$\\rho$\u003c/code\u003e是\u003ccode\u003e$X$\u003c/code\u003e和\u003ccode\u003e$Y$\u003c/code\u003e的相关系数。\u003c/p\u003e\n\u003cstyle type=\"text/css\"\u003e\n     \n    .notice {\n        --title-color: #fff;\n        --title-background-color: #6be;\n        --content-color: #444;\n        --content-background-color: #e7f2fa;\n    }\n\n    .notice.proof {\n        --title-background-color: rgb(130, 130, 130);\n        --content-background-color: #f7f7f7;\n    }\n\n    .notice.info {\n        --title-background-color: #fb7;\n        --content-background-color: #fec;\n    }\n\n    .notice.tip {\n        --title-background-color: #5a5;\n        --content-background-color: #efe;\n    }\n\n    .notice.warning {\n        --title-background-color: #c33;\n        --content-background-color: #fee;\n    }\n\n     \n\n    body.dark .notice {\n        --title-color: #fff;\n        --title-background-color: #069;\n        --content-color: #ddd;\n        --content-background-color: #023;\n    }\n\n    body.dark .notice.proof {\n        --title-background-color: rgb(129, 129, 129);\n        --content-background-color: rgb(41, 41, 41);\n    }\n\n    body.dark .notice.info {\n        --title-background-color: #a50;\n        --content-background-color: #420;\n    }\n\n    body.dark .notice.tip {\n        --title-background-color: #363;\n        --content-background-color: #121;\n    }\n\n    body.dark .notice.warning {\n        --title-background-color: #800;\n        --content-background-color: #400;\n    }\n\n     \n    .notice {\n        padding: 18px;\n        line-height: 24px;\n        margin-bottom: 24px;\n        border-radius: 4px;\n        color: var(--content-color);\n        background: var(--content-background-color);\n    }\n\n    .notice p:last-child {\n        margin-bottom: 0\n    }\n\n     \n    .notice-title {\n        margin: -18px -18px 12px;\n        padding: 4px 18px;\n        border-radius: 4px 4px 0 0;\n        font-weight: 700;\n        color: var(--title-color);\n        background: var(--title-background-color);\n    }\n\n     \n    .icon-notice {\n        display: inline-flex;\n        align-self: center;\n        margin-right: 8px;\n    }\n\n    .icon-notice img,\n    .icon-notice svg {\n        height: 1em;\n        width: 1em;\n        fill: currentColor;\n    }\n\n    .icon-notice img,\n    .icon-notice.baseline svg {\n        top: .125em;\n        position: relative;\n    }\n\u003c/style\u003e\u003cdiv class=\"notice proof\" \u003e\n    \u003cp class=\"notice-title\"\u003e\n        \u003cspan class=\"icon-notice baseline\"\u003e\n            \n        \u003c/span\u003eproof\u003c/p\u003e","title":"简记：Attention Logits究竟如何放缩？"},{"content":"TL;DR 笔者通过自定义$XX^\\top$算子，加速Muon优化器中的核心操作——Newton-Schulz迭代，算子单测在8192维度上相比原生实现计算时间降低约一半，端到端的Newton-Schulz迭代运行时间降低至原来的0.71倍。相关代码已经发布在：\nhttps://github.com/nil0x9/flash-muon\n读者可以通过如下方式来试用优化版本的Muon实现或者核心算子。\ngit clone --recurse-submodules https://github.com/nil0x9/flash-muon.git pip install -e ./ 具体做法 Muon优化器的核心机制是通过Newton-Schulz迭代法来代替SVD分解，实现GPU友好的$\\boldsymbol{U}\\boldsymbol{V}^T$计算（之前笔者写过这篇blog介绍Muon背后的数学原理）。\nNewton-Schulz迭代法的核心公式如下所示，通过指定合适的多项式系数$a,b,c$，将SVD分解的 $\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T$中的$\\boldsymbol{\\Sigma}$在迭代中消除 $$ \\boldsymbol{X}\\leftarrow a\\boldsymbol{X} + b(\\boldsymbol{X}\\boldsymbol{X}^\\top)\\boldsymbol{X} + c(\\boldsymbol{X}\\boldsymbol{X}^\\top)(\\boldsymbol{X}\\boldsymbol{X}^\\top)\\boldsymbol{X} $$ 其中$\\boldsymbol{X}$是梯度矩阵或梯度的动量矩阵。在Jordan的实现 中这个迭代过程由如下的PyTorch代码实现：\nfor _ in range(steps): A = X @ X.T B = b * A + c * A @ A X = a * X + B @ X 注意到这里提前计算了$\\boldsymbol{A}=\\boldsymbol{X}\\boldsymbol{X}^\\top$来减少重复计算。由于$\\boldsymbol{A}$是一个对称矩阵，则循环内第二行的$\\boldsymbol{AA} = \\boldsymbol{AA}^\\top$。这里我们可以看到一个被使用了两次的运算lambda x: matmul(x, x.T)，如果这个算子可以有比matmul更高效的实现，就可以实现更快的Newton-Schulz迭代。\n幸运的是$\\boldsymbol{X}\\boldsymbol{X}^\\top$是一个对称矩阵，理论上我们可以只计算上三角部分，下三角部分可以直接复用下三角部分的结果。\n这一点在Laker Newhouse的这篇文章中有提及，但是可惜的是，他们并没有实现一个可用的kernel版本。\n笔者沿着这个思路实现了一个可用的CUDA kernel。它的设计思路其实非常直观（也可能比较clumsy，欢迎批评指正）——在一般的GEMM算子中，每个线程会从全局内存load计算一定分块的矩阵算元到自己的寄存器内，在计算完对应分块的结果矩阵后，将这部分的结果store到输出算元对应的全局内存空间。GEMM的相关分析教程实在是太多了，笔者就不赘述了。——而对于$\\boldsymbol{X}\\boldsymbol{X}^\\top$这种对称运算，笔者的让下三角对应的线程块在kernel一开始直接退出，上三角部分的线程在计算结束后做完一般的store操作后，根据线程和线程块的id，找到对应的下三角部分的全局内存空间，将当前线程对应的结果分块转置后store到下三角的分块。 用图像表达如下：\nmatmul_transpose的kernel设计\n为了防止warp divergence，笔者这里采取了一种偷懒的做法，直接以线程块为粒度做提早退出，这样总能保证同一个warp内执行的指令尽量对齐。\nCUDA kernel的具体实现就不赘述了，相关代码可以在flash-muon中找到，CUDA版本在这里，triton版本在这里。\n效果 笔者在RTX 4090设备上使用$\\{2^i\\}_{i=9}^{13}$的不同维度的方阵上测试了这个自定义kernel的单测速度，以及对应的插入到Newton-Schulz迭代后的整体速度。如下图所示：\n本文方法与torch原生matmul的运行时间对比\n可以看到在8192的维度下，单测运行时间降低了一半左右，插入到NS迭代（5步）时，运行时间约为基线版本的0.71倍。\n笔者也在不同的设备上测试了相应的执行速度（triton实现）：\ndevice dim flash torch compiled H800 1024 0.0124 0.0112 0.0107 H800 2048 0.0322 0.0384 0.0384 H800 4096 0.1838 0.2955 0.3000 H800 8192 1.4528 2.2643 2.2804 H20 1024 0.0164 0.0275 0.0275 H20 2048 0.0746 0.1588 0.1587 H20 4096 0.5068 1.0431 1.0431 H20 8192 3.9265 7.9691 7.9508 A100 1024 0.0191 0.0228 0.0232 A100 2048 0.0689 0.1166 0.1164 A100 4096 0.3733 0.6644 0.6649 A100 8192 2.9815 5.1604 5.2858 4090 1024 0.0208 0.0213 0.0208 4090 2048 0.0823 0.1098 0.1095 4090 4096 0.5249 0.8535 0.8546 4090 8192 3.5689 6.7631 6.7869 结语 本文介绍了一种使用自定义CUDA kernel的方式来提升Muon优化器的运行速度的方法，相关代码开源在nil0x9/flash-muon，欢迎读者Star、试用，并给笔者一些反馈（笔者水平有限，实现还存在诸多问题）！\n值得一提的是，本文的这种计算x@x.T的kernel，其实在cuBLAS中有类似的API，叫做SYRK，但可惜的是，他们并没有提供半精度的版本，因此笔者才自己实现了这样一个功能。（番外：如何合理设计kernel（以及thread-data布局）实现cublas的syrk函数（XX\u0026rsquo;）？）\n参考 Jordan 2024. Muon: An optimizer for hidden layers in neural networks https://github.com/KellerJordan/Muon/ Newhouse et al. Faster symmetric matrix multiplication with ThunderKittens ","permalink":"https://nil9.net/posts/flash-muon/","summary":"\u003ch1 id=\"tldr\"\u003eTL;DR\u003c/h1\u003e\n\u003cp\u003e笔者通过自定义\u003ccode\u003e$XX^\\top$\u003c/code\u003e算子，加速Muon优化器中的核心操作——Newton-Schulz迭代，算子单测在8192维度上相比原生实现计算时间降低约一半，端到端的Newton-Schulz迭代运行时间降低至原来的0.71倍。相关代码已经发布在：\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/nil0x9/flash-muon\"\u003ehttps://github.com/nil0x9/flash-muon\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e读者可以通过如下方式来试用优化版本的Muon实现或者核心算子。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-sh\" data-lang=\"sh\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003egit clone --recurse-submodules https://github.com/nil0x9/flash-muon.git\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003epip install -e ./\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch1 id=\"具体做法\"\u003e具体做法\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://kellerjordan.github.io/posts/muon/\"\u003eMuon优化器\u003c/a\u003e的核心机制是通过Newton-Schulz迭代法来代替SVD分解，实现GPU友好的\u003ccode\u003e$\\boldsymbol{U}\\boldsymbol{V}^T$\u003c/code\u003e计算（之前笔者写过\u003ca href=\"https://zhuanlan.zhihu.com/p/28236539668\"\u003e这篇blog\u003c/a\u003e介绍Muon背后的数学原理）。\u003c/p\u003e\n\u003cp\u003eNewton-Schulz迭代法的核心公式如下所示，通过指定合适的多项式系数\u003ccode\u003e$a,b,c$\u003c/code\u003e，将SVD分解的 \u003ccode\u003e$\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^T$\u003c/code\u003e中的\u003ccode\u003e$\\boldsymbol{\\Sigma}$\u003c/code\u003e在迭代中消除\n\u003ccode\u003e$$ \\boldsymbol{X}\\leftarrow a\\boldsymbol{X} + b(\\boldsymbol{X}\\boldsymbol{X}^\\top)\\boldsymbol{X} + c(\\boldsymbol{X}\\boldsymbol{X}^\\top)(\\boldsymbol{X}\\boldsymbol{X}^\\top)\\boldsymbol{X} $$\u003c/code\u003e\n其中\u003ccode\u003e$\\boldsymbol{X}$\u003c/code\u003e是梯度矩阵或梯度的动量矩阵。在\u003ca href=\"https://github.com/KellerJordan/Muon/tree/master\"\u003eJordan的实现\u003c/a\u003e 中这个迭代过程由如下的PyTorch代码实现：\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e _ \u003cspan style=\"color:#f92672\"\u003ein\u003c/span\u003e range(steps):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    A \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e X \u003cspan style=\"color:#f92672\"\u003e@\u003c/span\u003e X\u003cspan style=\"color:#f92672\"\u003e.\u003c/span\u003eT\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    B \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e b \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e A \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e c \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e A \u003cspan style=\"color:#f92672\"\u003e@\u003c/span\u003e A\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    X \u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e a \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e X \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e B \u003cspan style=\"color:#f92672\"\u003e@\u003c/span\u003e X\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e注意到这里提前计算了\u003ccode\u003e$\\boldsymbol{A}=\\boldsymbol{X}\\boldsymbol{X}^\\top$\u003c/code\u003e来减少重复计算。由于\u003ccode\u003e$\\boldsymbol{A}$\u003c/code\u003e是一个对称矩阵，则循环内第二行的\u003ccode\u003e$\\boldsymbol{AA} = \\boldsymbol{AA}^\\top$\u003c/code\u003e。这里我们可以看到一个被使用了两次的运算\u003ccode\u003elambda x: matmul(x, x.T)\u003c/code\u003e，如果这个算子可以有比\u003ccode\u003ematmul\u003c/code\u003e更高效的实现，就可以实现更快的Newton-Schulz迭代。\u003c/p\u003e\n\u003cp\u003e幸运的是\u003ccode\u003e$\\boldsymbol{X}\\boldsymbol{X}^\\top$\u003c/code\u003e是一个对称矩阵，理论上我们可以只计算上三角部分，下三角部分可以直接复用下三角部分的结果。\u003c/p\u003e\n\u003cp\u003e这一点在Laker Newhouse的这篇\u003ca href=\"https://www.lakernewhouse.com/assets/writing/faster-symmul-with-thunderkittens.pdf\"\u003e文章\u003c/a\u003e中有提及，但是可惜的是，他们并没有实现一个可用的kernel版本。\u003c/p\u003e\n\u003cp\u003e笔者沿着这个思路实现了一个可用的CUDA kernel。它的设计思路其实非常直观（也可能比较clumsy，欢迎批评指正）——在一般的GEMM算子中，每个线程会从全局内存load计算一定分块的矩阵算元到自己的寄存器内，在计算完对应分块的结果矩阵后，将这部分的结果store到输出算元对应的全局内存空间。GEMM的相关分析教程实在是太多了，笔者就不赘述了。——而对于\u003ccode\u003e$\\boldsymbol{X}\\boldsymbol{X}^\\top$\u003c/code\u003e这种对称运算，笔者的让下三角对应的线程块在kernel一开始直接退出，\u003c!-- raw HTML omitted --\u003e上三角部分的线程在计算结束后做完一般的store操作后，根据线程和线程块的id，找到对应的下三角部分的全局内存空间，将当前线程对应的结果分块转置后store到下三角的分块\u003c!-- raw HTML omitted --\u003e。\n用图像表达如下：\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\n  \u003cimg alt=\"design of matmul_transpose kernel\" loading=\"lazy\" src=\"/images/flash-muon/matmul_transpose_kernel.png\" title=\"matmul_transpose的kernel设计\"\u003e\u003cfigcaption class=\"image-caption\"\u003ematmul_transpose的kernel设计\u003c/figcaption\u003e\u003c/figure\u003e\u003c/p\u003e","title":"自定义CUDA kernel加速Muon优化器"},{"content":"Prologue 如何衡量神经网络参数空间中的距离（i.e., 选取合适的范数）？这个问题我们之前已经在这篇文章中有所涉及，本文是更完整的拓展。\n首先应该明确为什么这个问题很重要？因为在梯度下降这样的优化算法框架下，总是依赖目标参数的某种距离的衡量。例如，当目标参数可以对应到欧几里得空间的坐标时，就可以得到局部目标函数下降最快的方向是梯度负方向。\n尽管在深度学习优化中，直接使用欧几里得范数（衡量参数空间的距离）非常吸引人，并且实际上也有效。但是，这种做法会丢失结构信息，因为它实质上是将参数看做一个扁平的向量。那么得到的优化方向就可能是相对低效的。\n对于常见的神经网络而言，对参数矩阵使用谱范数似乎是个更好的选择[1][2]。使用谱范数的相关约束，可以引出在现在的大规模模型训练中非常重要的两个工作：\n$\\mu P$ (Maximal Update Parameterization)：使用特定的初始化与学习率的参数化，可以做到在小模型上调节部分超参数，迁移到同构的大模型。 Muon优化器的更新规则，即对梯度矩阵做SVD分解，消除奇异值矩阵后将$-\\boldsymbol{U}\\boldsymbol{V}^\\top$作为对应参数的更新方向。 从而涵盖了给定神经网络结构下的参数初始化、参数更新的步长和方向三个方面（模型成功训练的三个基本要素）。\nfeature learning的谱条件 为什么需要使用谱范数来衡量参数的大小和参数更新步长呢？在Greg Yang的系列工作中，考虑的主要是feature learning的问题。Yang在他的TP4中发现，在标准参数化或者NTK参数化的设定下，无穷宽的神经网络没有办法学习特征（一次更新后特征与初始化状态无异），这就与传统的linear transfer learning的智慧形成了悖论，因为如果没有feature learning，那么预训练就对后续的transfer没有任何增益。\n于是为了达成非平凡的feature learning，就需要对神经网络的特征$\\boldsymbol{h}_\\ell(\\boldsymbol{x})\\in\\mathbb{R}^{n_\\ell}$的范数与一步更新的特征范数变化量$\\boldsymbol{h}_\\ell(\\boldsymbol{x})$做如下的约束[1]：\n$$ \\|\\boldsymbol{h}_\\ell\\|_2=\\Theta(\\sqrt{n_\\ell})\\text{ and } \\|\\Delta\\boldsymbol{h}_\\ell\\|_2=\\Theta(\\sqrt{n_\\ell}), \\text{ for }\\ell=1,\\ldots,L-1\\tag{1} $$\n这个条件意味着，对于中间的任何特征向量而言，每个元素平均下来的范数是常数阶的，一次更新后的变化量也是常数阶的（不会随着宽度$n_\\ell$趋近无穷而爆炸或者弥散）。\n对于常见的由一系列矩阵参数构成的神经网络而言，实现这样的约束，需要对参数矩阵做如下的谱条件约束：\n$$ \\|\\boldsymbol{W}_\\ell\\|_2=\\Theta\\left(\\sqrt{\\frac{n_\\ell}{n_{\\ell-1}}}\\right)\\text{ and } \\|\\Delta\\boldsymbol{W}_\\ell\\|_2=\\Theta\\left(\\sqrt{\\frac{n_\\ell}{n_{\\ell-1}}}\\right), \\text{ for }\\ell=1,\\ldots,L-1\\tag{2} $$\n这里的$\\|\\cdot\\|_2$是矩阵的谱范数，即从向量$\\ell_2$空间映射到向量$\\ell_2$空间的算子诱导范数\n$$ \\|\\boldsymbol{A}\\|_2=\\underset{\\boldsymbol{x}\\in\\mathbb{R}^n}{\\max} \\frac{\\|\\boldsymbol{A}\\boldsymbol{x}\\|_2}{\\|\\boldsymbol{x}\\|_2}\\text{ for } \\boldsymbol{A}\\in\\mathbb{R}^{m\\times n} $$\n这个谱条件能够保证公式$(1)$成立，证明思路主要利用到谱范数的性质$\\|\\boldsymbol{Av}\\|_2\\le\\|\\boldsymbol{A}\\|_2\\|\\boldsymbol{v}\\|_2$，证明出上述谱条件诱导的$\\|\\boldsymbol{h}_\\ell\\|_2,\\|\\Delta\\boldsymbol{h}_\\ell\\|_2$的上界均为$\\Theta(\\sqrt{n_\\ell})$，接着证明这个上界依概率总是取得。具体可以看这篇论文的第三节（相比Greg的TP系列，还算是很容易理解的）。\n公式$(2)$中的第二个条件是很直观的，对于SGD优化器而言，直接按照每个参数矩阵的维度设定对应的学习率即可\n$$ \\eta_\\ell = \\Theta\\left(\\sqrt{\\frac{n_\\ell}{n_{\\ell-1}}}\\right) $$\n对于第一个条件$\\boldsymbol{W}_\\ell = \\Theta\\left(\\sqrt{\\frac{n_\\ell}{n_{\\ell-1}}}\\right)$，需要修改初始化的方式——我们可以从一个标准正态分布中i.i.d.采样一个权重矩阵$\\boldsymbol{W}'\\in\\mathbb{R}^{n_\\ell\\times n_{\\ell-1}}$，接着做缩放$\\boldsymbol{W} = \\sigma\\boldsymbol{W}'$（这里略去下标$\\ell$）得到最终的初始化参数矩阵。\n这个$\\sigma$怎么确定呢？由于矩阵的stable rank按定义可以将矩阵的谱范数和Frobenius范数联系起来： $$ \\text{stable-rank}(\\boldsymbol{W}) = \\frac{\\|\\boldsymbol{W}\\|_F^2}{\\|\\boldsymbol{W}\\|_2^2} $$\n这里分子是谱范数平方，分母是Frobenius范数平方。对于标准正态分布中i.i.d.采样的矩阵$\\boldsymbol{W}$而言，其stable rank在渐进意义下满足 $$ \\text{stable-rank}(\\boldsymbol{W}) = \\Theta(\\min\\{n_\\ell, n_{\\ell-1}\\}) $$ 那么根据RMS范数与F范数、F范数与谱范数的换算关系，只需要让RMS范数满足 $$ \\begin{align} \\|\\boldsymbol{W}\\|_{\\text{RMS}} \u0026amp;= \\frac{1}{\\sqrt{n_\\ell\\cdot n_{\\ell-1}}}\\|\\boldsymbol{W}\\|_F\\\\ \u0026amp;= \\frac{1}{\\sqrt{n_\\ell\\cdot n_{\\ell-1}}}\\cdot \\sqrt{\\text{stable-rank}(\\boldsymbol{W})\\cdot \\|\\boldsymbol{W}\\|_2^2}\\\\ \u0026amp;= \\frac{1}{\\sqrt{n_\\ell\\cdot n_{\\ell-1}}}\\cdot \\Theta(\\sqrt{\\min\\{n_\\ell, n_{\\ell-1}\\}})\\cdot \\Theta\\left(\\sqrt{\\frac{n_\\ell}{n_{\\ell-1}}}\\right)\\\\ \u0026amp;= \\Theta\\left( \\frac{1}{\\sqrt{n_{\\ell-1}}}\\min\\left\\{1,\\sqrt{\\frac{n_\\ell}{n_{\\ell-1}}}\\right\\} \\right) \\end{align} $$\n也就是说，我们设定$\\sigma=\\Theta\\left( \\frac{1}{\\sqrt{n_{\\ell-1}}}\\min\\left\\{1,\\sqrt{\\frac{n_\\ell}{n_{\\ell-1}}}\\right\\}\\right)$并做缩放$\\boldsymbol{W} = \\sigma\\boldsymbol{W}'$即可满足上述的谱条件。\n这个初始化系数与学习率比例就恰好对应了$\\mu P$的核心公式！\n在这个初始化和学习率设定下，由于可以满足$ \\|\\Delta\\boldsymbol{h}_\\ell\\|_2=\\Theta(\\sqrt{n_\\ell})$，即$\\|\\Delta\\boldsymbol{h}_\\ell\\|_{\\text{RMS}}=\\Theta(1)$，优化过程的特征学习不会随着宽度$n_\\ell$增大而爆炸或者弥散，因此可以做到zero-shot hyper-parameter transfer，从而能在较小的模型中调节部分超参数（学习率、初始化方差和放缩系数，在TP4中称为abc-parameterization），满足这组参数化对大的同构模型也是优势超参。\n谱条件引导的最速优化方向 除了谱范数约束推导出初始化方差与学习率（的比例）之外，使用谱范数约束推导最速下降方向： $$ \\underset{\\Delta\\boldsymbol{W}_1,\\ldots,\\Delta\\boldsymbol{W}_L}{\\text{arg min}}\\left[ \\sum_{l=1}^L {\\langle \\boldsymbol{G}_l, \\Delta\\boldsymbol{W}_l \\rangle}_F + \\frac{\\lambda}{2}\\max_{l=1}^L{{\\|\\Delta\\boldsymbol{W}_l\\|^2_{2}}} \\right]\\\\ $$\n可以得出最速下降的解是 $$ \\begin{align} \\Delta \\boldsymbol{W}_l\u0026amp;=-\\eta{\\boldsymbol{U}\\boldsymbol{V}^\\top}\\\\ \\end{align} $$\n也就是Muon和Shampoo优化器（如果忽略动量或累加项）的优化方向。具体的证明笔者在这篇文章已经介绍过了，读者也可以直接阅读这篇论文。\n需要提及的是，运用这种方法的前提是，参数矩阵表达的是某种线性映射，如果是Embeddings矩阵或分类器矩阵，需要参考具体含义使用其他的诱导范数。\n结语 本文简要介绍了使用谱范数衡量参数空间的距离，对初始权重矩阵、权重矩阵的更新量做约束，可以推导出零次超参数迁移方法$\\mu P$中的初始化、学习率设置，以及SOTA优化器Muon/Shampoo的优化方向。这是我们科学地优化神经网络的一个起点。\n参考阅读 Yang 2023. A Spectral Condition for Feature Learning Bernstein \u0026amp; Newhouse 2024. Old Optimizer, New Norm: An Anthology Bernstein \u0026amp; Newhouse 2024. Modular Duality in Deep Learning ","permalink":"https://nil9.net/posts/spectral-condition/","summary":"\u003ch1 id=\"prologue\"\u003ePrologue\u003c/h1\u003e\n\u003cp\u003e如何衡量神经网络参数空间中的距离（i.e., 选取合适的范数）？这个问题我们之前已经在\u003ca href=\"https://nil9.net/posts/gd-norm-const/\"\u003e这篇文章\u003c/a\u003e中有所涉及，本文是更完整的拓展。\u003c/p\u003e\n\u003cp\u003e首先应该明确为什么这个问题很重要？因为在梯度下降这样的优化算法框架下，总是依赖目标参数的某种距离的衡量。例如，当目标参数可以对应到欧几里得空间的坐标时，就可以得到局部目标函数下降最快的方向是梯度负方向。\u003c/p\u003e\n\u003cp\u003e尽管在深度学习优化中，直接使用欧几里得范数（衡量参数空间的距离）非常吸引人，并且实际上也有效。但是，\u003cem\u003e\u003cstrong\u003e这种做法会丢失结构信息\u003c/strong\u003e\u003c/em\u003e，因为它实质上是将参数看做一个扁平的向量。那么得到的优化方向就可能是相对低效的。\u003c/p\u003e\n\u003cp\u003e对于常见的神经网络而言，对参数矩阵使用谱范数似乎是个更好的选择[1][2]。使用谱范数的相关约束，可以引出在现在的大规模模型训练中非常重要的两个工作：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003e$\\mu P$\u003c/code\u003e (Maximal Update Parameterization)：使用特定的初始化与学习率的参数化，可以做到在小模型上调节部分超参数，迁移到同构的大模型。\u003c/li\u003e\n\u003cli\u003eMuon优化器的更新规则，即对梯度矩阵做SVD分解，消除奇异值矩阵后将\u003ccode\u003e$-\\boldsymbol{U}\\boldsymbol{V}^\\top$\u003c/code\u003e作为对应参数的更新方向。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e从而涵盖了给定神经网络结构下的参数初始化、参数更新的步长和方向三个方面（模型成功训练的三个基本要素）。\u003c/p\u003e\n\u003ch1 id=\"feature-learning的谱条件\"\u003efeature learning的谱条件\u003c/h1\u003e\n\u003cp\u003e为什么需要使用谱范数来衡量参数的大小和参数更新步长呢？在Greg Yang的系列工作中，考虑的主要是feature learning的问题。Yang在他的TP4中发现，在标准参数化或者NTK参数化的设定下，无穷宽的神经网络没有办法学习特征（一次更新后特征与初始化状态无异），这就与传统的linear transfer learning的智慧形成了悖论，因为如果没有feature learning，那么预训练就对后续的transfer没有任何增益。\u003c/p\u003e\n\u003cp\u003e于是为了达成非平凡的feature learning，就需要对神经网络的特征\u003ccode\u003e$\\boldsymbol{h}_\\ell(\\boldsymbol{x})\\in\\mathbb{R}^{n_\\ell}$\u003c/code\u003e的范数与一步更新的特征范数变化量\u003ccode\u003e$\\boldsymbol{h}_\\ell(\\boldsymbol{x})$\u003c/code\u003e做如下的约束[1]：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\|\\boldsymbol{h}_\\ell\\|_2=\\Theta(\\sqrt{n_\\ell})\\text{ and } \\|\\Delta\\boldsymbol{h}_\\ell\\|_2=\\Theta(\\sqrt{n_\\ell}), \\text{ for }\\ell=1,\\ldots,L-1\\tag{1} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这个条件意味着，对于中间的任何特征向量而言，每个元素平均下来的范数是常数阶的，一次更新后的变化量也是常数阶的（不会随着宽度\u003ccode\u003e$n_\\ell$\u003c/code\u003e趋近无穷而爆炸或者弥散）。\u003c/p\u003e\n\u003cp\u003e对于常见的由一系列矩阵参数构成的神经网络而言，实现这样的约束，需要对参数矩阵做如下的谱条件约束：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\|\\boldsymbol{W}_\\ell\\|_2=\\Theta\\left(\\sqrt{\\frac{n_\\ell}{n_{\\ell-1}}}\\right)\\text{ and } \\|\\Delta\\boldsymbol{W}_\\ell\\|_2=\\Theta\\left(\\sqrt{\\frac{n_\\ell}{n_{\\ell-1}}}\\right), \\text{ for }\\ell=1,\\ldots,L-1\\tag{2} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这里的\u003ccode\u003e$\\|\\cdot\\|_2$\u003c/code\u003e是矩阵的谱范数，即从向量\u003ccode\u003e$\\ell_2$\u003c/code\u003e空间映射到向量\u003ccode\u003e$\\ell_2$\u003c/code\u003e空间的算子诱导范数\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\|\\boldsymbol{A}\\|_2=\\underset{\\boldsymbol{x}\\in\\mathbb{R}^n}{\\max} \\frac{\\|\\boldsymbol{A}\\boldsymbol{x}\\|_2}{\\|\\boldsymbol{x}\\|_2}\\text{ for } \\boldsymbol{A}\\in\\mathbb{R}^{m\\times n} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这个谱条件能够保证公式\u003ccode\u003e$(1)$\u003c/code\u003e成立，证明思路主要利用到谱范数的性质\u003ccode\u003e$\\|\\boldsymbol{Av}\\|_2\\le\\|\\boldsymbol{A}\\|_2\\|\\boldsymbol{v}\\|_2$\u003c/code\u003e，证明出上述谱条件诱导的\u003ccode\u003e$\\|\\boldsymbol{h}_\\ell\\|_2,\\|\\Delta\\boldsymbol{h}_\\ell\\|_2$\u003c/code\u003e的上界均为\u003ccode\u003e$\\Theta(\\sqrt{n_\\ell})$\u003c/code\u003e，接着证明这个上界依概率总是取得。具体可以看\u003ca href=\"http://arxiv.org/abs/2310.17813\"\u003e这篇论文\u003c/a\u003e的第三节（相比Greg的TP系列，还算是很容易理解的）。\u003c/p\u003e\n\u003cp\u003e公式\u003ccode\u003e$(2)$\u003c/code\u003e中的第二个条件是很直观的，对于SGD优化器而言，直接按照每个参数矩阵的维度设定对应的学习率即可\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\eta_\\ell = \\Theta\\left(\\sqrt{\\frac{n_\\ell}{n_{\\ell-1}}}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对于第一个条件\u003ccode\u003e$\\boldsymbol{W}_\\ell = \\Theta\\left(\\sqrt{\\frac{n_\\ell}{n_{\\ell-1}}}\\right)$\u003c/code\u003e，需要修改初始化的方式——我们可以从一个标准正态分布中i.i.d.采样一个权重矩阵\u003ccode\u003e$\\boldsymbol{W}'\\in\\mathbb{R}^{n_\\ell\\times n_{\\ell-1}}$\u003c/code\u003e，接着做缩放\u003ccode\u003e$\\boldsymbol{W} = \\sigma\\boldsymbol{W}'$\u003c/code\u003e（这里略去下标\u003ccode\u003e$\\ell$\u003c/code\u003e）得到最终的初始化参数矩阵。\u003c/p\u003e\n\u003cp\u003e这个\u003ccode\u003e$\\sigma$\u003c/code\u003e怎么确定呢？由于矩阵的stable rank按定义可以将矩阵的谱范数和Frobenius范数联系起来：\n\u003ccode\u003e$$ \\text{stable-rank}(\\boldsymbol{W}) = \\frac{\\|\\boldsymbol{W}\\|_F^2}{\\|\\boldsymbol{W}\\|_2^2} $$\u003c/code\u003e\u003c/p\u003e","title":"谱条件：如何衡量神经网络参数空间中的距离？"},{"content":"上篇文章介绍了Muon等新兴深度学习优化器背后的原理，即约束参数矩阵的诱导范数下得到新的更新方向。\n在Muon对参数更新方向$-\\boldsymbol{U}\\boldsymbol{V}^\\top$的计算中用到了Newton-Schulz迭代方法，本质上是在寻找这样一个多项式函数 $$ f(x)=ax+bx^3+cx^5+\\ldots $$\n使其满足对任意$x\\in(0, 1]$，对$x$应用多次$f(\\cdot)$，都能收敛到1附近。这里我们尝试设计一个能work的参数组合。\n我的一个简单的想法是，设计一个多项式函数，使$x=1$是它的一个吸引不动点：\n定义1（不动点）当$x_0$被函数$f(\\cdot)$映射到自身，即$f(x_0)=x_0$时，称$x_0$是函数$f(\\cdot)$的一个不动点。\n定义2（吸引不动点）$f$的吸引不动点是$f$的不动点$x_0$使得，对在足够接近$x_0$的定义域中的任何$x$值而言，迭代函数序列$x,f(x),f(f(x)),f(f(f(x))),\\ldots$收敛于$x_0$。\n要令$x=1$是$f(x)$的一个吸引不动点，要满足如下的必要条件：\n$f(1)=1$ $|f'(1)|\u0026lt;1$ 使用这两个条件是无法确定具体的参数值$a,b,\\ldots$的，但是对于三阶（参数包括$a,b$两个）或者五阶（参数包括$a,b,c$三个）的Newton-Schulz迭代，可以大大缩小搜索的空间。下面展开看下。\n三阶迭代 先讨论三阶迭代的形式\n$$ f(x)=ax+bx^3 $$\n代入上面的两个必要条件： $$ \\begin{split} f(1)=a+b = 1\\\\ -1 \u0026lt; f'(1)=a+3b \u0026lt; 1\\\\ \\end{split} $$\n根据第一个条件，可以把$b$用$1-a$重参数化，然后就有可行的条件 $$ 1 \u0026lt; a \u0026lt; 2 $$\n我们记五次迭代后的函数$\\phi(x)=f(f(f(f(f(x)))))$，可视化看一下不同$a$取值下对应的情况（理想情况下，对于$(0,1]$区间内的$x$，曲线要尽可能接近$y=1$）\n三阶迭代下，a取不同取值时对应的φ(x)\n注意到在$a$接近1的时候，$\\phi(x)$收敛到1附近的邻域是比较窄的，随着$a\\to 2$，收敛到1附近的「邻域」范围逐渐拓宽，但在$a=2$附近，曲线开始出现一定的抖动。对于优化器而言，这样的局部近似的方差是可以容忍的，因此我们可以选取一个比较接近2的值作为$a$的参数，例如$a=1.99,b=-0.99$。\n作为对比，在Bernstein \u0026amp; Newhouse 2024.中，作者给出的参数是$a=3/2,b=-1/2$。可以在下图中对照两种设定下的$\\phi(x)$.\n两种φ(x)对比\n可以看到Bernstein给出的参数虽然更平滑地收敛于1，但是对于在0附近的初始$x$，普遍无法收敛到1。也就是说对于较小的奇异值对应的$\\boldsymbol{u}_i, \\boldsymbol{v}_i$，倾向于在更新中被忽略。\n在$x=0$附近$\\phi(x)$能否快速接近1，主要取决于参数$a$的大小，这是因为$\\phi'(0)=a^5$。所以应该在尽可能保证$\\phi(x)\\approx 1,\\forall x\\in(0,1]$的同时，让$a$尽可能大。\n五阶迭代 现在来考虑五阶迭代的形式\n$$ f(x)=ax+bx^3+cx^5 $$\n代入上面的两个必要条件： $$ \\begin{split} f(1)=a+b+c = 1\\\\ -1 \u0026lt; f'(1)=a+3b+5c \u0026lt; 1\\\\ \\end{split} $$\n这里的参数比条件多，所以需要给更多的假设（e.g., 固定一个参数）。\n将$c=1-a-b$代入第2个条件，得到: $$ -1 \u0026lt; 5-4a-2b \u0026lt; 1\\\\ $$\n得到\n$$ 2-2a\u0026lt;b\u0026lt;3-2a $$\n不妨固定$b=2.99-2a$，则$c=a-1.99$，这样我们可以变化$a$的值看下$\\phi(x)$曲线长什么样：\n五阶迭代下，a取不同取值时对应的φ(x)\n类似三阶迭代中的情况，随着$a$变大，收敛到1附近的$x$范围逐步扩大，但是$a$超过一定阈值的时候，曲线就发散了。这是因为当$a$过大时，f(x)在$x=1$右侧的增长率越来越大，则$f(x)$会在迭代中落到超出$(0,1]$区间特别远的地方。\n我们可以选取一个互联网人的幸运数字$a=3.25$，这样$b=3.51,c=1.26$，对比Keller Jordan给出的参数$(3.4445, -4.7750, 2.0315)$如下：\n两种φ(x)对比\n可以看到我们按照简单的规则选取的参数在快速收敛到1这个性质上和Jordan的版本差不多，方差更小（但是需要注意在$x=0$附近这里的增长率是小于Jordan的版本的，因为这个增长率与$a$正相关）。\n根据笔者的实验，微调一下学习率的情况下，这组参数跟Jordan的参数收敛性能做到基本一致。\n参考：\nJordan 2024. Muon: An optimizer for hidden layers in neural networks Bernstein \u0026amp; Newhouse 2024. Modular Duality in Deep Learning ","permalink":"https://nil9.net/posts/newton-schulz-4muon/","summary":"\u003cp\u003e\u003ca href=\"https://nil9.net/posts/gd-norm-const/\"\u003e上篇文章\u003c/a\u003e介绍了Muon等新兴深度学习优化器背后的原理，即约束参数矩阵的诱导范数下得到新的更新方向。\u003c/p\u003e\n\u003cp\u003e在Muon对参数更新方向\u003ccode\u003e$-\\boldsymbol{U}\\boldsymbol{V}^\\top$\u003c/code\u003e的计算中用到了Newton-Schulz迭代方法，本质上是在寻找这样一个多项式函数\n\u003ccode\u003e$$ f(x)=ax+bx^3+cx^5+\\ldots $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e使其满足对任意\u003ccode\u003e$x\\in(0, 1]$\u003c/code\u003e，对\u003ccode\u003e$x$\u003c/code\u003e应用多次\u003ccode\u003e$f(\\cdot)$\u003c/code\u003e，都能收敛到1附近。这里我们尝试设计一个能work的参数组合。\u003c/p\u003e\n\u003cp\u003e我的一个简单的想法是，设计一个多项式函数，使\u003ccode\u003e$x=1$\u003c/code\u003e是它的一个\u003cstrong\u003e吸引不动点\u003c/strong\u003e：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e定义1\u003c/strong\u003e（\u003cem\u003e不动点\u003c/em\u003e）当\u003ccode\u003e$x_0$\u003c/code\u003e被函数\u003ccode\u003e$f(\\cdot)$\u003c/code\u003e映射到自身，即\u003ccode\u003e$f(x_0)=x_0$\u003c/code\u003e时，称\u003ccode\u003e$x_0$\u003c/code\u003e是函数\u003ccode\u003e$f(\\cdot)$\u003c/code\u003e的一个不动点。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e定义2\u003c/strong\u003e（\u003cem\u003e吸引不动点\u003c/em\u003e）\u003ccode\u003e$f$\u003c/code\u003e的吸引不动点是\u003ccode\u003e$f$\u003c/code\u003e的不动点\u003ccode\u003e$x_0$\u003c/code\u003e使得，对在足够接近\u003ccode\u003e$x_0$\u003c/code\u003e的定义域中的任何\u003ccode\u003e$x$\u003c/code\u003e值而言，迭代函数序列\u003ccode\u003e$x,f(x),f(f(x)),f(f(f(x))),\\ldots$\u003c/code\u003e收敛于\u003ccode\u003e$x_0$\u003c/code\u003e。\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003e要令\u003ccode\u003e$x=1$\u003c/code\u003e是\u003ccode\u003e$f(x)$\u003c/code\u003e的一个吸引不动点，要满足如下的必要条件：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003e$f(1)=1$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e$|f'(1)|\u0026lt;1$\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e使用这两个条件是无法确定具体的参数值\u003ccode\u003e$a,b,\\ldots$\u003c/code\u003e的，但是对于三阶（参数包括\u003ccode\u003e$a,b$\u003c/code\u003e两个）或者五阶（参数包括\u003ccode\u003e$a,b,c$\u003c/code\u003e三个）的Newton-Schulz迭代，可以大大缩小搜索的空间。下面展开看下。\u003c/p\u003e\n\u003ch1 id=\"三阶迭代\"\u003e三阶迭代\u003c/h1\u003e\n\u003cp\u003e先讨论三阶迭代的形式\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ f(x)=ax+bx^3 $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e代入上面的两个必要条件：\n\u003ccode\u003e$$ \\begin{split} f(1)=a+b = 1\\\\ -1 \u0026lt; f'(1)=a+3b \u0026lt; 1\\\\ \\end{split} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e根据第一个条件，可以把\u003ccode\u003e$b$\u003c/code\u003e用\u003ccode\u003e$1-a$\u003c/code\u003e重参数化，然后就有可行的条件\n\u003ccode\u003e$$ 1 \u0026lt; a \u0026lt; 2 $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e我们记五次迭代后的函数\u003ccode\u003e$\\phi(x)=f(f(f(f(f(x)))))$\u003c/code\u003e，可视化看一下不同\u003ccode\u003e$a$\u003c/code\u003e取值下对应的情况（理想情况下，对于\u003ccode\u003e$(0,1]$\u003c/code\u003e区间内的\u003ccode\u003e$x$\u003c/code\u003e，曲线要尽可能接近\u003ccode\u003e$y=1$\u003c/code\u003e）\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\n  \u003cimg alt=\"三阶迭代下，a取不同取值时对应的φ(x)\" loading=\"lazy\" src=\"/images/newton-schulz-4muon/ns3.gif\" title=\"三阶迭代下，a取不同取值时对应的φ(x)\"\u003e\u003cfigcaption class=\"image-caption\"\u003e三阶迭代下，a取不同取值时对应的φ(x)\u003c/figcaption\u003e\u003c/figure\u003e\u003c/p\u003e\n\u003cp\u003e注意到在\u003ccode\u003e$a$\u003c/code\u003e接近1的时候，\u003ccode\u003e$\\phi(x)$\u003c/code\u003e收敛到1附近的邻域是比较窄的，随着\u003ccode\u003e$a\\to 2$\u003c/code\u003e，收敛到1附近的「邻域」范围逐渐拓宽，但在\u003ccode\u003e$a=2$\u003c/code\u003e附近，曲线开始出现一定的抖动。对于优化器而言，这样的局部近似的方差是可以容忍的，因此我们可以选取一个比较接近2的值作为\u003ccode\u003e$a$\u003c/code\u003e的参数，例如\u003ccode\u003e$a=1.99,b=-0.99$\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e作为对比，在\u003ca href=\"https://arxiv.org/pdf/2410.21265\"\u003eBernstein \u0026amp; Newhouse 2024.\u003c/a\u003e中，作者给出的参数是\u003ccode\u003e$a=3/2,b=-1/2$\u003c/code\u003e。可以在下图中对照两种设定下的\u003ccode\u003e$\\phi(x)$\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\n  \u003cimg alt=\"两种φ(x)对比\" loading=\"lazy\" src=\"/images/newton-schulz-4muon/phi_curve_ns3.png\" title=\"两种φ(x)对比\"\u003e\u003cfigcaption class=\"image-caption\"\u003e两种φ(x)对比\u003c/figcaption\u003e\u003c/figure\u003e\u003c/p\u003e\n\u003cp\u003e可以看到Bernstein给出的参数虽然更平滑地收敛于1，但是对于在0附近的初始\u003ccode\u003e$x$\u003c/code\u003e，普遍无法收敛到1。也就是说对于较小的奇异值对应的\u003ccode\u003e$\\boldsymbol{u}_i, \\boldsymbol{v}_i$\u003c/code\u003e，倾向于在更新中被忽略。\u003c/p\u003e\n\u003cp\u003e在\u003ccode\u003e$x=0$\u003c/code\u003e附近\u003ccode\u003e$\\phi(x)$\u003c/code\u003e能否快速接近1，主要取决于参数\u003ccode\u003e$a$\u003c/code\u003e的大小，这是因为\u003ccode\u003e$\\phi'(0)=a^5$\u003c/code\u003e。所以应该在尽可能保证\u003ccode\u003e$\\phi(x)\\approx 1,\\forall x\\in(0,1]$\u003c/code\u003e的同时，让\u003ccode\u003e$a$\u003c/code\u003e尽可能大。\u003c/p\u003e\n\u003ch1 id=\"五阶迭代\"\u003e五阶迭代\u003c/h1\u003e\n\u003cp\u003e现在来考虑五阶迭代的形式\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ f(x)=ax+bx^3+cx^5 $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e代入上面的两个必要条件：\n\u003ccode\u003e$$ \\begin{split} f(1)=a+b+c = 1\\\\ -1 \u0026lt; f'(1)=a+3b+5c \u0026lt; 1\\\\ \\end{split} $$\u003c/code\u003e\u003c/p\u003e","title":"简记：Muon中设计Newton-Schulz迭代的系数？"},{"content":"在深度学习中最常用的优化方法是梯度下降方法及其变体。在过去很长一段时间中，Adam优化器是NLP社区的默认选择，在ViT出现之后，CV方面的工作也逐渐开始使用Adam和Adam的变体（在ViT之前，一种常见的观点是Adam不适用于Vision任务）。\n最近Muon优化器在Kimi的新工作带动下又火了一把，相较于Adam优化器需要同时维护一阶、二阶矩估计量，Muon只需要维护一份梯度的动量估计，因此在大规模训练中有很大优势。最近笔者顺着Muon的reference看了Jeremy Bernstein在优化的一些文章，觉得很有意思，因此写这篇文章梳理一下这一系列工作的部分精要。本文的核心论点是：使用诱导范数来约束梯度更新，可以推导出最近的一些新出现的优化方法，这也可能是未来深度学习优化的一个有潜力的探索方向。\n梯度下降（Recap） 当前深度学习优化算法的基石是梯度下降。之前笔者写过一篇拙文（自然梯度（二）：黎曼距离下的最速下降）整理过梯度下降的推导，核心的结论是：当我们假设参数空间是一个欧几里得空间、参数的距离可以用欧几里得距离来衡量时，我们在某个点约束$\\|\\Delta\\theta\\|_2\\le\\epsilon(\\epsilon\u0026gt;0)$时，$\\Delta\\theta$取梯度的反方向时可以让目标函数下降最多（具体的证明请参阅上述引文）。\n使用梯度下降最大的问题是，它实际上忽略了模型的结构。换句话说，梯度下降相当于将模型所有参数展平为1维向量，并且用向量2范数来衡量每次更新的「步长」。这种抽象是实用的，但是也存在一定的问题。两组参数有可能在欧几里得空间中距离很近，但是诱导的模型输出空间距离很远。造成的结果就是更新的方向实际上不是目标函数下降最快的方向。\n这个问题要如何解决呢？在自然梯度（二）：黎曼距离下的最速下降中，我们介绍了自然梯度方法，即使用Fisher信息矩阵的逆作为梯度的pre-conditioner来矫正梯度下降的方向，从原理上是使用参数更新前后引导的概率分布的KL散度作为每次更新的步长约束。但是对于常见的深度神经网络来说，这样做仍然是不切实际的，因为FIM是一个$N\\times N$的大矩阵（其中$N$是参数量），对于这么大的矩阵存储或求逆都是很难做到的。\n诱导范数作为步长约束 是否有一种更「廉价」的方法，可以考虑模型的参数结构，同时将参数的变化对于输出的影响作为约束呢？\n幸运的是，对于当下最流行的神经网络（e.g., Transformer）而言，模型往往可以拆解为很多小模块，其中最常见的是Linear模块（线性映射，这里忽略bias term）\n$$ f(\\boldsymbol{x};\\boldsymbol{W})=\\boldsymbol{Wx},\\ \\boldsymbol{W}\\in\\mathbb{R}^{n\\times m},\\boldsymbol{x}\\in \\mathbb{R}^{m}\\\\ $$\n在标准的Transformer中，Attention、FFN、LM分类器都是由Linear模块组成的，Embedding从数学原理上也是输入为one-hot encoding的线性映射。假设现在对于某个Linear模块的参数$\\boldsymbol{W}$做$\\Delta\\boldsymbol{W}$的更新（$\\boldsymbol{W}'\\leftarrow \\boldsymbol{W}+\\Delta\\boldsymbol{W}$），我们需要衡量这个更新对于最终输出的影响是多少（从而可以约束这个影响）。由于神经网络比较复杂，衡量$\\Delta\\boldsymbol{W}$对于最终目标函数的影响是相对繁琐的，但我们可以退而求其次，衡量$\\Delta\\boldsymbol{W}$对于这个Linear模块的输出$\\boldsymbol{Wx}$的影响。 考虑线性模块的输入与输出空间的距离都使用欧几里得范数$\\|\\cdot\\|_{\\ell_2}$衡量，那么这个约束可以通过如下不等式实现\n$$ \\|\\Delta\\boldsymbol{W}\\boldsymbol{x}\\|_{\\ell_2} \\le {\\color[rgb]{0, 0.5, 0.8}{\\|\\Delta\\boldsymbol{W}\\|_{\\ell_2\\to\\ell_2}}}\\|\\boldsymbol{x}\\|_{\\ell_2} $$ 这里的${\\color[rgb]{0, 0.5, 0.8}{\\|\\Delta\\boldsymbol{W}\\|_{\\ell_2\\to\\ell_2}}}$是矩阵2范数。这个不等式告诉我们，如果约束了参数更新量的谱范数（不等式右侧），也就约束了更新前后这个线性模块输出的变化量。\n假设现在需要优化的神经网络是由一系列的线性模块堆叠组成（e.g., MLP），我们可以参照梯度下降的推导构造如下的更新1\n$$ \\underset{\\Delta\\boldsymbol{W}_1,\\ldots,\\Delta\\boldsymbol{W}_L}{\\text{arg min}}\\left[ \\sum_{l=1}^L {\\langle \\boldsymbol{G}_l, \\Delta\\boldsymbol{W}_l \\rangle}_F + \\frac{\\lambda}{2}\\max_{l=1}^L{\\color[rgb]{0, 0.5, 0.8}{\\|\\Delta\\boldsymbol{W}_l\\|^2_{\\ell_2\\to\\ell_2}}} \\right]\\\\ $$\n这里$\\boldsymbol{G}_l$表示$\\boldsymbol{W}_l$对应的梯度矩阵（布局与原参数矩阵相同），${\\langle \\cdot, \\cdot \\rangle}_F$表示Frobenius内积（对矩阵而言，逐元素相乘求和）。这里之所以使用$\\max_{l=1}^L$（而不是直接求和），是因为我们引入这个约束时希望目标函数在$\\Delta\\boldsymbol{W}_l$变化下，能够保持平滑的性质2，因此需要bound所有参数矩阵更新量的谱范数的最大值。\n我们来逐步推导这个最小值成立时的$\\Delta\\boldsymbol{W}_1,\\ldots,\\Delta\\boldsymbol{W}_L$取值3。为了方便，把每个$\\Delta\\boldsymbol{W}_l$拆解成大小和方向两部分：$\\Delta\\boldsymbol{W}_l=c_l\\boldsymbol{T}_l(c_l\\triangleq\\|\\Delta\\boldsymbol{W}_l\\|_{\\ell_2\\to\\ell_2})$\n（为了可读性，下面的$\\|\\cdots\\|$均表示谱范数$\\|\\cdot\\|_{\\ell_2\\to\\ell_2}$）\n$$ \\begin{align} \u0026amp;\\underset{\\Delta\\boldsymbol{W}_1,\\ldots,\\Delta\\boldsymbol{W}_L}{\\text{min}}\\left[ \\sum_{l=1}^L {\\langle \\boldsymbol{G}_l, \\Delta\\boldsymbol{W}_l \\rangle}_F + \\frac{\\lambda}{2}\\max_{l=1}^L\\|\\Delta\\boldsymbol{W}_l\\|^2 \\right]\\\\ \u0026amp;=\\underset{c_1,\\ldots,c_L\\ge 0}{\\text{min}}\\left[ \\sum_{l=1}^L c_l\\min_{\\|\\boldsymbol{T}_l\\|=1}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F + \\frac{\\lambda}{2}\\max_{l=1}^Lc_l^2 \\right]\\\\ \u0026amp;=\\underset{c_1,\\ldots,c_L\\ge 0}{\\text{min}}\\left[ -\\sum_{l=1}^L c_l\\max_{\\|\\boldsymbol{T}_l\\|=1}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F + \\frac{\\lambda}{2}\\max_{l=1}^Lc_l^2 \\right]\\\\ \u0026amp;=\\underset{c_1,\\ldots,c_L\\ge 0}{\\text{min}}\\left[ -\\sum_{l=1}^L c_l \\|\\boldsymbol{G}_l\\|_* + \\frac{\\lambda}{2}\\max_{l=1}^Lc_l^2 \\right]\\quad\\triangleright\\|\\cdot\\|_*\\text{表示核范数}\\\\ \u0026amp;=\\underset{\\eta\\ge 0}{\\text{min}}\\left[ -\\sum_{l=1}^L \\eta\\|\\boldsymbol{G}_l\\|_* + \\frac{\\lambda}{2}\\max_{l=1}^L \\eta^2 \\right]\\tag{1}\\\\ \\end{align} $$\n上面的每个等号成立的解释如下：\n第一个等号成立，是直接带入$c_l\\triangleq\\|\\Delta\\boldsymbol{W}_l\\|_{\\ell_2\\to\\ell_2}$的结果，注意到$\\min$的条件被拆成了大小和方向两部分。\n第二个等号成立，是因为Frobenius内积具有线性性质， $$ \\begin{align} \\min_{\\|\\boldsymbol{T}_l\\|=1}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F\u0026amp;=-\\max_{\\|\\boldsymbol{T}_l\\|=1}{-\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F\\\\ \u0026amp;=-\\max_{\\|\\boldsymbol{T}_l\\|=1}{\\langle \\boldsymbol{G}_l, -\\boldsymbol{T}_l \\rangle}_F\\\\ \u0026amp;=-\\max_{\\|\\boldsymbol{T}'_l\\|=1}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}'_l \\rangle}_F\\\\ \\end{align} $$\n第三个等号成立，是因为Frobenius内积${\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F$最大值是矩阵$\\boldsymbol{G}_l$的核范数（nuclear norm，即奇异值之和）(求解过程放在文末)。\n第四个等号成立，即所有系数$c_l$都取相同值$\\eta$：假设我们当前有某组$c_1,\\ldots,c_L$取到目标最小值，且满足$\\max_i c_i=\\eta$，此时如果存在$c_j\u0026lt;\\eta$，则只需要增加$c_j$的值，即可在第二项$\\lambda/2\\max_{l=1}^Lc_l^2$不变的情况下，进一步减小第一项$-\\sum_{l=1}^L c_l \\|\\boldsymbol{G}_l\\|_*$的值（注意到核范数是非负的）。因此这里令目标函数取最小的必要条件是所有$c_1,\\ldots,c_L$均取最大值$\\eta$。\n式$(1)$看起来可能有一些复杂，但是本质上关于$\\eta$是一个简单的二次函数，用初等数学4即可求出它的最优值对应的$\\eta$： $$ \\eta = \\frac{1}{\\lambda}\\sum_l^L \\|\\boldsymbol{G}_l\\|_* $$\n现在我们已经求出了$\\Delta \\boldsymbol{W}_l$的最优范数$c^*_l=\\eta$了，还需要知道它的方向$\\boldsymbol{T}^*_l$\n$$ \\boldsymbol{T}^*_l=\\underset{\\boldsymbol{T}_l:\\|\\boldsymbol{T}_l\\|=1}{\\text{arg min}}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F=-\\underset{\\boldsymbol{T}_l:\\|\\boldsymbol{T}_l\\|=1}{\\text{arg max}}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F $$\n这里直接给出$\\boldsymbol{T}_l$的解，求解的过程放在文末。假设$\\boldsymbol{G}_l\\in\\mathbb{R}^{m\\times n}$的最简SVD分解为$\\boldsymbol{U}_{m\\times r}\\boldsymbol{\\Sigma}_{r\\times r}\\boldsymbol{V}_{r\\times n}^\\top, r=\\text{rank}(\\boldsymbol{G}_l)$，则\n$$ \\boldsymbol{T}^*_l=-\\boldsymbol{U}\\boldsymbol{V}^\\top $$\n也就是说，上述带谱范数约束下的最速下降的解是 $$ \\begin{align} \\Delta \\boldsymbol{W}_l\u0026amp;=-\\eta\\ {\\color[rgb]{0, 0.5, 0.8}{\\boldsymbol{U}\\boldsymbol{V}^\\top}}\\\\ \\end{align} $$\n到这一步，如果读者熟悉Muon优化器的话，会发现$-{\\color[rgb]{0, 0.5, 0.8}{\\boldsymbol{U}\\boldsymbol{V}^\\top}}$正是Muon优化器的更新方向（如果忽略动量估计的话）。另外，对于Shampoo优化器，如果忽略掉左右conditioner的累加的话，也可以得出同样的更新方向。 $$ \\begin{align} \u0026amp;-(\\boldsymbol{G}\\boldsymbol{G}^\\top)^{-\\frac{1}{4}}\\boldsymbol{G}(\\boldsymbol{G}^\\top\\boldsymbol{G})^{-\\frac{1}{4}}\\\\ =\u0026amp;- (\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^\\top\\boldsymbol{V}\\boldsymbol{\\Sigma}\\boldsymbol{U}^\\top)^{-\\frac{1}{4}}\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^\\top(\\boldsymbol{V}\\boldsymbol{\\Sigma}\\boldsymbol{U}^\\top\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^\\top)^{-\\frac{1}{4}}\\\\ =\u0026amp;-(\\boldsymbol{U}\\boldsymbol{\\Sigma}^2\\boldsymbol{U}^\\top)^{-\\frac{1}{4}}\\boldsymbol{U}\\boldsymbol{\\Sigma}\\boldsymbol{V}^\\top(\\boldsymbol{V}\\boldsymbol{\\Sigma}^2\\boldsymbol{V}^\\top)^{-\\frac{1}{4}}\\\\ =\u0026amp;-\\boldsymbol{U}\\boldsymbol{\\Sigma}^{-\\frac{1}{2}}\\boldsymbol{\\Sigma}\\boldsymbol{\\Sigma}^{-\\frac{1}{2}}\\boldsymbol{V}^\\top=-\\boldsymbol{U}\\boldsymbol{V}^\\top\\\\ \\end{align} $$\n总结上面的这一小节内容：Muon、Shampoo优化器本质上是约束每个线性模块的算子诱导范数（在上面的例子中，是谱范数$\\|\\cdot\\|_{\\ell_2\\to\\ell_2}$）衡量的参数更新量，从而推导出使用$\\boldsymbol{U}\\boldsymbol{V}^\\top$替代梯度矩阵$\\boldsymbol{G}$作为更新方向。这种约束的优势是，考虑了每次参数更新对于线性模块的输出的影响，更新的方向相比于标准梯度下降（使用欧氏距离衡量参数更新量）会更稳定。\n在矩阵约束中考虑语义信息 故事到这里还没有结束。在神经网络中，同样是线性模块，根据输入输出语义的不同，表达的含义是不一样的。因此我们可以使用更泛化的诱导范数来约束每个模块参数的更新。\n考虑一个矩阵$\\boldsymbol{W}\\in\\mathbb{R}^{m\\times n}$，它链接着赋范空间$(\\mathbb{R}^n, \\|\\cdot\\|_\\alpha)$和$(\\mathbb{R}^m, \\|\\cdot\\|_\\beta)$，矩阵$\\boldsymbol{W}$的算子诱导范数定义如下 $$ \\|\\boldsymbol{W}\\|_{\\alpha\\to\\beta}=\\underset{\\boldsymbol{x}\\in\\mathbb{R}^n}{\\max} \\frac{\\|\\boldsymbol{W}\\boldsymbol{x}\\|_\\beta}{\\|\\boldsymbol{x}\\|_\\alpha} $$\n在前面的例子中，我们假设了输入与输出都是使用$\\ell_2$范数来衡量的，得到的最优更新方向是$\\boldsymbol{U}\\boldsymbol{V}^\\top$。现在，我们考虑另一个例子——Embedding矩阵，它的输入是one-hot向量(仅token对应的位置是1，其他位置都是0的向量)，此时我们可以使用$\\ell_1\\to\\text{RMS}$的诱导范数（输入空间是用$\\ell_1$范数，输出空间用RMS范数） $$ \\|\\boldsymbol{W}\\|_{\\ell_1\\to\\text{RMS}}=\\underset{\\boldsymbol{x}\\in\\mathbb{R}^n}{\\max} \\frac{\\|\\boldsymbol{W}\\boldsymbol{x}\\|_\\text{RMS}}{\\|\\boldsymbol{x}\\|_{\\ell_1}} $$ 考虑到$\\boldsymbol{x}$总是one-hot向量，则上述诱导范数退化为Embedding RMS Norm的最大值： $$ \\|\\boldsymbol{W}\\|_{\\ell_1\\to\\text{RMS}}=\\underset{1\\le j\\le n}{\\max} \\|\\boldsymbol{W}_{:,j}\\|_\\text{RMS} $$ 参照上一小节的推导，假设这个Embedding矩阵的梯度是$\\boldsymbol{G}$，则对应的最速更新方向由如下给出： $$ -\\underset{\\boldsymbol{T}}{\\text{arg max}}{\\langle \\boldsymbol{G}, \\boldsymbol{T} \\rangle}_F \\quad\\text{s.t. }\\underset{1\\le j\\le n}{\\max} \\|\\boldsymbol{T}_{:,j}\\|_\\text{RMS}=1 $$\n上述问题的解由如下构造（即对梯度的每列做RMS标准化）： $$ \\boldsymbol{T}_{:,j}=\\boldsymbol{G}_{:,j}/\\|\\boldsymbol{G}_{:,j}\\|_\\text{RMS} $$\n在Bernstein \u0026amp; Newhouse 2024.中，作者还针对Conv2D算子给出了更新方向（在Bernstein的框架中，这个更新方向就是给定范数下的对偶映射（duality map））。最近Pethick 2025.把这个框架进一步拓展，给出了$\\ell_1\\to\\infty$诱导范数下的最优更新方向$\\text{sign}(\\boldsymbol{G})$，他们将这个更新规则用在了分类模型的输出分类器上（最终他们的优化器实现类似一个Muon与SignSGD的混合）。总而言之，根据参数矩阵的输入、输出的语义不同，应当选取不同的诱导范数约束，对应的更新规则也不同。\n结语 本篇文章分享了笔者阅读Bernstein一系列文章之后的一些重点摘录。对于深度学习中常见的矩阵参数，可以考虑根据输入与输出的不同语义，对梯度矩阵赋予不同的诱导范数约束，从而得到不同的更新规则。\n这一系列的工作还有很多值得挖掘的地方，例如最优的学习率如何分配（对应到上述优化问题中，我们可以为每个诱导范数的约束项增加一个加权系数）？在Large 2024.中作者提到适当地设计模块之间的约束加权，则不同模型宽度（隐藏层维度）下的最优学习率是一致的（learning rate transfer）。如果这点在更多模型上成立的话，那么大规模神经网络的训练就可以有更加第一性的调参路径了。对于这部分相关工作，笔者还没有做深入的阅读，今后有机会可以再写文章分享。\n附录 求解$\\underset{\\boldsymbol{T}_l:\\|\\boldsymbol{T}_l\\|=1}{\\text{max}}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F$和$\\underset{\\boldsymbol{T}_l:\\|\\boldsymbol{T}_l\\|=1}{\\text{arg max}}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F$（下面的矩阵范数均为谱范数$\\|\\cdot\\|_{\\ell_2\\to\\ell_2}$）：\n假设$\\boldsymbol{G}_l$的SVD分解为$\\boldsymbol{U\\Sigma V}^\\top=\\sum_i \\sigma_i\\boldsymbol{u}_i\\boldsymbol{v}_i^\\top$，则 $$ \\begin{align} {\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F \u0026amp;= \\text{Tr}\\left(\\boldsymbol{G}_l^\\top \\boldsymbol{T}_l\\right)\\\\ \u0026amp;=\\text{Tr}\\left(\\left(\\sum_i \\sigma_i\\boldsymbol{u}_i\\boldsymbol{v}_i^\\top\\right)^\\top \\boldsymbol{T}_l\\right)\\\\ \u0026amp;=\\text{Tr}\\left(\\sum_i \\sigma_i\\boldsymbol{v}_i\\boldsymbol{u}_i^\\top \\boldsymbol{T}_l\\right)\\\\ \u0026amp;=\\text{Tr}\\left(\\sum_i \\sigma_i\\boldsymbol{u}_i^\\top \\boldsymbol{T}_l\\boldsymbol{v}_i\\right)\\\\ \u0026amp;=\\sum_i \\sigma_i\\boldsymbol{u}_i^\\top \\boldsymbol{T}_l\\boldsymbol{v}_i\\quad\\triangleright\\text{括号内是标量}\\\\ \u0026amp;\\le\\sum_i \\sigma_i\\\\ \\end{align} $$\n最后一个不等式成立是由于Cauchy-Schwarz不等式（$\\{\\boldsymbol{u}_i\\},\\{\\boldsymbol{u}_i\\}$是正交规范向量组，范数是1）： $$ |\\boldsymbol{u}_i^\\top \\boldsymbol{T}_l\\boldsymbol{v}_i|\\le \\|\\boldsymbol{u}_i\\| \\|\\boldsymbol{T}_l\\boldsymbol{v}_i\\| \\le \\|\\boldsymbol{u}_i\\| \\|\\boldsymbol{T}_l\\|\\|\\boldsymbol{v}_i\\|=1 $$\n注意到，如果带入$\\boldsymbol{T}_l=\\boldsymbol{U}\\boldsymbol{V}^\\top=\\sum_j \\boldsymbol{u}_j\\boldsymbol{v}_j^\\top$（可以验证此时$\\|\\boldsymbol{T}_l\\|=1$）\n$$ \\begin{align} \\sum_i \\sigma_i\\boldsymbol{u}_i^\\top \\boldsymbol{T}_l\\boldsymbol{v}_i \u0026amp;=\\sum_i \\sigma_i\\boldsymbol{u}_i^\\top (\\sum_j \\boldsymbol{u}_j\\boldsymbol{v}_j^\\top)\\boldsymbol{v}_i\\\\ \u0026amp;=\\sum_i \\sigma_i \\sum_j \\boldsymbol{u}_i^\\top\\boldsymbol{u}_j\\boldsymbol{v}_j^\\top \\boldsymbol{v}_i\\\\ \u0026amp;=\\sum_i \\sigma_i \\end{align} $$ 因此$\\sum_i\\sigma_i$这个上界是可取到的，则 $$ \\begin{split} \\underset{\\boldsymbol{T}_l:\\|\\boldsymbol{T}_l\\|=1}{\\text{max}}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F=\\sum_i\\sigma_i\\|\\boldsymbol{G}_l\\|_*\\\\ \\boldsymbol{U}\\boldsymbol{V}^\\top\\in \\underset{\\boldsymbol{T}_l:\\|\\boldsymbol{T}_l\\|=1}{\\text{arg max}}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F\\\\ \\end{split} $$\n这里如果$\\boldsymbol{G}_l$是满秩的，则argmax问题的解是唯一的（可以用反证法来证明），如果非满秩，则$\\boldsymbol{U}\\boldsymbol{V}^\\top$不是唯一解（不影响主要结论）。\n参考阅读 Jordan 2024. Muon: An optimizer for hidden layers in neural networks Gupta 2018. Shampoo: Preconditioned Stochastic Tensor Optimization Bernstein \u0026amp; Newhouse 2024. Old Optimizer, New Norm: An Anthology Large 2024. Scalable Optimization in the Modular Norm Bernstein \u0026amp; Newhouse 2024. Modular Duality in Deep Learning Pethick 2025. Training Deep Learning Models with Norm-Constrained LMOs 这里直接按照Lagrange乘子展开。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n考虑极端情况，如果直接求和作为约束，那么得到的解可能会让某个参数矩阵变化极大，其他矩阵保持不变，那么目标函数的变化可能是非常剧烈的。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n参考了Bernstein \u0026amp; Newhouse 2024. Old Optimizer, New Norm: An Anthology\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n$ax^2+bx(a\u0026gt;0)$的最小值在$b/2a$取到。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://nil9.net/posts/gd-norm-const/","summary":"\u003cp\u003e在深度学习中最常用的优化方法是梯度下降方法及其变体。在过去很长一段时间中，Adam优化器是NLP社区的默认选择，在ViT出现之后，CV方面的工作也逐渐开始使用Adam和Adam的变体（在ViT之前，一种常见的观点是Adam不适用于Vision任务）。\u003c/p\u003e\n\u003cp\u003e最近Muon优化器在Kimi的新工作带动下又火了一把，相较于Adam优化器需要同时维护一阶、二阶矩估计量，Muon只需要维护一份梯度的动量估计，因此在大规模训练中有很大优势。最近笔者顺着Muon的reference看了Jeremy Bernstein在优化的一些文章，觉得很有意思，因此写这篇文章梳理一下这一系列工作的部分精要。本文的核心论点是：使用诱导范数来约束梯度更新，可以推导出最近的一些新出现的优化方法，这也可能是未来深度学习优化的一个有潜力的探索方向。\u003c/p\u003e\n\u003ch1 id=\"梯度下降recap\"\u003e梯度下降（Recap）\u003c/h1\u003e\n\u003cp\u003e当前深度学习优化算法的基石是梯度下降。之前笔者写过一篇拙文（\u003ca href=\"https://nil9.net/posts/natural-gradient-descent/\"\u003e自然梯度（二）：黎曼距离下的最速下降\u003c/a\u003e）整理过梯度下降的推导，核心的结论是：当我们假设参数空间是一个欧几里得空间、参数的距离可以用欧几里得距离来衡量时，我们在某个点约束\u003ccode\u003e$\\|\\Delta\\theta\\|_2\\le\\epsilon(\\epsilon\u0026gt;0)$\u003c/code\u003e时，\u003ccode\u003e$\\Delta\\theta$\u003c/code\u003e取梯度的反方向时可以让目标函数下降最多（具体的证明请参阅上述引文）。\u003c/p\u003e\n\u003cp\u003e使用梯度下降最大的问题是，它实际上\u003cstrong\u003e忽略了模型的结构\u003c/strong\u003e。换句话说，梯度下降相当于将模型所有参数展平为1维向量，并且用向量2范数来衡量每次更新的「步长」。这种抽象是实用的，但是也存在一定的问题。两组参数有可能在欧几里得空间中距离很近，但是诱导的模型输出空间距离很远。造成的结果就是更新的方向实际上不是目标函数下降最快的方向。\u003c/p\u003e\n\u003cp\u003e这个问题要如何解决呢？在\u003ca href=\"https://nil9.net/posts/natural-gradient-descent/\"\u003e自然梯度（二）：黎曼距离下的最速下降\u003c/a\u003e中，我们介绍了自然梯度方法，即使用Fisher信息矩阵的逆作为梯度的pre-conditioner来矫正梯度下降的方向，从原理上是使用参数更新前后引导的概率分布的KL散度作为每次更新的步长约束。但是对于常见的深度神经网络来说，这样做仍然是不切实际的，因为FIM是一个\u003ccode\u003e$N\\times N$\u003c/code\u003e的大矩阵（其中\u003ccode\u003e$N$\u003c/code\u003e是参数量），对于这么大的矩阵存储或求逆都是很难做到的。\u003c/p\u003e\n\u003ch1 id=\"诱导范数作为步长约束\"\u003e诱导范数作为步长约束\u003c/h1\u003e\n\u003cp\u003e是否有一种更「廉价」的方法，可以考虑\u003cstrong\u003e模型的参数结构\u003c/strong\u003e，同时将\u003cstrong\u003e参数的变化对于输出的影响\u003c/strong\u003e作为约束呢？\u003c/p\u003e\n\u003cp\u003e幸运的是，对于当下最流行的神经网络（e.g., Transformer）而言，模型往往可以拆解为很多小模块，其中最常见的是Linear模块（线性映射，这里忽略bias term）\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ f(\\boldsymbol{x};\\boldsymbol{W})=\\boldsymbol{Wx},\\ \\boldsymbol{W}\\in\\mathbb{R}^{n\\times m},\\boldsymbol{x}\\in \\mathbb{R}^{m}\\\\ $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e在标准的Transformer中，Attention、FFN、LM分类器都是由Linear模块组成的，Embedding从数学原理上也是输入为one-hot encoding的线性映射。假设现在对于某个Linear模块的参数\u003ccode\u003e$\\boldsymbol{W}$\u003c/code\u003e做\u003ccode\u003e$\\Delta\\boldsymbol{W}$\u003c/code\u003e的更新（\u003ccode\u003e$\\boldsymbol{W}'\\leftarrow \\boldsymbol{W}+\\Delta\\boldsymbol{W}$\u003c/code\u003e），我们需要衡量这个更新对于最终输出的影响是多少（从而可以约束这个影响）。由于神经网络比较复杂，衡量\u003ccode\u003e$\\Delta\\boldsymbol{W}$\u003c/code\u003e对于最终目标函数的影响是相对繁琐的，但我们可以退而求其次，衡量\u003ccode\u003e$\\Delta\\boldsymbol{W}$\u003c/code\u003e对于这个Linear模块的输出\u003ccode\u003e$\\boldsymbol{Wx}$\u003c/code\u003e的影响。\n考虑线性模块的输入与输出空间的距离都使用欧几里得范数\u003ccode\u003e$\\|\\cdot\\|_{\\ell_2}$\u003c/code\u003e衡量，那么这个约束可以通过如下不等式实现\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\|\\Delta\\boldsymbol{W}\\boldsymbol{x}\\|_{\\ell_2} \\le {\\color[rgb]{0, 0.5, 0.8}{\\|\\Delta\\boldsymbol{W}\\|_{\\ell_2\\to\\ell_2}}}\\|\\boldsymbol{x}\\|_{\\ell_2} $$\u003c/code\u003e\n这里的\u003ccode\u003e${\\color[rgb]{0, 0.5, 0.8}{\\|\\Delta\\boldsymbol{W}\\|_{\\ell_2\\to\\ell_2}}}$\u003c/code\u003e是矩阵2范数。这个不等式告诉我们，如果约束了参数更新量的谱范数（不等式右侧），也就约束了更新前后这个线性模块输出的变化量。\u003c/p\u003e\n\u003cp\u003e假设现在需要优化的神经网络是由一系列的线性模块堆叠组成（e.g., MLP），我们可以参照梯度下降的推导构造如下的更新\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\underset{\\Delta\\boldsymbol{W}_1,\\ldots,\\Delta\\boldsymbol{W}_L}{\\text{arg min}}\\left[ \\sum_{l=1}^L {\\langle \\boldsymbol{G}_l, \\Delta\\boldsymbol{W}_l \\rangle}_F + \\frac{\\lambda}{2}\\max_{l=1}^L{\\color[rgb]{0, 0.5, 0.8}{\\|\\Delta\\boldsymbol{W}_l\\|^2_{\\ell_2\\to\\ell_2}}} \\right]\\\\ $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这里\u003ccode\u003e$\\boldsymbol{G}_l$\u003c/code\u003e表示\u003ccode\u003e$\\boldsymbol{W}_l$\u003c/code\u003e对应的梯度矩阵（布局与原参数矩阵相同），\u003ccode\u003e${\\langle \\cdot, \\cdot \\rangle}_F$\u003c/code\u003e表示Frobenius内积（对矩阵而言，逐元素相乘求和）。这里之所以使用\u003ccode\u003e$\\max_{l=1}^L$\u003c/code\u003e（而不是直接求和），是因为我们引入这个约束时希望目标函数在\u003ccode\u003e$\\Delta\\boldsymbol{W}_l$\u003c/code\u003e变化下，能够保持平滑的性质\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e，因此需要bound所有参数矩阵更新量的谱范数的最大值。\u003c/p\u003e\n\u003cp\u003e我们来逐步推导这个最小值成立时的\u003ccode\u003e$\\Delta\\boldsymbol{W}_1,\\ldots,\\Delta\\boldsymbol{W}_L$\u003c/code\u003e取值\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e。为了方便，把每个\u003ccode\u003e$\\Delta\\boldsymbol{W}_l$\u003c/code\u003e拆解成大小和方向两部分：\u003ccode\u003e$\\Delta\\boldsymbol{W}_l=c_l\\boldsymbol{T}_l(c_l\\triangleq\\|\\Delta\\boldsymbol{W}_l\\|_{\\ell_2\\to\\ell_2})$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e（为了可读性，下面的\u003ccode\u003e$\\|\\cdots\\|$\u003c/code\u003e均表示谱范数\u003ccode\u003e$\\|\\cdot\\|_{\\ell_2\\to\\ell_2}$\u003c/code\u003e）\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{align} \u0026amp;\\underset{\\Delta\\boldsymbol{W}_1,\\ldots,\\Delta\\boldsymbol{W}_L}{\\text{min}}\\left[ \\sum_{l=1}^L {\\langle \\boldsymbol{G}_l, \\Delta\\boldsymbol{W}_l \\rangle}_F + \\frac{\\lambda}{2}\\max_{l=1}^L\\|\\Delta\\boldsymbol{W}_l\\|^2 \\right]\\\\ \u0026amp;=\\underset{c_1,\\ldots,c_L\\ge 0}{\\text{min}}\\left[ \\sum_{l=1}^L c_l\\min_{\\|\\boldsymbol{T}_l\\|=1}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F + \\frac{\\lambda}{2}\\max_{l=1}^Lc_l^2 \\right]\\\\ \u0026amp;=\\underset{c_1,\\ldots,c_L\\ge 0}{\\text{min}}\\left[ -\\sum_{l=1}^L c_l\\max_{\\|\\boldsymbol{T}_l\\|=1}{\\langle \\boldsymbol{G}_l, \\boldsymbol{T}_l \\rangle}_F + \\frac{\\lambda}{2}\\max_{l=1}^Lc_l^2 \\right]\\\\ \u0026amp;=\\underset{c_1,\\ldots,c_L\\ge 0}{\\text{min}}\\left[ -\\sum_{l=1}^L c_l \\|\\boldsymbol{G}_l\\|_* + \\frac{\\lambda}{2}\\max_{l=1}^Lc_l^2 \\right]\\quad\\triangleright\\|\\cdot\\|_*\\text{表示核范数}\\\\ \u0026amp;=\\underset{\\eta\\ge 0}{\\text{min}}\\left[ -\\sum_{l=1}^L \\eta\\|\\boldsymbol{G}_l\\|_* + \\frac{\\lambda}{2}\\max_{l=1}^L \\eta^2 \\right]\\tag{1}\\\\ \\end{align} $$\u003c/code\u003e\u003c/p\u003e","title":"从约束视角看深度学习优化若干新进展"},{"content":"最近在阅读Muon is Scalable for LLM Training这篇文章的时候注意到他们使用无权重衰减（weight decay）版本的Muon优化LLM的时候，优化器的收敛优势会随着训练过程逐渐消失，又看到@小明同学在评论区提到的一个细节，很多开源的LLM在技术报告中都提到了使用0.1作为权重衰减的系数，觉得是个比较有意思的发现。结合Kimi的文章中关于bf16的简单陈述，笔者在本文中稍微展开讲下，权重衰减对于LLM的低精度训练中有什么作用。\n首先把结论放在前面：除了一般认知中的正则化作用，权重衰减也可能降低精度损失的风险——对于计算机的浮点数而言，绝对值越大，精度越低。对于低精度/混合精度训练而言，使用权重衰减可以控制参数的绝对值范围，从而保证模型参数不落入低精度的数值区间。\n浮点数的存储与精度 上述结论主要与浮点数在计算机内的存储形式有关。学过计算机的一些基本课程的读者可能有印象，浮点数的存储是二进制的形式，分为符号位、指数位和尾数位三段。深度学习中常见的浮点数协议（fp32、fp16、bf16、tf32）的区别在于指数位和尾数位的比特数量不同。由于浮点数是一个「指数」的形式，因此它在实数空间的分布是不均匀的。\n这里我们考虑规范数的情形（指数位非全0），做一点分析。假设符号位、指数位、尾数位（mantissa）的二进制编码分别是$S$、$E$、$M$，那么对应的浮点数为： $$ \\text{value}=(-1)^S\\times 1.M\\times 2^{E-\\text{bias}} $$\n在单精度fp32标准中，$\\text{bias}$取${01111111}_{2}=127_{10}$ .\nfp32浮点数的一个例子\n例如在图中的例子中，$S=0$，$E=\\underbrace{00\\cdots0}_{7\\ 0's}1$，$M=\\underbrace{00\\cdots0}_{22\\ 0's}1$，相应的值为 $$ \\begin{align} \u0026amp;{-1}^0\\times 1.\\underbrace{00\\cdots0}_{22\\ 0's}1_2\\times 2^{00000001_2-{01111111}_{2}}\\\\ \u0026amp;\\approx [1.175494490952134\\times 10^{-38}]_{10} \\end{align} $$\n现在我们来考虑不同的数值范围内的浮点数精度。对于区间范围$[2^{x}, 2^{x+1}],\\forall -126\\le x\\le127$（这里的x已经是经过-bias之后得到的最终指数），我们希望在给定任意浮点数$y\\in[2^{x}, 2^{x+1}]$的基础上增加一个最小量$\\varepsilon$（即区间内两个浮点数的最小间隔），这个增加的过程是通过操纵二进制编码实现的，那么最小间隔只能是通过在$y$的尾数部分加上\n$$ 0.\\underbrace{00\\cdots0}_{22\\ 0's}1 $$\n来实现，对应的最小间隔是 $$ \\begin{align} \\varepsilon \u0026amp;= 0.\\underbrace{00\\cdots0}_{22\\text{ 0's}}1_2\\times 2^{x}\\\\ \u0026amp;=2^{-23}\\times2^{x}=2^{x-23}\\\\ \\end{align} $$\n如果你将上面的公式带入不同的指数位，可以验证与Wikipedia中给出的这张表的Gap是吻合的：\n不同指数位下的最小精度，来源：Wikipedia\n这个计算可以拓展到其他的精度格式：\n对于半精度格式fp16而言，其尾数位有10位，对应的最小间隔是$2^{x-10}$； 对于半精度格式bf16而言，其尾数位有7位，对应的最小间隔是$2^{x-7}$； 从这里也可以看到，bf16相比fp16虽然拓宽了表示范围，但是减少了精度（同样数值范围内的最小间隔更宽了）。\n从这里我们得到了一个结论：计算机存储的浮点数之间的最小间隔随着浮点数绝对值数值增加，指数级地增大，换言之，浮点数（绝对值）数值越大，精度越低。并且这个问题对于fp16或bf16格式的浮点数，问题要更加显著。\n这个结论的另一个引申的问题是舍入误差，假如一个较大的浮点数和一个较小的浮点数相加，由于浮点数的加法（减法过程相当于取补码后相加，结论是类似的）过程需要先将两个数的指数位对齐，因此绝对值较小的数字的尾数的最后几位数字可能会在加法中丢失。这里我们举一个极端的例子来说明。\n假设浮点数存储为fp32格式（8位指数、23位尾数）。 $$ \\begin{align} x=(-1)^0\\times 1.0_2\\times 2^{-1}\\\\ y=(-1)^0\\times 1.0_2\\times 2^{-25}\\\\ \\end{align} $$\n在执行浮点数加法的时候，指数位首先按照大的一个operand的指数对齐，也就是$y$的指数变为$2^{-25}\\to 2^{-1}$，相应地需要将尾数（包括前面隐含的1）向右移动$-1-(-25)=24$位。由于fp32的尾数位只有23位，因此在移动之后$y$变成了$0.000\\cdots0\\times2^{-1}$。也就是说$y$在对齐的过程中直接变成了0！\n读者可以运行如下的代码验证这一结论：\nimport numpy as np x = np.float32(2**-1) # 0.5 y = np.float32(2**-25) # 0.0000001192092896 result = x + y print(f\u0026#34;x = {x}, y = {y}\u0026#34;) print(f\u0026#34;x+y = {result}\u0026#34;) print(f\u0026#34;x == x+y? {x==result}\u0026#34;) # x = 0.5, y = 2.9802322387695312e-08 # x+y = 0.5 # x == x+y? True 对深度模型训练的影响 上面我们从浮点数的存储格式建立了「计算机浮点数的数值绝对值越大，则精度越低」的结论，并且引申到舍入误差的问题。接下来我们把这个现象带入到一个神经网络的训练过程中来看可能引发什么样的问题。\n在神经网络的训练过程中，一个经典的训练过程是（以监督学习为例）：\n（forward）给定当前参数$\\boldsymbol{\\theta}$和小批量数据$\\boldsymbol{x},\\boldsymbol{y}$，计算损失函数$\\mathcal{L}(\\boldsymbol{\\theta};\\boldsymbol{x},\\boldsymbol{y})$； （backward）反向传播，得到每个参数的梯度； （update）更新优化器状态（梯度的统计量，例如梯度动量，Adam中的一、二阶矩统计量），更新模型参数$\\boldsymbol{\\theta}$. 对于这个过程而言，前一节中的结论会造成两方面的结果：\n使用低精度浮点数保存和更新模型参数时，如果模型参数绝对值比较大，而更新的步幅比较小，那么更新会由于舍入误差而失效； 从一个高精度的模型转化为低精度模型的时候，参数的绝对值越大，则丢失的精度越多。 如果读者有训练一些LM或者其他神经网络的经验，可能会发现Transformer这类深度模型在训练过程中，参数的范数会随着训练过程中逐渐增大。Merrill 2020指出对于T5模型而言，其参数范数的增长正比于$\\sqrt{t}$（$t$是更新次数）. 因此，对于训练后期，随着参数的量级逐渐变大，精度变差的风险也会增加。\n值得指出的是，当下的混合精度训练范式一般会在低精度的权重之外，维护一份fp32的权重，优化器的states一般也会使用高精度版本，防止累加的过程中出现严重的舍入误差。而且在低精度的GEMM中，也会使用高精度的accumulator来存储分块内的内积。但是这些操作仍然不能完全杜绝由浮点数存储带来的精度问题。\n混合精度训练的图示，图片来源：https://zhuanlan.zhihu.com/p/678116738\n例如，在模型更新了fp32的备份之后，还需要将fp32的权重转化为低精度的版本，参与后续的forward过程。由于浮点数的精度随着绝对值的增加而降低，因此参数的绝对值越大，在精度的转化中损失的精度也越多。此外，在前向和反向计算的过程中，激活值夜会存在类似的精度损失问题。\n如果我们在训练过程中引入权重衰减：\n$$ \\theta^+\\leftarrow \\theta - \\eta(\\tilde{\\Delta}+{\\color[rgb]{0, 0.5, 0.8}{\\lambda\\theta}}) $$ 其中$\\tilde{\\Delta}$是优化器计算出来的权重更新量，$\\lambda$是权重衰减的系数，那么模型的权重的绝对值就可以得到一定的控制。除了提供一定的正则化效应之外，也能够降低由于模型的参数范数增长而导致的精度损失的风险。\n总结 本文从浮点数的存储原理出发，建立了「数值越大，精度越低」的结论，从而（ad-hoc地）解释了LLM的训练对权重衰减的依赖。但是也需要指出的是，权重的范数增长与模型的结构是有一定关系的，这个规律不一定对所有模型成立。\n参考 Merrill 2020. Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent\nhttps://en.wikipedia.org/wiki/IEEE_754-1985\nMoonshot AI 2025. Muon is Scalable for LLM Training\nDeepseek AI 2024. DeepSeek-V3 Technical Report\n","permalink":"https://nil9.net/posts/wd-model-precision/","summary":"\u003cp\u003e最近在阅读\u003ca href=\"https://github.com/MoonshotAI/Moonlight/blob/master/Moonlight.pdf\"\u003eMuon is Scalable for LLM Training\u003c/a\u003e这篇文章的时候注意到他们使用无权重衰减（weight decay）版本的Muon优化LLM的时候，优化器的收敛优势会随着训练过程逐渐消失，又看到\u003ca href=\"https://www.zhihu.com/people/xiao-ming-tong-xue-86-81\"\u003e@小明同学\u003c/a\u003e在评论区提到的一个细节，很多开源的LLM在技术报告中都提到了使用0.1作为权重衰减的系数，觉得是个比较有意思的发现。结合Kimi的文章中关于bf16的简单陈述，笔者在本文中稍微展开讲下，权重衰减对于LLM的低精度训练中有什么作用。\u003c/p\u003e\n\u003cp\u003e首先把结论放在前面：除了一般认知中的正则化作用，权重衰减也可能降低精度损失的风险——对于计算机的浮点数而言，\u003cstrong\u003e绝对值越大，精度越低\u003c/strong\u003e。对于低精度/混合精度训练而言，使用权重衰减可以控制参数的绝对值范围，从而保证模型参数不落入低精度的数值区间。\u003c/p\u003e\n\u003ch1 id=\"浮点数的存储与精度\"\u003e浮点数的存储与精度\u003c/h1\u003e\n\u003cp\u003e上述结论主要与浮点数在计算机内的存储形式有关。学过计算机的一些基本课程的读者可能有印象，浮点数的存储是二进制的形式，分为符号位、指数位和尾数位三段。深度学习中常见的浮点数协议（fp32、fp16、bf16、tf32）的区别在于指数位和尾数位的比特数量不同。由于浮点数是一个「指数」的形式，\u003cstrong\u003e因此它在实数空间的分布是不均匀的\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e这里我们考虑规范数的情形（指数位非全0），做一点分析。假设符号位、指数位、尾数位（mantissa）的二进制编码分别是\u003ccode\u003e$S$\u003c/code\u003e、\u003ccode\u003e$E$\u003c/code\u003e、\u003ccode\u003e$M$\u003c/code\u003e，那么对应的浮点数为：\n\u003ccode\u003e$$ \\text{value}=(-1)^S\\times 1.M\\times 2^{E-\\text{bias}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e在单精度fp32标准中，\u003ccode\u003e$\\text{bias}$\u003c/code\u003e取\u003ccode\u003e${01111111}_{2}=127_{10}$\u003c/code\u003e .\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\n  \u003cimg alt=\"fp32浮点数的一个例子\" loading=\"lazy\" src=\"/images/wd-model-precision/ieee-fp32-example.png\" title=\"fp32浮点数的一个例子\"\u003e\u003cfigcaption class=\"image-caption\"\u003efp32浮点数的一个例子\u003c/figcaption\u003e\u003c/figure\u003e\u003c/p\u003e\n\u003cp\u003e例如在图中的例子中，\u003ccode\u003e$S=0$\u003c/code\u003e，\u003ccode\u003e$E=\\underbrace{00\\cdots0}_{7\\ 0's}1$\u003c/code\u003e，\u003ccode\u003e$M=\\underbrace{00\\cdots0}_{22\\ 0's}1$\u003c/code\u003e，相应的值为\n\u003ccode\u003e$$ \\begin{align} \u0026amp;{-1}^0\\times 1.\\underbrace{00\\cdots0}_{22\\ 0's}1_2\\times 2^{00000001_2-{01111111}_{2}}\\\\ \u0026amp;\\approx [1.175494490952134\\times 10^{-38}]_{10} \\end{align} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e现在我们来考虑不同的数值范围内的浮点数精度。对于区间范围\u003ccode\u003e$[2^{x}, 2^{x+1}],\\forall -126\\le x\\le127$\u003c/code\u003e（这里的x已经是经过-bias之后得到的最终指数），我们希望在给定任意浮点数\u003ccode\u003e$y\\in[2^{x}, 2^{x+1}]$\u003c/code\u003e的基础上增加一个最小量\u003ccode\u003e$\\varepsilon$\u003c/code\u003e（即区间内两个浮点数的最小间隔），这个增加的过程是通过操纵二进制编码实现的，那么最小间隔只能是通过在\u003ccode\u003e$y$\u003c/code\u003e的尾数部分加上\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ 0.\\underbrace{00\\cdots0}_{22\\ 0's}1 $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e来实现，对应的最小间隔是\n\u003ccode\u003e$$ \\begin{align} \\varepsilon \u0026amp;= 0.\\underbrace{00\\cdots0}_{22\\text{ 0's}}1_2\\times 2^{x}\\\\ \u0026amp;=2^{-23}\\times2^{x}=2^{x-23}\\\\ \\end{align} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e如果你将上面的公式带入不同的指数位，可以验证与\u003ca href=\"https://en.wikipedia.org/wiki/IEEE_754-1985\"\u003eWikipedia\u003c/a\u003e中给出的这张表的Gap是吻合的：\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\n  \u003cimg alt=\"不同指数位下的最小精度\" loading=\"lazy\" src=\"/images/wd-model-precision/ieee-fp32-prec.png\" title=\"不同指数位下的最小精度，来源：Wikipedia\"\u003e\u003cfigcaption class=\"image-caption\"\u003e不同指数位下的最小精度，来源：Wikipedia\u003c/figcaption\u003e\u003c/figure\u003e\u003c/p\u003e\n\u003cp\u003e这个计算可以拓展到其他的精度格式：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e对于半精度格式fp16而言，其尾数位有10位，对应的最小间隔是\u003ccode\u003e$2^{x-10}$\u003c/code\u003e；\u003c/li\u003e\n\u003cli\u003e对于半精度格式bf16而言，其尾数位有7位，对应的最小间隔是\u003ccode\u003e$2^{x-7}$\u003c/code\u003e；\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e从这里也可以看到，bf16相比fp16虽然拓宽了表示范围，但是减少了精度（同样数值范围内的最小间隔更宽了）。\u003c/p\u003e\n\u003cp\u003e从这里我们得到了一个结论：计算机存储的浮点数之间的最小间隔随着浮点数绝对值数值增加，指数级地增大，换言之，\u003cstrong\u003e浮点数（绝对值）数值越大，精度越低\u003c/strong\u003e。并且这个问题对于fp16或bf16格式的浮点数，问题要更加显著。\u003c/p\u003e\n\u003cp\u003e这个结论的另一个引申的问题是\u003cstrong\u003e舍入误差\u003c/strong\u003e，假如一个较大的浮点数和一个较小的浮点数相加，由于浮点数的加法（减法过程相当于取补码后相加，结论是类似的）过程需要先将两个数的指数位对齐，因此绝对值较小的数字的尾数的最后几位数字可能会在加法中丢失。这里我们举一个极端的例子来说明。\u003c/p\u003e\n\u003cp\u003e假设浮点数存储为fp32格式（8位指数、23位尾数）。\n\u003ccode\u003e$$ \\begin{align} x=(-1)^0\\times 1.0_2\\times 2^{-1}\\\\ y=(-1)^0\\times 1.0_2\\times 2^{-25}\\\\ \\end{align} $$\u003c/code\u003e\u003c/p\u003e","title":"为什么LLM一般使用较大的权重衰减系数？"},{"content":"最近Deepseek比较出圈，连带着里面用到的MLA也被讨论得更多了。MLA无疑是一个非常出色的attention改进，但是由于它的KV cache设计，不能很好地兼容RoPE，因此作者们使用了decoupled RoPE这样的「补丁」来引入位置关系，这无疑也增加了实现的复杂度。\n最近，修改注意力KV Cache这一线工作又增添了TPA这个新成员，笔者觉得这篇文章比较有趣，因此希望写一篇简短的导读。之所以叫「导读」，是因为本文不打算太深入文章的formulation和实验，而是从笔者自己的视角出发介绍文章的一些重点贡献。。\nMHA的拆解 最标准的多头注意力（Vaswani的版本），大致可以拆解成3个步骤（这里默认讨论自注意力）1： $$ \\begin{aligned} \\text{step 1 }\u0026amp; \\begin{cases} \\boldsymbol{q}_i^{(h)} = \\boldsymbol{x}_i\\boldsymbol{W}_q^{(h)}\\in\\mathbb{R}^{d_k},\\quad \\boldsymbol{W}_q^{(h)}\\in\\mathbb{R}^{d\\times d_k} \\\\ \\boldsymbol{k}_i^{(h)} = \\boldsymbol{x}_i\\boldsymbol{W}_k^{(h)}\\in\\mathbb{R}^{d_k},\\quad \\boldsymbol{W}_k^{(h)}\\in\\mathbb{R}^{d\\times d_k}\\\\ \\boldsymbol{v}_i^{(h)} = \\boldsymbol{x}_i\\boldsymbol{W}_v^{(h)}\\in\\mathbb{R}^{d_v},\\quad \\boldsymbol{W}_v^{(h)}\\in\\mathbb{R}^{d\\times d_v} \\end{cases} \\\\ \\text{step 2 }\u0026amp; \\begin{cases} \\boldsymbol{o}_t^{(h)} = \\text{Attention}\\left(\\boldsymbol{q}_t^{(h)}, \\boldsymbol{k}_{\\leq t}^{(h)}, \\boldsymbol{v}_{\\leq t}^{(h)}\\right)\\triangleq\\frac{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(h)} \\boldsymbol{k}_i^{(h)}{}^{\\top}\\right)\\boldsymbol{v}_i^{(h)}}{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(h)} \\boldsymbol{k}_i^{(h)}{}^{\\top}\\right)} \\\\ \\end{cases} \\\\ \\text{step 3 }\u0026amp; \\begin{cases} \\boldsymbol{o}_t = \\left[\\boldsymbol{o}_t^{(1)}, \\boldsymbol{o}_t^{(2)}, \\cdots, \\boldsymbol{o}_t^{(H)}\\right] \\end{cases} \\\\ \\end{aligned}\\\\ $$\n这里$\\boldsymbol{x}_1,\\boldsymbol{x}_2,\\cdots,\\boldsymbol{x}_l$是输入向量，上标$h\\in \\{1,\\ldots,H\\}$表示注意力头，$d,d_k,d_v$分别表示输入维度、key维度、value维度。在第一步中，我们将每个token的表征独立地线性投射到不同的自空间上，第二步是标准的点积注意力，第三步是拼接（后续再做一步线性变换）。\n我们重点来审视一下第一个步骤，这里由于每个token的表征是独立操作的（类似FFN），因此我们可以将每个token的表征看做一个样本点，这里做的事情就是将一个$d$维度的向量，转化成3个矩阵$\\tilde{\\boldsymbol{Q}}\\in\\mathbb{R}^{H\\times d_k},\\tilde{\\boldsymbol{K}}\\in\\mathbb{R}^{H\\times d_k},\\tilde{\\boldsymbol{V}}\\in\\mathbb{R}^{H\\times d_v}$，每一个头对应这个矩阵中的一行。接着我们逐行计算每行对应的三组向量的点积注意力，就构成了标准的多头注意力。在标准实现中，这个变换是通过参数矩阵的线性变换+ reshape实现的。\n在自回归模型推理的阶段，这里涉及到的$\\boldsymbol{k}_i^{(h)},\\boldsymbol{v}_i^{(h)}$会被后续的token使用到，因此可以将其缓存起来，避免重复计算，这就是KV cache的思想。序列中每个位置则需要缓存$2Hd_{kv}$个值（实现中一般$d_k=d_v$）。\nTPA：一种低秩重参化技巧 上面我们描述了在标准注意力中，在计算点积注意力之前，需要通过一系列形如$f:\\mathbb{R}^d\\to\\mathbb{R}^{H\\times d'}$的映射，把输入投射到query-key-value子空间。在TPA这个文章中，作者介绍了一Contextual Factorization (CF)技巧来构造这个映射。笔者将这个技巧描述为「低秩重参化」。\n以$\\tilde{\\boldsymbol{Q}}$的构造为例，对于输入$\\boldsymbol{x}_i$，引入两个参数矩阵$\\boldsymbol{W}^A\\in\\mathbb{R}^{d\\times (r\\cdot H)},\\boldsymbol{W}^B\\in\\mathbb{R}^{d\\times (r\\cdot d_k)}$，得到两个向量2 $$ \\begin{align} \\boldsymbol{a}_i \u0026amp;= \\boldsymbol{x}_i \\boldsymbol{W}^A\\in\\mathbb{R}^{r\\cdot H}\\\\ \\boldsymbol{b}_i \u0026amp;= \\boldsymbol{x}_i \\boldsymbol{W}^B\\in\\mathbb{R}^{r\\cdot d_k}\\\\ \\end{align}\\\\ $$\n这里$R$是我们指定的最大的秩，是一个超参数，将这两个向量reshape成矩阵形式： $$ \\begin{align} \\tilde{\\boldsymbol{A}}_i \u0026amp;\\in\\mathbb{R}^{r\\times H}\\\\ \\tilde{\\boldsymbol{B}}_i \u0026amp;\\in\\mathbb{R}^{r\\times d_k}\\\\ \\end{align}\\tag{1}\\\\ $$\n这样我们就可以构造出一个$\\tilde{\\boldsymbol{Q}}$\n$$ \\tilde{\\boldsymbol{Q}}_i = \\frac{1}{R}\\tilde{\\boldsymbol{A}}_i^\\top \\tilde{\\boldsymbol{B}}_i\\in\\mathbb{R}^{H\\times d_k}\\\\ $$\n同样的方法可以构造出$\\tilde{\\boldsymbol{K}}_i\\in\\mathbb{R}^{H\\times d_k}$和$\\tilde{\\boldsymbol{V}}_i\\in\\mathbb{R}^{H\\times d_v}$。按照上一节中的介绍，后面需要做的就是将三组矩阵的每一行分别当做每个注意力头的query-key-value做点积注意力。与标准MHA不同的是，这里的映射是带有非线性的。\n顺便一提，原作中这个重参化的引入是用的外积和的形式，笔者觉得有点冗余了，因为外积和与矩阵的乘法是等价的，相信多数读者对于矩阵的乘法是更加熟悉的。\n更少的KV Cache 使用这种重参化的形式的一个好处是，在推理的时候，只需要缓存$\\{\\tilde{\\boldsymbol{A}}^K_j,\\tilde{\\boldsymbol{B}}^K_j,\\tilde{\\boldsymbol{A}}^V_j,\\tilde{\\boldsymbol{B}}^V_j\\}_{j\\le t}$即可，对应每个token位置的KV Cache量在 $$ r_k(H+d_k)+r_v(H+d_v)\\\\ $$\n如果代入原作的设定$r_k=r_v=2$，TPA的KV Cache可以大致计算为$4(H+d_{kv})$，比起标准MHA的$2Hd_{kv}$要低不少。根据知乎@寒月灼华的计算，Medium大小的模型上，TPA每token的KV Cache为444，相比MHA的2048和MLA的1056，都是更有优势的。\n兼容旋转位置编码 众所周知，现在最广泛使用的位置编码方式RoPE，可以通过在点积注意力的query-key上分别乘上分块对角旋转矩阵来实现高效的相对位置表征3。而在MLA中，由于KV Cache保存的压缩向量并不是点积注意力最终的key，因此不能直接兼容RoPE。而TPA的形式恰好可以直接兼容RoPE，原作中有比较完整的证明过程，这里笔者按照上一节的符号做一个简短的sketch proof。\nRoPE的基本思想是，在第$i$个token位置引入旋转编码矩阵$\\boldsymbol{\\mathcal{R}}_i$，从而 $$ \\left(\\boldsymbol{q}_i\\boldsymbol{\\mathcal{R}}_i\\right)\\left(\\boldsymbol{k}_j\\boldsymbol{\\mathcal{R}}_j\\right)^\\top = \\boldsymbol{q}_i\\boldsymbol{\\mathcal{R}}_{j-i}\\boldsymbol{k}_j^\\top\\tag{2}\\\\ $$\n在KV Cache的框架下，问题的关键在于令key的旋转位置编码被包含在KV Cache中，刚好TPA能够满足这个要求。考虑上一节构造的$\\tilde{\\boldsymbol{Q}},\\tilde{\\boldsymbol{K}}$\n$$ \\begin{align} \\tilde{\\boldsymbol{Q}}_i \u0026amp;= \\frac{1}{R}(\\tilde{\\boldsymbol{A}}_i^Q)^\\top \\tilde{\\boldsymbol{B}}_i^Q\\in\\mathbb{R}^{H\\times d_k}\\\\ \\tilde{\\boldsymbol{K}}_j \u0026amp;= \\frac{1}{R}(\\tilde{\\boldsymbol{A}}_j^K)^\\top \\tilde{\\boldsymbol{B}}_j^K\\in\\mathbb{R}^{H\\times d_k}\\\\ \\end{align}\\\\ $$\n我们已经提到，第$h$个注意力头就是在$\\tilde{\\boldsymbol{Q}},\\tilde{\\boldsymbol{K}},\\tilde{\\boldsymbol{V}}$矩阵的第$h$行向量基础上做点积注意力。我们假设$\\boldsymbol{a}_i^Q$是$\\tilde{\\boldsymbol{A}}_i^Q$的第$h$列，$\\boldsymbol{a}_j^K$是$\\tilde{\\boldsymbol{A}}_j^K$的第$h$列，则在TPA中，第$h$个注意力头的点积注意力的输入分别是 $$ \\begin{align} \\boldsymbol{q}_i \u0026amp;= \\frac{1}{R}\\boldsymbol{a}_j^Q \\tilde{\\boldsymbol{B}}_i^Q\\in\\mathbb{R}^{d_k}\\\\ \\boldsymbol{k}_j \u0026amp;= \\frac{1}{R}\\boldsymbol{a}_j^K \\tilde{\\boldsymbol{B}}_j^K\\in\\mathbb{R}^{d_k}\\\\ \\end{align}\\\\ $$\n按照公式$(2)$的原理，只需要将旋转位置编码乘在上述两项的右侧即可 $$ \\begin{align} \\boldsymbol{q}_i{\\color[rgb]{0, 0.5, 0.8}{\\boldsymbol{\\mathcal{R}}_i}} \u0026amp;= \\frac{1}{R}\\boldsymbol{a}_j^Q \\tilde{\\boldsymbol{B}}_i^Q{\\color[rgb]{0, 0.5, 0.8}{\\boldsymbol{\\mathcal{R}}_i}}=\\frac{1}{R}\\boldsymbol{a}_j^Q \\left(\\tilde{\\boldsymbol{B}}_i^Q{\\color[rgb]{0, 0.5, 0.8}{\\boldsymbol{\\mathcal{R}}_i}}\\right)\\\\ \\boldsymbol{k}_j{\\color[rgb]{0, 0.5, 0.8}{\\boldsymbol{\\mathcal{R}}_j}} \u0026amp;= \\frac{1}{R}\\boldsymbol{a}_j^K \\tilde{\\boldsymbol{B}}_j^K{\\color[rgb]{0, 0.5, 0.8}{\\boldsymbol{\\mathcal{R}}_j}}=\\frac{1}{R}\\boldsymbol{a}_j^K \\left(\\tilde{\\boldsymbol{B}}_j^K{\\color[rgb]{0, 0.5, 0.8}{\\boldsymbol{\\mathcal{R}}_j}}\\right)\\\\ \\end{align}\\\\ $$\n在实现中，这相当于在公式$(1)$的变换之后，分别对$\\tilde{\\boldsymbol{B}}^Q, \\tilde{\\boldsymbol{B}}^K$应用RoPE编码4\nB_q, B_k = apply_rotary_emb(B_q, cos, sin), apply_rotary_emb(B_k, cos, sin) 在推理的时候，将$\\{\\tilde{\\boldsymbol{A}}^K_j,\\tilde{\\boldsymbol{B}}^K_j{\\color[rgb]{0, 0.5, 0.8}{\\boldsymbol{\\mathcal{R}}_j}},\\tilde{\\boldsymbol{A}}^V_j,\\tilde{\\boldsymbol{B}}^V_j\\}_{j\\le t}$缓存，剩余部分正常计算。\n总结 本文简单介绍了Tensor-Product Attention（TPA）的基本方法和两个性质：较少的KV Cache缓存量和RoPE的兼容性。更细节的描述和详尽的实验请阅读Zhang 2025. Tensor Product Attention Is All You Need以及作者维护的仓库tensorgi/T6。\n参考阅读 Zhang 2025. Tensor Product Attention Is All You Need 缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA Transformer升级之路：2、博采众长的旋转式位置编码 本篇中的向量是行向量。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n为了显示简洁，公式中去掉了Q的标注。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n这是逻辑上的描述，实现上不会实例化一整个矩阵。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/tensorgi/T6/blob/bd6dd4ab682a9955d256d395fa9bf0d5da8a804b/model/T6.py#L122C9-L122C84\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://nil9.net/posts/tensor-product-attention/","summary":"\u003cp\u003e最近Deepseek比较出圈，连带着里面用到的MLA也被讨论得更多了。MLA无疑是一个非常出色的attention改进，但是由于它的KV cache设计，不能很好地兼容\u003ca href=\"https://kexue.fm/archives/8265\"\u003eRoPE\u003c/a\u003e，因此作者们使用了decoupled RoPE这样的「补丁」来引入位置关系，这无疑也增加了实现的复杂度。\u003c/p\u003e\n\u003cp\u003e最近，修改注意力KV Cache这一线工作又增添了\u003ca href=\"https://arxiv.org/pdf/2501.06425\"\u003eTPA\u003c/a\u003e这个新成员，笔者觉得这篇文章比较有趣，因此希望写一篇简短的导读。之所以叫「导读」，是因为本文不打算太深入文章的formulation和实验，而是从笔者自己的视角出发介绍文章的一些重点贡献。。\u003c/p\u003e\n\u003ch1 id=\"mha的拆解\"\u003eMHA的拆解\u003c/h1\u003e\n\u003cp\u003e最标准的多头注意力（Vaswani的版本），大致可以拆解成3个步骤（这里默认讨论自注意力）\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e：\n\u003ccode\u003e$$ \\begin{aligned} \\text{step 1 }\u0026amp; \\begin{cases} \\boldsymbol{q}_i^{(h)} = \\boldsymbol{x}_i\\boldsymbol{W}_q^{(h)}\\in\\mathbb{R}^{d_k},\\quad \\boldsymbol{W}_q^{(h)}\\in\\mathbb{R}^{d\\times d_k} \\\\  \\boldsymbol{k}_i^{(h)} = \\boldsymbol{x}_i\\boldsymbol{W}_k^{(h)}\\in\\mathbb{R}^{d_k},\\quad \\boldsymbol{W}_k^{(h)}\\in\\mathbb{R}^{d\\times d_k}\\\\  \\boldsymbol{v}_i^{(h)} = \\boldsymbol{x}_i\\boldsymbol{W}_v^{(h)}\\in\\mathbb{R}^{d_v},\\quad \\boldsymbol{W}_v^{(h)}\\in\\mathbb{R}^{d\\times d_v}  \\end{cases} \\\\ \\text{step 2 }\u0026amp; \\begin{cases} \\boldsymbol{o}_t^{(h)} = \\text{Attention}\\left(\\boldsymbol{q}_t^{(h)}, \\boldsymbol{k}_{\\leq t}^{(h)}, \\boldsymbol{v}_{\\leq t}^{(h)}\\right)\\triangleq\\frac{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(h)} \\boldsymbol{k}_i^{(h)}{}^{\\top}\\right)\\boldsymbol{v}_i^{(h)}}{\\sum_{i\\leq t}\\exp\\left(\\boldsymbol{q}_t^{(h)} \\boldsymbol{k}_i^{(h)}{}^{\\top}\\right)} \\\\  \\end{cases} \\\\ \\text{step 3 }\u0026amp; \\begin{cases} \\boldsymbol{o}_t = \\left[\\boldsymbol{o}_t^{(1)}, \\boldsymbol{o}_t^{(2)}, \\cdots, \\boldsymbol{o}_t^{(H)}\\right]  \\end{cases} \\\\ \\end{aligned}\\\\ $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这里\u003ccode\u003e$\\boldsymbol{x}_1,\\boldsymbol{x}_2,\\cdots,\\boldsymbol{x}_l$\u003c/code\u003e是输入向量，上标\u003ccode\u003e$h\\in \\{1,\\ldots,H\\}$\u003c/code\u003e表示注意力头，\u003ccode\u003e$d,d_k,d_v$\u003c/code\u003e分别表示输入维度、key维度、value维度。在第一步中，我们将每个token的表征独立地线性投射到不同的自空间上，第二步是标准的点积注意力，第三步是拼接（后续再做一步线性变换）。\u003c/p\u003e\n\u003cp\u003e我们重点来审视一下第一个步骤，这里由于每个token的表征是独立操作的（类似FFN），因此我们可以将每个token的表征看做一个样本点，这里做的事情就是将一个\u003ccode\u003e$d$\u003c/code\u003e维度的向量，转化成3个矩阵\u003ccode\u003e$\\tilde{\\boldsymbol{Q}}\\in\\mathbb{R}^{H\\times d_k},\\tilde{\\boldsymbol{K}}\\in\\mathbb{R}^{H\\times d_k},\\tilde{\\boldsymbol{V}}\\in\\mathbb{R}^{H\\times d_v}$\u003c/code\u003e，每一个头对应这个矩阵中的一行。接着我们逐行计算每行对应的三组向量的点积注意力，就构成了标准的多头注意力。在标准实现中，这个变换是通过参数矩阵的线性变换+ reshape实现的。\u003c/p\u003e","title":"Tensor Product Attention (TPA) 导读"},{"content":"上篇文章中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在本篇文章中，我们利用上篇得到的结论，推导自然梯度中为何引入FIM来修正梯度方向，并介绍自然梯度的一些性质。\n梯度下降：欧氏距离下的最速下降 考虑一个最优化任务（$f:\\Theta\\to\\mathbb{R}$）： $$ \\underset{\\theta}{\\operatorname{min}} f(\\theta) $$\n最常见的一阶优化方法是梯度下降/steepest descent： $$ \\theta^+=\\theta-\\eta\\nabla_\\theta f $$\n其中$\\eta$是学习率。这里的「steepest」指的是在约束欧氏距离定义下的步长在极小范围内时，选取梯度的负方向能最大化一步之内目标函数下降的程度。 $$ \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta:\\|\\delta\\|\\le \\epsilon}{\\operatorname{argmin}} f(\\theta+\\delta)\\right)=-\\frac{\\nabla_\\theta f}{\\|\\nabla_\\theta f\\|}\\tag{1} $$\nproof\n对极限内的目标函数做一阶泰勒展开： $$ \\begin{align} \\underset{\\delta:\\|\\delta\\|\\le \\epsilon}{\\operatorname{argmin}} f(\\theta+\\delta) \u0026amp;\\approx \\underset{\\delta:\\|\\delta\\|\\le \\epsilon}{\\operatorname{argmin}} f(\\theta) + \\nabla_\\theta f^\\top\\delta\\\\ \u0026amp;=\\underset{\\delta:\\|\\delta\\|\\le \\epsilon}{\\operatorname{argmin}} \\nabla_\\theta f^\\top\\delta \\end{align} $$\n我们将约束条件稍加改写： $$ \\underset{\\delta}{\\min} \\nabla_\\theta f^\\top\\delta\\quad\\text{s.t. }\\|\\delta\\|^2\\le \\epsilon^2 $$\n定义拉格朗日函数 $$ \\mathcal{L}(\\delta, \\lambda):= \\nabla_\\theta f^\\top\\delta + \\lambda (\\|\\delta\\|^2-\\epsilon^2) $$\n根据KKT条件： $$ \\begin{align} \u0026amp;\\nabla_\\delta\\mathcal{L}(\\delta, \\lambda) = 0 \u0026amp;\\triangleright\\text{Stationarity}\\\\ \u0026amp;\\lambda(\\|\\delta\\|^2-\\epsilon^2)=0 \u0026amp;\\triangleright\\text{Complementary slackness}\\\\ \u0026amp;\\|\\delta\\|^2-\\epsilon^2 \\le0\u0026amp;\\triangleright\\text{Primal feasibility}\\\\ \u0026amp;\\lambda\\ge 0\u0026amp;\\triangleright\\text{Dual feasibility}\\\\ \\end{align} $$\n根据驻点条件得到 $$ \\begin{align} \u0026amp;\\nabla_\\theta f+2\\lambda\\delta=0\\\\ \u0026amp;\\delta = -\\frac{1}{2\\lambda}\\nabla_\\theta f\\\\ \\end{align} $$\n代入互补松弛条件（这里$\\lambda=0$可以排除，因为会造成$\\delta$为unbounded）： $$ \\begin{align} \\lambda\u0026amp;\\left(\\left\\|-\\frac{1}{2\\lambda}\\nabla_\\theta f\\right\\|^2-\\epsilon^2\\right)=0\\\\ \u0026amp;\\lambda = \\frac{1}{2\\epsilon}\\|\\nabla_\\theta f\\|\\\\ \\end{align} $$\n带回驻点条件： $$ \\delta^* = -\\epsilon \\frac{\\nabla_\\theta f}{\\|\\nabla_\\theta f\\|} $$\n将$\\delta^*$代入原极限表达式：\n$$ \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta:\\|\\delta\\|\\le \\epsilon}{\\operatorname{argmin}} f(\\theta+\\delta)\\right)=-\\frac{\\nabla_\\theta f}{\\|\\nabla_\\theta f\\|} $$\n注意到在上述的「steepest」的求解中，对步长的约束条件$\\|\\delta\\|\\le\\epsilon$基于欧几里得距离，这里隐含了如下假设：（1）参数空间是标准欧几里得空间；（2）参数构成一组正交归一（orthonormal）的坐标系统。当这个假设不能很好满足的时候，梯度下降的最速性质可能会大打折扣。\n为了说明这个问题，我们考虑一个二维的二次型函数$f(\\boldsymbol{x})=\\boldsymbol{x}^\\top\\boldsymbol{Ax}$，我们令 $$ \\boldsymbol{A} = \\left[\\begin{matrix}1 \u0026amp; 0.5\\\\0.5 \u0026amp; 2\\end{matrix}\\right] $$\n这个函数的等高线图是一个典型的椭圆型，在这个例子中，参数空间不是标准欧氏空间，而是被$\\boldsymbol{A}$定义的椭球几何所支配。如下图所示，使用标准的梯度下降时，由于梯度方向并不指向最低点，因此优化路径是一条曲线，或者（当学习率过大时）呈Z字形。图片右侧是使用自然梯度的优化路径，后面我们会推导自然梯度的形式。\n使用梯度下降与自然梯度在一个「椭球型」二次型函数上的优化路径\n自然梯度：黎曼距离下的最速下降 在揭示了梯度下降的可能问题之后，我们将公式$(1)$中的约束做如下的泛化（差异处我们用蓝色做了区分）：\n$$ \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} f(\\theta+\\delta)\\right)=?\\tag{2} $$\n这里我们引入了局部的度量张量$G(\\theta)$，上篇文章已经简要介绍过黎曼度量和其定义的线元（局部微小变化的长度）\n$$ |\\delta|^2 = \\delta^\\top G(\\theta) \\delta $$\n对$(2)$式做推导，得到的结果就是自然梯度的方向. $$ \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} f(\\theta+\\delta)\\right)=-CG(\\theta)^{-1}\\nabla_\\theta f \\tag{3} $$\n其中$C$是某个常数，可以被吸收到学习率中。这里的证明框架与梯度下降是基本一致的，只不过对约束条件做了一定修改（高亮为蓝色）：\nproof\n对极限内的目标函数做一阶泰勒展开： $$ \\begin{align} \\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} f(\\theta+\\delta) \u0026amp;\\approx \\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} f(\\theta) + \\nabla_\\theta f^\\top\\delta\\\\ \u0026amp;=\\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} \\nabla_\\theta f^\\top\\delta \\end{align} $$\n定义拉格朗日函数 $$ \\mathcal{L}(\\delta, \\lambda):= \\nabla_\\theta f^\\top\\delta + \\lambda ({\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta} -\\epsilon^2) $$\n根据KKT条件： $$ \\begin{align} \u0026amp;\\nabla_\\delta\\mathcal{L}(\\delta, \\lambda) = 0 \u0026amp;\\triangleright\\text{Stationarity}\\\\ \u0026amp;\\lambda ({\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta} -\\epsilon^2)=0 \u0026amp;\\triangleright\\text{Complementary slackness}\\\\ \u0026amp;{\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta} -\\epsilon^2 \\le0\u0026amp;\\triangleright\\text{Primal feasibility}\\\\ \u0026amp;\\lambda\\ge 0\u0026amp;\\triangleright\\text{Dual feasibility}\\\\ \\end{align} $$\n根据驻点条件得到 $$ \\begin{align} \u0026amp;\\nabla_\\theta f+2\\lambda {\\color[rgb]{0, 0.5, 0.8}G(\\theta)}\\delta=0\\\\ \u0026amp;\\delta = -\\frac{1}{2\\lambda} {\\color[rgb]{0, 0.5, 0.8}G(\\theta)^{-1}}\\nabla_\\theta f\\\\ \\end{align} $$\n代入互补松弛条件（$\\lambda=0$可以排除，因为会造成$\\delta$为unbounded）： $$ \\begin{align} {\\color[rgb]{0, 0.5, 0.8}\\left(-\\frac{1}{2\\lambda} G(\\theta)^{-1}\\nabla_\\theta f\\right)^\\top }\u0026amp;{\\color[rgb]{0, 0.5, 0.8}G(\\theta)\\left(-\\frac{1}{2\\lambda} G(\\theta)^{-1}\\nabla_\\theta f\\right)}-\\epsilon^2=0\\\\ \\lambda \u0026amp;= \\frac{1}{2\\epsilon}{\\color[rgb]{0, 0.5, 0.8}\\sqrt{ \\nabla_\\theta f^\\top G(\\theta)^{-1}\\nabla_\\theta f }}\\\\ \\end{align} $$\n带回驻点条件： $$ \\delta^* = -\\epsilon\\frac{{\\color[rgb]{0, 0.5, 0.8}G(\\theta)^{-1}}\\nabla_\\theta f}{{\\color[rgb]{0, 0.5, 0.8}\\sqrt{ \\nabla_\\theta f^\\top G(\\theta)^{-1}\\nabla_\\theta f }}} $$\n将$\\delta^*$代入原极限表达式：\n$$ \\begin{align} \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} f(\\theta+\\delta)\\right)\u0026amp;=-\\frac{{\\color[rgb]{0, 0.5, 0.8}G(\\theta)^{-1}}\\nabla_\\theta f}{{\\color[rgb]{0, 0.5, 0.8}\\sqrt{ \\nabla_\\theta f^\\top G(\\theta)^{-1}\\nabla_\\theta f }}}\\\\ \u0026amp;=-C{\\color[rgb]{0, 0.5, 0.8}G(\\theta)^{-1}}\\nabla_\\theta f\\\\ \\end{align} $$\n在上述结论中，我们实质上是对标准的梯度下降方向应用了度量张量的逆$G(\\theta)^{-1}$，从而修正了梯度的方向（可以将这个矩阵叫做pre-conditioner）。\n在机器学习中，我们常关注的是概率模型的最大似然优化问题，在自然梯度（一）：Fisher信息矩阵作为黎曼度量中，我们已经建立了Fisher信息矩阵是给定概率分布族的参数空间的黎曼度量张量这一结论。如果我们需要优化的函数是一个概率似然函数$\\ell(\\theta):=\\log p(x|\\theta)$，则自然梯度可以直接由Fisher信息矩阵作为pre-conditioner $$ \\begin{align} \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta+\\delta)\\right) \\le \\epsilon^2}}{\\operatorname{argmin}} \\ell(\\theta+\\delta)\\right)\u0026amp;\\approx\\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top F(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} \\ell(\\theta+\\delta)\\right)\\\\ \u0026amp;=-CF(\\theta)^{-1}\\nabla_\\theta \\ell \\end{align} $$\n对应的参数更新公式为\n$$ \\theta^+=\\theta-\\eta F(\\theta)^{-1}\\nabla_\\theta \\ell(\\theta|x) $$\n拓展到判别模型 在常见的监督学习设定下，我们学习的是一个判别模型，优化目标是一系列条件概率的联合对数似然函数，其中每个输入$\\boldsymbol{x}^{(i)}$对应一个条件概率分布 $$ \\ell(\\theta)=-\\frac{1}{n} \\sum_i^n \\left[\\log p_\\theta(y^{(i)}|\\boldsymbol{x}^{(i)})\\right] $$\n相应地，约束条件需要更改为在每个条件概率的KL散度的期望 $$ \\mathbb{E}_{x\\sim\\tilde{q}(x)}\\left[D_{\\text{KL}}\\left(p(y|x;\\theta)\\|p(y|x;\\theta+\\delta)\\right)\\right] \\le \\epsilon^2 $$\n这里的$\\tilde{q}(x)$是输入数据的真实分布或替代分布（与真实分布接近）。这个约束条件对应的Fisher信息矩阵的形式为\n$$ F(\\theta)=\\mathbb{E}_{x\\sim\\tilde{q}(x)}\\left[ \\mathbb{E}_{y\\sim p(y|x;\\theta)}\\left[ \\nabla_\\theta \\log p(y|x;\\theta)\\nabla_\\theta \\log p(y|x;\\theta)^\\top \\right] \\right]\\tag{4} $$\n自然梯度的特性 与二阶优化的联系与区别\n自然梯度的一般形式中，使用$G(\\theta)^{-1}$作为梯度的pre-conditioner。如果把自然梯度看做一个一般的框架（而不仅仅考虑概率模型），那么当优化目标满足一定条件（e.g.,凸函数）时，二阶优化可以看做是选取Hessian作为自然梯度的度量张量。\n对于常见的概率模型框架（优化对数似然损失），我们选取FIM作为度量张量，可以带来与二阶优化类似的性质，例如，在函数流形的局部曲率比较小的时候（plateau），自然梯度会将更新步长拉得比较大，从而可能有助于快速离开plateau。不过也需要注意，这里的曲率定义在模型的函数流形上，而不是最终的损失函数定义的函数流形上。\nFIM相比Hessian具有一些比较好的特性。一方面，FIM是一个协方差矩阵，它总是半正定的，而Hessian则不然（非正定矩阵的逆是不稳定的）。另一方面，我们观察$(4)$中定义的FIM，注意到内层的期望是定义在模型分布$p(y|x;\\theta)$上的，也就是说，估计一个FIM只需要输入分布和模型分布，而不需要知道标签的真实分布，这在mini-batch特别小（e.g., online learning）的时候非常方便——我们可以在一个无标注的数据集上估计FIM，然后将得到的统计量与一个有标注的batch数据计算得到的梯度结合更新模型参数。\n另外，在特定条件下，自然梯度可以等价于广义-高斯牛顿方法，而后者一般被认为是一个二阶优化方法，可以参考Martens 2020.。\n模型KL约束\n使用FIM的自然梯度通过约束模型在一步更新前后的KL散度得到的「最优」方向，这种约束与模型的参数化方式无关——无论什么样的模型，一步更新的结果都是恒定的KL散度变化约束。在模型的分布距离约束下，优化过程中每一步更新后，模型的分布都不会有非常剧烈的变化，这构成了一种「平滑」的效应，Pascanu \u0026amp; Bengio 2014.认为这一定程度上可以防止过拟合。\n应用限制：复杂度考虑\n到目前为止，自然梯度仍然没有在深度学习中得到广泛应用。自然梯度需要计算FIM，对于包含$M$个参数的模型而言，FIM的空间复杂度为$\\mathcal{O}(M^2)$，对于现在的神经网络而言，这是一个不小的负担——一阶优化方法只需要$\\mathcal{O}(M)$的优化器状态。另外，对一个大矩阵求逆也需要比较大的计算复杂度。将自然梯度推广到大模型中需要引入FIM的结构假设（e.g., 分块对角）。\n总结 在自然梯度的两篇文章中，我们从Fisher信息矩阵（FIM）的定义出发，将FIM与概率模型的参数空间的黎曼度量建立联系。在此基础上，我们推导了自然梯度中为何引入FIM来修正梯度方向，并讨论了自然梯度的特性、与二阶优化的联系与区别、以及应用的限制。\n参考阅读 Amari 1998. Natural Gradient Works Efficiently in Learning Amari. Information Geometry of Neural Networks Pascanu \u0026amp; Bengio 2014. Revisiting Natural Gradient for Deep Networks Martens 2020. New Insights and Perspectives on the Natural Gradient Method ","permalink":"https://nil9.net/posts/natural-gradient-descent/","summary":"\u003cp\u003e\u003ca href=\"https://nil9.net/posts/fisher-info-matrix/\"\u003e上篇文章\u003c/a\u003e中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在本篇文章中，我们利用上篇得到的结论，推导自然梯度中为何引入FIM来修正梯度方向，并介绍自然梯度的一些性质。\u003c/p\u003e\n\u003ch1 id=\"梯度下降欧氏距离下的最速下降\"\u003e梯度下降：欧氏距离下的最速下降\u003c/h1\u003e\n\u003cp\u003e考虑一个最优化任务（\u003ccode\u003e$f:\\Theta\\to\\mathbb{R}$\u003c/code\u003e）：\n\u003ccode\u003e$$ \\underset{\\theta}{\\operatorname{min}} f(\\theta) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e最常见的一阶优化方法是梯度下降/steepest descent：\n\u003ccode\u003e$$ \\theta^+=\\theta-\\eta\\nabla_\\theta f $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中\u003ccode\u003e$\\eta$\u003c/code\u003e是学习率。这里的「steepest」指的是在约束欧氏距离定义下的步长在极小范围内时，选取梯度的负方向能最大化一步之内目标函数下降的程度。\n\u003ccode\u003e$$ \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta:\\|\\delta\\|\\le \\epsilon}{\\operatorname{argmin}}  f(\\theta+\\delta)\\right)=-\\frac{\\nabla_\\theta f}{\\|\\nabla_\\theta f\\|}\\tag{1} $$\u003c/code\u003e\u003c/p\u003e\n\u003cstyle type=\"text/css\"\u003e\n     \n    .notice {\n        --title-color: #fff;\n        --title-background-color: #6be;\n        --content-color: #444;\n        --content-background-color: #e7f2fa;\n    }\n\n    .notice.proof {\n        --title-background-color: rgb(130, 130, 130);\n        --content-background-color: #f7f7f7;\n    }\n\n    .notice.info {\n        --title-background-color: #fb7;\n        --content-background-color: #fec;\n    }\n\n    .notice.tip {\n        --title-background-color: #5a5;\n        --content-background-color: #efe;\n    }\n\n    .notice.warning {\n        --title-background-color: #c33;\n        --content-background-color: #fee;\n    }\n\n     \n\n    body.dark .notice {\n        --title-color: #fff;\n        --title-background-color: #069;\n        --content-color: #ddd;\n        --content-background-color: #023;\n    }\n\n    body.dark .notice.proof {\n        --title-background-color: rgb(129, 129, 129);\n        --content-background-color: rgb(41, 41, 41);\n    }\n\n    body.dark .notice.info {\n        --title-background-color: #a50;\n        --content-background-color: #420;\n    }\n\n    body.dark .notice.tip {\n        --title-background-color: #363;\n        --content-background-color: #121;\n    }\n\n    body.dark .notice.warning {\n        --title-background-color: #800;\n        --content-background-color: #400;\n    }\n\n     \n    .notice {\n        padding: 18px;\n        line-height: 24px;\n        margin-bottom: 24px;\n        border-radius: 4px;\n        color: var(--content-color);\n        background: var(--content-background-color);\n    }\n\n    .notice p:last-child {\n        margin-bottom: 0\n    }\n\n     \n    .notice-title {\n        margin: -18px -18px 12px;\n        padding: 4px 18px;\n        border-radius: 4px 4px 0 0;\n        font-weight: 700;\n        color: var(--title-color);\n        background: var(--title-background-color);\n    }\n\n     \n    .icon-notice {\n        display: inline-flex;\n        align-self: center;\n        margin-right: 8px;\n    }\n\n    .icon-notice img,\n    .icon-notice svg {\n        height: 1em;\n        width: 1em;\n        fill: currentColor;\n    }\n\n    .icon-notice img,\n    .icon-notice.baseline svg {\n        top: .125em;\n        position: relative;\n    }\n\u003c/style\u003e\u003cdiv class=\"notice proof\" \u003e\n    \u003cp class=\"notice-title\"\u003e\n        \u003cspan class=\"icon-notice baseline\"\u003e\n            \n        \u003c/span\u003eproof\u003c/p\u003e","title":"自然梯度（二）：黎曼距离下的最速下降"},{"content":"在一般的梯度下降中，我们认为目标函数梯度的负方向可以最小化一步更新后的目标函数值，这里隐含地假设了参数空间是欧氏空间，且参数构成了一组正交归一的坐标系统。在很多情况下，这一假设是不成立的，作为结果，优化过程的收敛效率可能受到影响。\n作为解决这一问题的一种思路，自然梯度使用Fisher信息矩阵（的逆）作为梯度的pre-conditioner来矫正梯度的方向。本文将分为两篇，在第一篇中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在第二篇中，我们推导自然梯度中为何引入FIM来修正梯度方向，以及自然梯度的一些性质。\nScore function与FIM 假设我们有一个由$\\theta$参数化的概率模型，模型分布为$p(x|\\theta)$，记对数似然函数为$\\ell(\\theta|x):=\\log p(x|\\theta)$。与对数似然函数相关的有两个定义，score function和fisher information。\n定义1（score function）：score function $s(\\theta|x)$被定义为对数似然函数关于参数$\\theta$的梯度\n$$ s(\\theta|x)=\\nabla_\\theta \\ell(\\theta|x) $$\n一些文章会提到score function是用来为参数的好坏打分（score），这是不严谨的。score function中的「score」其实不是为参数打分，而是在Fisher研究的遗传统计问题中给基因异常家庭的「打分」(参见：Interpretation of \u0026ldquo;score\u0026rdquo;)。因此，score function只是约定俗成的一种名称，其实质就是似然函数的梯度，描述的是似然函数对于参数变化的敏感程度。\n性质1：Score function期望为0 $$ \\mathbb{E}_{p(x|\\theta)}[s(\\theta|x)]=\\boldsymbol{0} $$\nproof\n$$ \\begin{align} \\mathbb{E}_{p(x|\\theta)}[s(\\theta|x)] \u0026amp;=\\mathbb{E}_{p(x|\\theta)}[\\nabla_\\theta \\ell(\\theta|x)]\\\\ \u0026amp;=\\int_x p(x|\\theta)\\nabla_\\theta \\log p(x|\\theta) dx\\\\ \u0026amp;=\\int_x p(x|\\theta)\\frac{\\nabla_\\theta p(x|\\theta)}{p(x|\\theta)} dx\\\\ \u0026amp;=\\int_x \\nabla_\\theta p(x|\\theta) dx\\\\ \u0026amp;=\\nabla_\\theta\\int_x p(x|\\theta) dx\\\\ \u0026amp;=\\nabla_\\theta 1 = \\boldsymbol{0}\\\\ \\end{align} $$\n通过性质1可以很顺利地引出Fisher信息矩阵的定义，由于score function是「零均值」的，因此协方差可以直接定义为score function的外积的期望。\n定义2（Fisher information matrix）：Fisher矩阵是score function的方差-协方差矩阵： $$ \\begin{align} \\boldsymbol{F}(\\theta) \u0026amp;:= \\mathbb{E}_{p(x|\\theta)}\\left[ \\left(s(\\theta|x)-\\mathbb{E}_{p(x|\\theta)}[s(\\theta|x)]\\right)\\left(s(\\theta|x)-\\mathbb{E}_{p(x|\\theta)}[s(\\theta|x)]\\right)^\\top \\right]\\\\ \u0026amp;=\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla_\\theta \\ell(\\theta|x)\\nabla_\\theta \\ell(\\theta|x)^\\top \\right]\\quad\\triangleright\\text{性质1} \\end{align} $$\nFisher信息的应用有很多，但我们主要关心它在优化方法中的应用。我们下面会证明，Fisher矩阵是分布$p(x|\\theta)$和$p(x|\\theta')$的KL散度的Hessian在$\\theta'=\\theta$处的取值。这点结论之所以重要，是因为这提供了衡量概率模型参数在函数流形上的距离的一种方法，也就是自然梯度方法的基础。\nFIM与KL散度近似 下面我们尝试通过如下路径建立FIM与KL散度的关系：首先我们证明，FIM与似然函数的Hessian的期望的负值相等（性质2），接着我们利用这一性质，进一步得到：FIM是分布$p(x|\\theta)$和$p(x|\\theta')$的KL散度的Hessian在$\\theta'=\\theta$处的取值（性质3）。通过对KL散度做二阶泰勒展开，可以使用FIM局部近似KL散度，衡量参数在概率模型函数流形上的距离，后面我们会用这个近似关系来推导自然梯度。\n性质2：FIM与似然函数的Hessian的期望的负值相等\n$$ \\boldsymbol{F}(\\theta) = -\\mathbb{E}_{p(x|\\theta)}[\\boldsymbol{H}(\\ell(\\theta|x))] $$\nproof\n$$ \\begin{align} \\boldsymbol{H}(\\ell(\\theta|x)) \u0026amp;=\\nabla^2_\\theta\\left(\\log p(x|\\theta)\\right)\\\\ \u0026amp;=\\nabla_\\theta\\left(\\frac{\\nabla_\\theta p(x|\\theta)}{p(x|\\theta)}\\right)\\\\ \u0026amp;=\\frac{p(x|\\theta)\\nabla^2_\\theta p(x|\\theta)-\\nabla_\\theta p(x|\\theta)\\nabla_\\theta p(x|\\theta)^\\top}{p(x|\\theta)^2}\\\\ \u0026amp;=\\frac{\\nabla^2_\\theta p(x|\\theta)}{p(x|\\theta)}-\\frac{\\nabla_\\theta p(x|\\theta)\\nabla_\\theta p(x|\\theta)^\\top}{p(x|\\theta)^2}\\\\ \u0026amp;=\\frac{\\boldsymbol{H}(p(x|\\theta))}{p(x|\\theta)}-\\nabla_\\theta\\log p(x|\\theta)\\nabla_\\theta\\log p(x|\\theta)^\\top\\\\ \u0026amp;=\\frac{\\boldsymbol{H}(p(x|\\theta))}{p(x|\\theta)}-\\nabla_\\theta\\ell(\\theta|x)\\nabla_\\theta\\ell(\\theta|x)^\\top\\\\ \\end{align} $$\n$$ \\begin{align} \\mathbb{E}_{p(x|\\theta)}[\\boldsymbol{H}(\\ell(\\theta|x))] \u0026amp;=\\mathbb{E}_{p(x|\\theta)}\\left[\\frac{\\boldsymbol{H}(p(x|\\theta))}{p(x|\\theta)}\\right]-\\mathbb{E}_{p(x|\\theta)}\\left[\\nabla_\\theta\\ell(\\theta|x)\\nabla_\\theta\\ell(\\theta|x)^\\top\\right]\\\\ \u0026amp;=\\int_x p(x|\\theta)\\frac{\\boldsymbol{H}(p(x|\\theta))}{p(x|\\theta)}dx-\\boldsymbol{F}(\\theta)\\\\ \u0026amp;=\\int_x \\boldsymbol{H}(p(x|\\theta))dx-\\boldsymbol{F}(\\theta)\\\\ \u0026amp;=\\boldsymbol{H}\\left(\\int_xp(x|\\theta)dx\\right)-\\boldsymbol{F}(\\theta)\\\\ \u0026amp;=\\boldsymbol{H}(1)-\\boldsymbol{F}(\\theta)\\\\ \u0026amp;=0-\\boldsymbol{F}(\\theta)\\\\ \u0026amp;=-\\boldsymbol{F}(\\theta)\\\\ \\end{align} $$\n性质3：FIM是分布$p(x|\\theta)$和$p(x|\\theta')$的KL散度的Hessian在$\\theta'=\\theta$处的取值。\n$$ \\boldsymbol{F}(\\theta)=\\nabla_{\\theta'}^2 D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)|_{\\theta'=\\theta} $$\nproof\n首先将KL散度展开： $$ \\begin{align} D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right) \u0026amp;=\\mathbb{E}_{p(x|\\theta)}\\left[ \\log\\left( \\frac{p(x|\\theta)}{p(x|\\theta')} \\right) \\right]\\\\ \u0026amp;=\\mathbb{E}_{p(x|\\theta)}\\left[ \\log\\left(p(x|\\theta) \\right) \\right]-\\mathbb{E}_{p(x|\\theta)}\\left[ \\log\\left(p(x|\\theta') \\right) \\right]\\\\ \\end{align} $$\n接着，求一阶梯度：\n$$ \\begin{align} \\nabla_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right) \u0026amp;=\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla_{\\theta'}\\log\\left(p(x|\\theta) \\right) \\right]-\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla_{\\theta'}\\log\\left(p(x|\\theta') \\right) \\right]\\\\ \u0026amp;=0-\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla_{\\theta'}\\log\\left(p(x|\\theta') \\right) \\right]\\\\ \u0026amp;=-\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla_{\\theta'}\\log\\left(p(x|\\theta') \\right) \\right]\\\\ \\end{align} $$\n继续求导：\n$$ \\begin{align} \\nabla^2_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right) \u0026amp;=-\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla^2_{\\theta'}\\log\\left(p(x|\\theta') \\right) \\right]\\\\ \u0026amp;=-\\mathbb{E}_{p(x|\\theta)}\\left[ \\boldsymbol{H}(\\ell(\\theta'|x)) \\right]\\\\ \\end{align} $$\n代入$\\theta'=\\theta$： $$ \\begin{align} \\nabla^2_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)|_{\\theta'=\\theta}\u0026amp;=-\\mathbb{E}_{p(x|\\theta)}\\left[ \\boldsymbol{H}(\\ell(\\theta|x)) \\right]\\\\ \u0026amp;=-\\boldsymbol{F}(\\theta) \\quad\\triangleright\\text{性质2} \\end{align} $$\n由于FIM是KL散度的Hessian，可以将其用于KL的近似。\n性质4：FIM可以用于KL散度的局部二阶近似： $$ D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta+\\Delta_\\theta)\\right)\\approx \\frac{1}{2}\\Delta_\\theta^\\top\\boldsymbol{F}(\\theta)\\Delta_\\theta $$\nproof\n我们记$\\theta'=\\theta+\\Delta_\\theta$ $$ \\begin{align} D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)\u0026amp;\\approx D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)|_{\\theta'=\\theta} + \\Delta_\\theta^\\top\\left(\\nabla_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)\\right)|_{\\theta'=\\theta}\\\\ \u0026amp;+\\frac{1}{2}\\Delta_\\theta^\\top\\left(\\nabla^2_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)\\right)|_{\\theta'=\\theta}\\Delta_\\theta\\\\ \u0026amp;=0 + 0 + \\frac{1}{2}\\Delta_\\theta^\\top\\boldsymbol{F}(\\theta)\\Delta_\\theta\\\\ \u0026amp;=\\frac{1}{2}\\Delta_\\theta^\\top\\boldsymbol{F}(\\theta)\\Delta_\\theta\\\\ \\end{align} $$\n其中二阶导到FIM的转化直接利用了性质3；一阶项为0是因为：\n$$ \\begin{align} \\Delta_\\theta^\\top\\left(\\nabla_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)\\right)|_{\\theta'=\\theta} \u0026amp;= \\Delta_\\theta^\\top\\left.\\nabla_{\\theta'}\\mathbb{E}_{p(x|\\theta)}\\left[\\log{\\frac{p(x|\\theta)}{p(x|\\theta')}}\\right]\\right|_{\\theta'=\\theta} \\\\ \u0026amp;= \\Delta_\\theta^\\top\\left.\\mathbb{E}_{p(x|\\theta)}\\left[\\nabla_{\\theta'}{\\log\\frac{p(x|\\theta)}{p(x|\\theta')}}\\right]\\right|_{\\theta'=\\theta}\\\\ \u0026amp;= \\Delta_\\theta^\\top\\left.\\mathbb{E}_{p(x|\\theta)}\\left[\\nabla_{\\theta'}{-\\log p(x|\\theta')}\\right]\\right|_{\\theta'=\\theta}\\\\ \u0026amp;= -\\Delta_\\theta^\\top\\left.\\mathbb{E}_{p(x|\\theta)}\\left[\\nabla_{\\theta'}{\\log p(x|\\theta')}\\right]\\right|_{\\theta'=\\theta} \\\\ \u0026amp;= 0\\quad\\triangleright\\text{性质1} \\end{align} $$\nFIM作为黎曼度量 在建立了FIM可用于KL散度近似的结论之后，我们将其放到一个新的框架里来审视——这种近似关系实际上定义了一种黎曼度量。\n在黎曼几何中，度量张量（metric tensor）提供了一种将坐标值转化为距离（或内积）的工具。假设在流形上的给定点$p$的切空间$T_pM$上定义了一组基向量$\\{e_1,\\cdots,e_n\\}$，黎曼度量可以定义为基向量两两之间的内积： $$ g_{ij} = \\langle \\boldsymbol{e}_i, \\boldsymbol{e}_j\\rangle $$\n对于任意切向量$\\boldsymbol{u},\\boldsymbol{v}\\in T_pM$，可以利用这个黎曼度量来定义切向量的内积：\n$$ \\begin{align} \\langle \\boldsymbol{u}, \\boldsymbol{v}\\rangle _p \u0026amp;= \\langle \\sum_i u_i\\boldsymbol{e}_i, \\sum_j v_j\\boldsymbol{e}_j\\rangle _p\\\\ \u0026amp;=\\sum_{ij}u_i v_j \\langle \\boldsymbol{e}_i, \\boldsymbol{e}_j\\rangle\\\\ \u0026amp;=\\sum_{ij}u_i v_j g_{ij}\\\\ \u0026amp;= \\boldsymbol{u}^\\top \\boldsymbol{G}\\boldsymbol{v} \\end{align} $$\n其中$(\\boldsymbol{G})_{ij}=g_{ij}$，即将度量组织成一个矩阵的形式。基于这个内积可以衍生出$p$点附近的微小距离（线元）的计算，考虑$p$点上的一个微小位移$\\boldsymbol{\\delta}$，则对应的线元为： $$ |\\boldsymbol{\\delta}|^2 = \\boldsymbol{\\delta}^\\top\\boldsymbol{G}\\boldsymbol{\\delta} $$\n作为一个特例，考虑常见的欧氏空间，由于基向量都是规范正交的，任意两个不同基向量的内积是0，且基向量与自身的内积为1，因此对应的度量张量是$\\boldsymbol{G}=\\boldsymbol{I}$，这一点对于任意点都成立。因此欧氏空间对应的内积总是$\\boldsymbol{u}^\\top \\boldsymbol{v}$，线元总是$|\\boldsymbol{\\delta}|^2 = \\boldsymbol{\\delta}^\\top\\boldsymbol{\\delta}$。\n回顾上一节推导得到的性质4（$D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta+\\Delta_\\theta)\\right)\\approx \\frac{1}{2}\\Delta_\\theta^\\top\\boldsymbol{F}(\\theta)\\Delta_\\theta$），通过对比上述的线元的计算，可以发现二者的形式是一致的。\n现在，如果我们将不同参数$\\theta$实例化的分布族$\\mathcal{F}=\\{p_\\theta:\\theta\\in \\Theta\\}$拓展为一个流形$\\mathcal{M}$，使得流形$\\mathcal{M}$上的点与分布族$\\mathcal{F}$中的分布构成双射（一一对应关系）。这样，分布中的参数$\\theta$可以看做流形上的坐标。如果我们将参数的距离定义为参数引导的概率分布的KL散度1，则由性质4的近似，$\\boldsymbol{F}(\\theta)$可以看做点$\\theta$的黎曼度量张量。\n在下篇文章中，我们会通过泛化标准梯度下降的距离约束条件为黎曼距离，从而得到自然梯度方法的更新公式，并给出这种新的约束下对应的一些性质。\n这里的距离是广义而言的，因为KL散度不满足距离的约定。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://nil9.net/posts/fisher-info-matrix/","summary":"\u003cp\u003e在一般的梯度下降中，我们认为目标函数梯度的负方向可以最小化一步更新后的目标函数值，这里隐含地假设了参数空间是欧氏空间，且参数构成了一组正交归一的坐标系统。在很多情况下，这一假设是不成立的，作为结果，优化过程的收敛效率可能受到影响。\u003c/p\u003e\n\u003cp\u003e作为解决这一问题的一种思路，自然梯度使用Fisher信息矩阵（的逆）作为梯度的pre-conditioner来矫正梯度的方向。本文将分为两篇，在第一篇中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在第二篇中，我们推导自然梯度中为何引入FIM来修正梯度方向，以及自然梯度的一些性质。\u003c/p\u003e\n\u003ch1 id=\"score-function与fim\"\u003eScore function与FIM\u003c/h1\u003e\n\u003cp\u003e假设我们有一个由\u003ccode\u003e$\\theta$\u003c/code\u003e参数化的概率模型，模型分布为\u003ccode\u003e$p(x|\\theta)$\u003c/code\u003e，记对数似然函数为\u003ccode\u003e$\\ell(\\theta|x):=\\log p(x|\\theta)$\u003c/code\u003e。与对数似然函数相关的有两个定义，score function和fisher information。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e定义1（score function）\u003c/strong\u003e：score function \u003ccode\u003e$s(\\theta|x)$\u003c/code\u003e被定义为对数似然函数关于参数\u003ccode\u003e$\\theta$\u003c/code\u003e的梯度\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ s(\\theta|x)=\\nabla_\\theta \\ell(\\theta|x) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e一些文章会提到score function是用来为参数的好坏打分（score），这是不严谨的。score function中的「score」其实不是为参数打分，而是在Fisher研究的遗传统计问题中给基因异常家庭的「打分」(参见：\u003ca href=\"https://stats.stackexchange.com/questions/326091/interpretation-of-score\"\u003eInterpretation of \u0026ldquo;score\u0026rdquo;\u003c/a\u003e)。因此，score function只是约定俗成的一种名称，其实质就是似然函数的梯度，描述的是似然函数对于参数变化的敏感程度。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e性质1\u003c/strong\u003e：Score function期望为0\n\u003ccode\u003e$$ \\mathbb{E}_{p(x|\\theta)}[s(\\theta|x)]=\\boldsymbol{0} $$\u003c/code\u003e\u003c/p\u003e\n\u003cstyle type=\"text/css\"\u003e\n     \n    .notice {\n        --title-color: #fff;\n        --title-background-color: #6be;\n        --content-color: #444;\n        --content-background-color: #e7f2fa;\n    }\n\n    .notice.proof {\n        --title-background-color: rgb(130, 130, 130);\n        --content-background-color: #f7f7f7;\n    }\n\n    .notice.info {\n        --title-background-color: #fb7;\n        --content-background-color: #fec;\n    }\n\n    .notice.tip {\n        --title-background-color: #5a5;\n        --content-background-color: #efe;\n    }\n\n    .notice.warning {\n        --title-background-color: #c33;\n        --content-background-color: #fee;\n    }\n\n     \n\n    body.dark .notice {\n        --title-color: #fff;\n        --title-background-color: #069;\n        --content-color: #ddd;\n        --content-background-color: #023;\n    }\n\n    body.dark .notice.proof {\n        --title-background-color: rgb(129, 129, 129);\n        --content-background-color: rgb(41, 41, 41);\n    }\n\n    body.dark .notice.info {\n        --title-background-color: #a50;\n        --content-background-color: #420;\n    }\n\n    body.dark .notice.tip {\n        --title-background-color: #363;\n        --content-background-color: #121;\n    }\n\n    body.dark .notice.warning {\n        --title-background-color: #800;\n        --content-background-color: #400;\n    }\n\n     \n    .notice {\n        padding: 18px;\n        line-height: 24px;\n        margin-bottom: 24px;\n        border-radius: 4px;\n        color: var(--content-color);\n        background: var(--content-background-color);\n    }\n\n    .notice p:last-child {\n        margin-bottom: 0\n    }\n\n     \n    .notice-title {\n        margin: -18px -18px 12px;\n        padding: 4px 18px;\n        border-radius: 4px 4px 0 0;\n        font-weight: 700;\n        color: var(--title-color);\n        background: var(--title-background-color);\n    }\n\n     \n    .icon-notice {\n        display: inline-flex;\n        align-self: center;\n        margin-right: 8px;\n    }\n\n    .icon-notice img,\n    .icon-notice svg {\n        height: 1em;\n        width: 1em;\n        fill: currentColor;\n    }\n\n    .icon-notice img,\n    .icon-notice.baseline svg {\n        top: .125em;\n        position: relative;\n    }\n\u003c/style\u003e\u003cdiv class=\"notice proof\" \u003e\n    \u003cp class=\"notice-title\"\u003e\n        \u003cspan class=\"icon-notice baseline\"\u003e\n            \n        \u003c/span\u003eproof\u003c/p\u003e","title":"自然梯度（一）：Fisher信息矩阵作为黎曼度量"},{"content":"Tied embeddings，即将语言模型中的输入Embeddings权重与输出分类器的权重两组参数共享的操作，一度是语言建模和机器翻译任务的标准配置。在语言模型大规模化之后，这种设计在开源模型中愈发少见了。前几天看到@苏剑林 之前的一篇博客语言模型输出端共享Embedding的重新探索，为tied embeddings的消失提供了一种视角，但也还有值得商榷的地方，本文想从这篇文章出发做一点探讨。\n初始Loss的视角 这里先简要概括一下苏老师文章中的阐述框架1。在使用Transformer做语言建模的时候，可能会使用类似DeepNorm等初始化手段，从而使每一个Transformer Block接近于一个恒等映射2，同时由于词元表征是0均值的，因此LayerNorm可以看做与RMSNorm等价3。所以，假设每个残差分支都初始化为0，假设输入中某个位置的初始embedding是$\\boldsymbol{w}_i$（对应词表中的第$i$个词，维度是$d$），那么最终得到的表征满足\n$$ \\frac{\\boldsymbol{w}_i}{\\Vert\\boldsymbol{w}_i\\Vert \\big/\\sqrt{d}} \\approx \\frac{\\boldsymbol{w}_i}{\\sigma} $$\n假设在该位置的真实标签是词元$j$，则损失函数可以由如下逼近\n$$ \\begin{align}\\mathcal{L}\\triangleq -\\log p(j|i) \u0026amp;= \\log \\sum\\limits_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma} - \\boldsymbol{w}_i\\cdot \\boldsymbol{w}_j \\big/ \\sigma \\\\ \u0026amp;\\approx \\log \\sum_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma}\\\\ \u0026amp;=\\log \\left(e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_i / \\sigma} + \\sum\\limits_{k|k\\neq i} e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma}\\right)\\\\ \u0026amp;\\approx\\log \\left({\\color[rgb]{0, 0.5, 0.8}e^{d \\sigma}} + (n-1)\\right) \\end{align} $$\n其中$|n|$是词表大小。在常见的模型维度下，这里的第一项${\\color[rgb]{0, 0.5, 0.8}e^{d \\sigma}}$是比较大的。我们可以代入几个维度值看下第一项的大小，这里我们假设词表大小是32k，并考虑两种$\\sigma$取法，一种是比较常见的初始化超参数$\\sigma=0.02$，一种是取$\\sigma=1/\\sqrt{d}$。可以看到无论是哪种初始化方法，对应的${\\color[rgb]{0, 0.5, 0.8}e^{d \\sigma}}$都已经远远超过词表大小，响应地初始损失值也处于比较高的水平（按均匀分布的交叉熵是$\\log(n)\\approx 10.37$）。\n不同设定下的「初始损失值」\n以上是苏文中给出的关于语言建模中不再共享embedding的一个视角——tied embeddings会使语言模型的初始损失值很大。\n但这个问题实际上可以用一个rescale来解决，我们可以简单地将输出端乘以$1/\\sqrt{d}$，则损失函数可以做如下近似 $$ \\begin{align}\\mathcal{L} \u0026amp;= \\log \\sum\\limits_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / (\\sigma\\sqrt{d})} - \\boldsymbol{w}_i\\cdot \\boldsymbol{w}_j \\big/(\\sigma\\sqrt{d}) \\\\ \u0026amp;\\approx\\log \\left({\\color[rgb]{0, 0.5, 0.8}e^{\\sigma\\sqrt{d}}} + (n-1)\\right) \\end{align} $$\n这时候，对于常见的$\\sigma=1/\\sqrt{d}$或者$\\sigma=0.02$，${\\color[rgb]{0, 0.5, 0.8}e^{\\sigma\\sqrt{d}}}$这一项相对于词表大小都可以忽略不计了。\n事实上，早期共享embeddings的预训练模型T5的实现4中就使用了这个技巧：\nif self.shared_embedding_and_softmax_weights: logits = mtf.einsum( [x * (self.model_dim.size ** -0.5), embedding_weights], reduced_dims=[self.model_dim]) 另外，在一些公开的预训练实现中，一般残差项的初始化不会使用特别小的值，例如在OLMo-2中，就是对残差分支中的各个参数矩阵就是直接用了标准差为0.02的truncated normal初始化。\n我们可以使用Llama的模型结构做一个简单的实验，我们使用常见的正态分布初始化，在固定层数为12的情况下，测试不同embedding维度下的初始loss值，结果如下表所示。\n初始loss 768 1024 2048 4096 $\\log(n)$ 10.37 - - - untied 10.52 10.56 10.78 11.19 tied 10.53 10.58 10.77 11.24 untied+rescale 10.37 10.37 10.37 10.37 tied+rescale 10.37 10.37 10.37 10.37 可以看到，\n无论是否应用tied embeddings，初始loss都有略高于$\\log(n)$的情况； 在输出端应用rescale技巧，可以将初始loss控制在$\\log(n)$左右。 寻根溯源 笔者认为，初始Loss虽然是一个非常好的视角，但是不能解释当前tied embeddings的式微。讨论tied embeddings的应用，还得稍微追溯学术史，先看看他们是为何被提出的。\n在语言建模中引入tied embeddings技巧可以追溯到LSTM-LM时代的两篇工作：Inan 2016.和Press and Wolf 2017.。其中，Inan 2016.通过类似KD的框架构造出一种soft label\n$$ \\begin{aligned} \\boldsymbol{u}_t \u0026amp;= \\boldsymbol{L}\\boldsymbol{y}^{*}_t \\\\ \\tilde{\\boldsymbol{y}}_t \u0026amp;= \\text{softmax}(\\frac{\\boldsymbol{L}^\\top \\boldsymbol{u}_t}{\\tau}) \\end{aligned} $$\n这里$\\boldsymbol{L}, \\boldsymbol{y}^{*}_t$分别表示embedding权重和第$t$个位置的目标词元。作者论证了在一定的假设下，tied embeddings设定的语言模型相当于在隐式地学习这个soft label（而不是一般的one-hot目标）。\nPress and Wolf 2017.则是通过一系列实验论证了如下几个结论5：\nRNNLM使用tied embeddings时，embeddings的演进方式更接近与untied版本中输出端的embeddings； 使用tied embeddings可以有效降低语言建模中的PPL（PTB数据集），无论是否使用dropout均成立； 在不使用dropout的情况下，在输出embedding之前添加一个额外的投影$P$，并对$P$添加正则化loss，可以进一步降低PPL指标。 这篇文章还提出在机器翻译模型中，对于en-fr这样比较相似的语言，可以在两个语言的语料合集上联合训练一个tokenizer，共享encoder与decoder的embeddings（即encoder的输入、decoder的输入与输出共享一个参数矩阵），后来的Transformer(Vaswani 2017.)也沿用了这一做法。笔者认为这就是初期的很多预训练模型都不约而同地沿用tied embeddings的设定的原因。\n但是如今回顾这两篇文章的时候，我们注意到几点：\n当时的语言模型一般基于浅层的RNN，输入与输出的embeddings参数在模型中占比很大； 当时的实验基于PTB和WikiText数据集，相对于如今的预训练语料规模，可谓是非常小了，尤其是前者。 笔者认为，tied embeddings的有效性与数据和模型规模离不开关系。当数据与模型的规模比较有限时，tied embeddings可以作为一种很好的正则化手段（显著降低参数数量），从Press and Wolf 2017.的实验来看，在PTB这样的小数据集上，tied embeddings的语言模型在训练集上的PPL并不占优势，这表明它的作用可能有部分来自于过拟合风险的降低。\n现在的LLM模做规模化主要是通过加大隐藏层维度和模型层数，non-embedding部分的参数量按$\\mathcal{O}(Ld^2)$的级别增长，而embeddings的参数量只随着隐藏层维度线性增长，因此现有的LLM的embeddings所占参数比例已经非常小了，通过tied embeddings减少参数量的作用非常有限。另外，现在的预训练语料的词元规模也通常在万亿这个量级，与PTB这种训练集不到一百万词的数据集已经不能同日而语了。\n训练的不稳定、工程的限制 前面我们提到，tied embeddings是源于数据与模型规模都较小的LSTMLM时期的一种正则化方法，逐渐成为一项标准设定，在预训练的早期也被沿用了下来。如今在数据与模型规模化的趋势下，正则化的强问题意识已经逐渐不成立了，这种强正则甚至可能成为训练的负担。例如，在OLMo的talk中作者提到，tied embeddings在7B的模型中会造成训练的不稳定。\n除此之外，在语言模型规模化以后，模型的训练越来越依赖于各种跨节点并行计算方法。而使用tied embeddings实际上对并行方法的选择也有一定的限制。例如，使用流水线并行（Pipeline Parallelism）要求将模型纵向拆分部署在多个节点上，那么此时如果将输入与输出层看做两个不同的层，部署在不同的节点上，则首先这两部分参数共享不会节约任何的存储，还需要付出额外的通信成本来同步两个层的梯度。不过笔者觉得这个原因是次要的，如果收益是正向的，那么额外的同步步骤也是值得的。\n结语 本文从语言模型输出端共享Embedding的重新探索中的初始loss视角出发，拓展讨论了在语言建模规模化之后，tied embeddings操作不再作为标准设定的原因：模型与数据规模的变化使得正则化的问题意识不再，且从一些公开的实验来看，tied embeddings可能引发训练的不稳定6，此外tied embeddings也对并行方法的选型有一定限制。\n拓展阅读 Inan 2016. Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling Press and Wolf 2017. Using the Output Embedding to Improve Language Models 语言模型输出端共享Embedding的重新探索 详细内容请查看原文。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n除了DeepNorm，ReZero等优化也有类似的思想。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n在常见实现中，LayerNorm在初始化时，$\\gamma,\\beta$参数分别被初始化为1和0.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n这里略过关于embedding similarity测验的结论。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n在深度学习领域，经验结论很重要，尤其是对于LLM这样试错成本较高的应用中。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://nil9.net/posts/tied-embeddings-in-lm/","summary":"\u003cp\u003eTied embeddings，即将语言模型中的输入Embeddings权重与输出分类器的权重两组参数共享的操作，一度是语言建模和机器翻译任务的标准配置。在语言模型大规模化之后，这种设计在开源模型中愈发少见了。前几天看到\u003ca href=\"https://www.zhihu.com/people/su-jian-lin-22\"\u003e@苏剑林\u003c/a\u003e 之前的一篇博客\u003ca href=\"https://kexue.fm/archives/9698\"\u003e语言模型输出端共享Embedding的重新探索\u003c/a\u003e，为tied embeddings的消失提供了一种视角，但也还有值得商榷的地方，本文想从这篇文章出发做一点探讨。\u003c/p\u003e\n\u003ch1 id=\"初始loss的视角\"\u003e初始Loss的视角\u003c/h1\u003e\n\u003cp\u003e这里先简要概括一下苏老师文章中的阐述框架\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e。在使用Transformer做语言建模的时候，可能会使用类似DeepNorm等初始化手段，从而使每一个Transformer Block接近于一个恒等映射\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e，同时由于词元表征是0均值的，因此LayerNorm可以看做与RMSNorm等价\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e。所以，假设每个残差分支都初始化为0，假设输入中某个位置的初始embedding是\u003ccode\u003e$\\boldsymbol{w}_i$\u003c/code\u003e（对应词表中的第\u003ccode\u003e$i$\u003c/code\u003e个词，维度是\u003ccode\u003e$d$\u003c/code\u003e），那么最终得到的表征满足\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\frac{\\boldsymbol{w}_i}{\\Vert\\boldsymbol{w}_i\\Vert \\big/\\sqrt{d}} \\approx \\frac{\\boldsymbol{w}_i}{\\sigma} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e假设在该位置的真实标签是词元\u003ccode\u003e$j$\u003c/code\u003e，则损失函数可以由如下逼近\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{align}\\mathcal{L}\\triangleq -\\log p(j|i) \u0026amp;= \\log \\sum\\limits_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma} - \\boldsymbol{w}_i\\cdot \\boldsymbol{w}_j \\big/ \\sigma \\\\ \u0026amp;\\approx \\log \\sum_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma}\\\\ \u0026amp;=\\log \\left(e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_i / \\sigma} + \\sum\\limits_{k|k\\neq i} e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma}\\right)\\\\ \u0026amp;\\approx\\log \\left({\\color[rgb]{0, 0.5, 0.8}e^{d \\sigma}} + (n-1)\\right) \\end{align} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中\u003ccode\u003e$|n|$\u003c/code\u003e是词表大小。在常见的模型维度下，这里的第一项\u003ccode\u003e${\\color[rgb]{0, 0.5, 0.8}e^{d \\sigma}}$\u003c/code\u003e是比较大的。我们可以代入几个维度值看下第一项的大小，这里我们假设词表大小是32k，并考虑两种\u003ccode\u003e$\\sigma$\u003c/code\u003e取法，一种是比较常见的初始化超参数\u003ccode\u003e$\\sigma=0.02$\u003c/code\u003e，一种是取\u003ccode\u003e$\\sigma=1/\\sqrt{d}$\u003c/code\u003e。可以看到无论是哪种初始化方法，对应的\u003ccode\u003e${\\color[rgb]{0, 0.5, 0.8}e^{d \\sigma}}$\u003c/code\u003e都已经远远超过词表大小，响应地初始损失值也处于比较高的水平（按均匀分布的交叉熵是\u003ccode\u003e$\\log(n)\\approx 10.37$\u003c/code\u003e）。\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\n  \u003cimg loading=\"lazy\" src=\"/images/tied-embeddings-in-lm/init_loss.png\" title=\"不同设定下的「初始损失值」\"\u003e\u003cfigcaption class=\"image-caption\"\u003e不同设定下的「初始损失值」\u003c/figcaption\u003e\u003c/figure\u003e\u003c/p\u003e\n\u003cp\u003e以上是苏文中给出的关于语言建模中不再共享embedding的一个视角——tied embeddings会使语言模型的初始损失值很大。\u003c/p\u003e","title":"关于语言建模中的Tied Embeddings的一点探讨"}]