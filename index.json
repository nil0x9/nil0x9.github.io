[{"content":"在一般的梯度下降中，我们认为目标函数梯度的负方向可以最小化一步更新后的目标函数值，这里隐含地假设了参数空间是欧氏空间，且参数构成了一组正交归一的坐标系统。在很多情况下，这一假设是不成立的，作为结果，优化过程的收敛效率可能受到影响。\n作为解决这一问题的一种思路，自然梯度使用Fisher信息矩阵（的逆）作为梯度的pre-conditioner来矫正梯度的方向。本文将分为两篇，在第一篇中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在第二篇中，我们介绍自然梯度中如何引入FIM来修正梯度方向，以及其带来的一些性质。\nScore function与FIM 假设我们有一个由$\\theta$参数化的概率模型，模型分布为$p(x|\\theta)$，记对数似然函数为$\\ell(\\theta|x):=\\log p(x|\\theta)$。与对数似然函数相关的有两个定义，score function和fisher information。\n定义1（score function）：score function $s(\\theta|x)$被定义为对数似然函数关于参数$\\theta$的梯度\n$$ s(\\theta|x)=\\nabla_\\theta \\ell(\\theta|x) $$\n一些文章会提到score function是用来为参数的好坏打分（score），这是不严谨的。score function中的「score」其实不是为参数打分，而是在Fisher研究的遗传统计问题中给基因异常家庭的「打分」(参见：Interpretation of \u0026ldquo;score\u0026rdquo;)。因此，score function只是约定俗成的一种名称，其实质就是似然函数的梯度，描述的是似然函数对于参数变化的敏感程度。\n性质1：Score function期望为0（$\\mathbb{E}_{p(x|\\theta)}[s(\\theta|x)]=\\boldsymbol{0}$）.\nproof\n$$ \\begin{align} \\mathbb{E}_{p(x|\\theta)}[s(\\theta|x)] \u0026amp;=\\mathbb{E}_{p(x|\\theta)}[\\nabla_\\theta \\ell(\\theta|x)]\\\\ \u0026amp;=\\int_x p(x|\\theta)\\nabla_\\theta \\log p(x|\\theta) dx\\\\ \u0026amp;=\\int_x p(x|\\theta)\\frac{\\nabla_\\theta p(x|\\theta)}{p(x|\\theta)} dx\\\\ \u0026amp;=\\int_x \\nabla_\\theta p(x|\\theta) dx\\\\ \u0026amp;=\\nabla_\\theta\\int_x p(x|\\theta) dx\\\\ \u0026amp;=\\nabla_\\theta 1 = \\boldsymbol{0}\\\\ \\end{align} $$\n通过性质1可以很顺利地引出Fisher信息矩阵的定义，由于score function是「零均值」的，因此协方差可以直接定义为score function的外积的期望。\n定义2（Fisher information matrix）：Fisher矩阵是score function的方差-协方差矩阵： $$ \\begin{align} \\boldsymbol{F}(\\theta) \u0026amp;:= \\mathbb{E}_{p(x|\\theta)}\\left[ \\left(s(\\theta|x)-\\mathbb{E}_{p(x|\\theta)}[s(\\theta|x)]\\right)\\left(s(\\theta|x)-\\mathbb{E}_{p(x|\\theta)}[s(\\theta|x)]\\right)^\\top \\right]\\\\ \u0026amp;=\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla_\\theta \\ell(\\theta|x)\\nabla_\\theta \\ell(\\theta|x)^\\top \\right]\\quad\\triangleright\\text{性质1} \\end{align} $$\nFisher信息的应用有很多，但我们主要关心它在优化方法中的应用。我们下面会证明，Fisher矩阵是分布$p(x|\\theta)$和$p(x|\\theta')$的KL散度的Hessian在$\\theta'=\\theta$处的取值。这点结论之所以重要，是因为这提供了衡量概率模型参数在函数流形上的距离的一种方法，也就是自然梯度方法的基础。\nFIM与KL散度近似 下面我们尝试通过如下路径建立FIM与KL散度的关系：首先我们证明，FIM与似然函数的Hessian的期望的负值相等（性质2），接着我们利用这一性质，进一步得到：FIM是分布$p(x|\\theta)$和$p(x|\\theta')$的KL散度的Hessian在$\\theta'=\\theta$处的取值（性质3）。通过对KL散度做二阶泰勒展开，可以使用FIM局部近似KL散度，衡量参数在概率模型函数流形上的距离，后面我们会用这个近似关系来推导自然梯度。\n性质2：FIM与似然函数的Hessian的期望的负值相等\n$$ \\boldsymbol{F}(\\theta) = -\\mathbb{E}_{p(x|\\theta)}[\\boldsymbol{H}(\\ell(\\theta|x))] $$\nproof\n$$ \\begin{align} \\boldsymbol{H}(\\ell(\\theta|x)) \u0026amp;=\\nabla^2_\\theta\\left(\\log p(x|\\theta)\\right)\\\\ \u0026amp;=\\nabla_\\theta\\left(\\frac{\\nabla_\\theta p(x|\\theta)}{p(x|\\theta)}\\right)\\\\ \u0026amp;=\\frac{p(x|\\theta)\\nabla^2_\\theta p(x|\\theta)-\\nabla_\\theta p(x|\\theta)\\nabla_\\theta p(x|\\theta)^\\top}{p(x|\\theta)^2}\\\\ \u0026amp;=\\frac{\\nabla^2_\\theta p(x|\\theta)}{p(x|\\theta)}-\\frac{\\nabla_\\theta p(x|\\theta)\\nabla_\\theta p(x|\\theta)^\\top}{p(x|\\theta)^2}\\\\ \u0026amp;=\\frac{\\boldsymbol{H}(p(x|\\theta))}{p(x|\\theta)}-\\nabla_\\theta\\log p(x|\\theta)\\nabla_\\theta\\log p(x|\\theta)^\\top\\\\ \u0026amp;=\\frac{\\boldsymbol{H}(p(x|\\theta))}{p(x|\\theta)}-\\nabla_\\theta\\ell(\\theta|x)\\nabla_\\theta\\ell(\\theta|x)^\\top\\\\ \\end{align} $$\n$$ \\begin{align} \\mathbb{E}_{p(x|\\theta)}[\\boldsymbol{H}(\\ell(\\theta|x))] \u0026amp;=\\mathbb{E}_{p(x|\\theta)}\\left[\\frac{\\boldsymbol{H}(p(x|\\theta))}{p(x|\\theta)}\\right]-\\mathbb{E}_{p(x|\\theta)}\\left[\\nabla_\\theta\\ell(\\theta|x)\\nabla_\\theta\\ell(\\theta|x)^\\top\\right]\\\\ \u0026amp;=\\int_x p(x|\\theta)\\frac{\\boldsymbol{H}(p(x|\\theta))}{p(x|\\theta)}dx-\\boldsymbol{F}(\\theta)\\\\ \u0026amp;=\\int_x \\boldsymbol{H}(p(x|\\theta))dx-\\boldsymbol{F}(\\theta)\\\\ \u0026amp;=\\boldsymbol{H}\\left(\\int_xp(x|\\theta)dx\\right)-\\boldsymbol{F}(\\theta)\\\\ \u0026amp;=\\boldsymbol{H}(1)-\\boldsymbol{F}(\\theta)\\\\ \u0026amp;=0-\\boldsymbol{F}(\\theta)\\\\ \u0026amp;=-\\boldsymbol{F}(\\theta)\\\\ \\end{align} $$\n性质3：FIM是分布$p(x|\\theta)$和$p(x|\\theta')$的KL散度的Hessian在$\\theta'=\\theta$处的取值。\n$$ \\boldsymbol{F}(\\theta)=\\nabla_{\\theta'}^2 D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)|_{\\theta'=\\theta} $$\nproof\n首先将KL散度展开： $$ \\begin{align} D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right) \u0026amp;=\\mathbb{E}_{p(x|\\theta)}\\left[ \\log\\left( \\frac{p(x|\\theta)}{p(x|\\theta')} \\right) \\right]\\\\ \u0026amp;=\\mathbb{E}_{p(x|\\theta)}\\left[ \\log\\left(p(x|\\theta) \\right) \\right]-\\mathbb{E}_{p(x|\\theta)}\\left[ \\log\\left(p(x|\\theta') \\right) \\right]\\\\ \\end{align} $$\n接着，求一阶梯度：\n$$ \\begin{align} \\nabla_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right) \u0026amp;=\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla_{\\theta'}\\log\\left(p(x|\\theta) \\right) \\right]-\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla_{\\theta'}\\log\\left(p(x|\\theta') \\right) \\right]\\\\ \u0026amp;=0-\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla_{\\theta'}\\log\\left(p(x|\\theta') \\right) \\right]\\\\ \u0026amp;=-\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla_{\\theta'}\\log\\left(p(x|\\theta') \\right) \\right]\\\\ \\end{align} $$\n继续求导：\n$$ \\begin{align} \\nabla^2_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right) \u0026amp;=-\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla^2_{\\theta'}\\log\\left(p(x|\\theta') \\right) \\right]\\\\ \u0026amp;=-\\mathbb{E}_{p(x|\\theta)}\\left[ \\boldsymbol{H}(\\ell(\\theta'|x)) \\right]\\\\ \\end{align} $$\n代入$\\theta'=\\theta$： $$ \\begin{align} \\nabla^2_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)|_{\\theta'=\\theta}\u0026amp;=-\\mathbb{E}_{p(x|\\theta)}\\left[ \\boldsymbol{H}(\\ell(\\theta|x)) \\right]\\\\ \u0026amp;=-\\boldsymbol{F}(\\theta) \\quad\\triangleright\\text{性质2} \\end{align} $$\n由于FIM是KL散度的Hessian，可以将其用于KL的近似。\n性质4：FIM可以用于KL散度的局部二阶近似： $$ D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta+\\Delta_\\theta)\\right)\\approx \\frac{1}{2}\\Delta_\\theta^\\top\\boldsymbol{F}(\\theta)\\Delta_\\theta $$\nproof\n我们记$\\theta'=\\theta+\\Delta_\\theta$ $$ \\begin{align} D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)\u0026amp;\\approx D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)|_{\\theta'=\\theta} + \\Delta_\\theta^\\top\\left(\\nabla_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)\\right)|_{\\theta'=\\theta}\\\\ \u0026amp;+\\frac{1}{2}\\Delta_\\theta^\\top\\left(\\nabla^2_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)\\right)|_{\\theta'=\\theta}\\Delta_\\theta\\\\ \u0026amp;=0 + 0 + \\frac{1}{2}\\Delta_\\theta^\\top\\boldsymbol{F}(\\theta)\\Delta_\\theta\\\\ \u0026amp;=\\frac{1}{2}\\Delta_\\theta^\\top\\boldsymbol{F}(\\theta)\\Delta_\\theta\\\\ \\end{align} $$\n其中二阶导到FIM的转化直接利用了性质3；一阶项为0是因为：\n$$ \\begin{align} \\Delta_\\theta^\\top\\left(\\nabla_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)\\right)|_{\\theta'=\\theta} \u0026amp;= \\Delta_\\theta^\\top\\left.\\nabla_{\\theta'}\\mathbb{E}_{p(x|\\theta)}\\left[\\log{\\frac{p(x|\\theta)}{p(x|\\theta')}}\\right]\\right|_{\\theta'=\\theta} \\\\ \u0026amp;= \\Delta_\\theta^\\top\\left.\\mathbb{E}_{p(x|\\theta)}\\left[\\nabla_{\\theta'}{\\log\\frac{p(x|\\theta)}{p(x|\\theta')}}\\right]\\right|_{\\theta'=\\theta}\\\\ \u0026amp;= \\Delta_\\theta^\\top\\left.\\mathbb{E}_{p(x|\\theta)}\\left[\\nabla_{\\theta'}{-\\log p(x|\\theta')}\\right]\\right|_{\\theta'=\\theta}\\\\ \u0026amp;= -\\Delta_\\theta^\\top\\left.\\mathbb{E}_{p(x|\\theta)}\\left[\\nabla_{\\theta'}{\\log p(x|\\theta')}\\right]\\right|_{\\theta'=\\theta} \\\\ \u0026amp;= 0\\quad\\triangleright\\text{性质1} \\end{align} $$\nFIM作为黎曼度量 在建立了FIM可用于KL散度近似的结论之后，我们将其放到一个新的框架里来审视——这种近似关系实际上定义了一种黎曼度量。\n在黎曼几何中，度量张量（metric tensor）提供了一种将坐标值转化为距离（或内积）的工具。假设在流形上的给定点$p$的切空间$T_pM$上定义了一组基向量$\\{e_1,\\cdots,e_n\\}$，黎曼度量可以定义为基向量两两之间的内积： $$ g_{ij} = \\langle \\boldsymbol{e}_i, \\boldsymbol{e}_j\\rangle $$\n对于任意切向量$\\boldsymbol{u},\\boldsymbol{v}\\in T_pM$，可以利用这个黎曼度量来定义切向量的内积：\n$$ \\begin{align} \\langle \\boldsymbol{u}, \\boldsymbol{v}\\rangle _p \u0026amp;= \\langle \\sum_i u_i\\boldsymbol{e}_i, \\sum_j v_j\\boldsymbol{e}_j\\rangle _p\\\\ \u0026amp;=\\sum_{ij}u_i v_j \\langle \\boldsymbol{e}_i, \\boldsymbol{e}_j\\rangle\\\\ \u0026amp;=\\sum_{ij}u_i v_j g_{ij}\\\\ \u0026amp;= \\boldsymbol{u}^\\top \\boldsymbol{G}\\boldsymbol{v} \\end{align} $$\n其中$(\\boldsymbol{G})_{ij}=g_{ij}$，即将度量组织成一个矩阵的形式。基于这个内积可以衍生出$p$点附近的微小距离（线元）的计算，考虑$p$点上的一个微小位移$\\boldsymbol{\\delta}$，则对应的线元为： $$ |\\boldsymbol{\\delta}|^2 = \\boldsymbol{\\delta}^\\top\\boldsymbol{G}\\boldsymbol{\\delta} $$\n作为一个特例，考虑常见的欧氏空间，由于基向量都是规范正交的，任意两个不同基向量的内积是0，且基向量与自身的内积为1，因此对应的度量张量是$\\boldsymbol{G}=\\boldsymbol{I}$，这一点对于任意点都成立。因此欧氏空间对应的内积总是$\\boldsymbol{u}^\\top \\boldsymbol{v}$，线元总是$|\\boldsymbol{\\delta}|^2 = \\boldsymbol{\\delta}^\\top\\boldsymbol{\\delta}$。\n回顾上一节推导得到的性质4（$D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta+\\Delta_\\theta)\\right)\\approx \\frac{1}{2}\\Delta_\\theta^\\top\\boldsymbol{F}(\\theta)\\Delta_\\theta$），通过对比上述的线元的计算，可以发现二者的形式是一致的。\n现在，如果我们将不同参数$\\theta$实例化的分布族$\\mathcal{F}=\\{p_\\theta:\\theta\\in \\Theta\\}$拓展为一个流形$\\mathcal{M}$，使得流形$\\mathcal{M}$上的点与分布族$\\mathcal{F}$中的分布构成双射（一一对应关系）。这样，分布中的参数$\\theta$可以看做流形上的坐标。如果我们将参数的距离定义为参数引导的概率分布的KL散度1，则由性质4的近似，$\\boldsymbol{F}(\\theta)$可以看做点$\\theta$的黎曼度量张量。\n在下篇文章中，我们会通过泛化标准梯度下降的距离约束条件为黎曼距离，从而得到自然梯度方法的更新公式，并给出这种新的约束下对应的一些性质。\n这里的距离是广义而言的，因为KL散度不满足距离的约定。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://nil9.net/posts/fisher-info-matrix/","summary":"\u003cp\u003e在一般的梯度下降中，我们认为目标函数梯度的负方向可以最小化一步更新后的目标函数值，这里隐含地假设了参数空间是欧氏空间，且参数构成了一组正交归一的坐标系统。在很多情况下，这一假设是不成立的，作为结果，优化过程的收敛效率可能受到影响。\u003c/p\u003e\n\u003cp\u003e作为解决这一问题的一种思路，自然梯度使用Fisher信息矩阵（的逆）作为梯度的pre-conditioner来矫正梯度的方向。本文将分为两篇，在第一篇中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在第二篇中，我们介绍自然梯度中如何引入FIM来修正梯度方向，以及其带来的一些性质。\u003c/p\u003e\n\u003ch1 id=\"score-function与fim\"\u003eScore function与FIM\u003c/h1\u003e\n\u003cp\u003e假设我们有一个由\u003ccode\u003e$\\theta$\u003c/code\u003e参数化的概率模型，模型分布为\u003ccode\u003e$p(x|\\theta)$\u003c/code\u003e，记对数似然函数为\u003ccode\u003e$\\ell(\\theta|x):=\\log p(x|\\theta)$\u003c/code\u003e。与对数似然函数相关的有两个定义，score function和fisher information。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e定义1（score function）\u003c/strong\u003e：score function \u003ccode\u003e$s(\\theta|x)$\u003c/code\u003e被定义为对数似然函数关于参数\u003ccode\u003e$\\theta$\u003c/code\u003e的梯度\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ s(\\theta|x)=\\nabla_\\theta \\ell(\\theta|x) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e一些文章会提到score function是用来为参数的好坏打分（score），这是不严谨的。score function中的「score」其实不是为参数打分，而是在Fisher研究的遗传统计问题中给基因异常家庭的「打分」(参见：\u003ca href=\"https://stats.stackexchange.com/questions/326091/interpretation-of-score\"\u003eInterpretation of \u0026ldquo;score\u0026rdquo;\u003c/a\u003e)。因此，score function只是约定俗成的一种名称，其实质就是似然函数的梯度，描述的是似然函数对于参数变化的敏感程度。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e性质1\u003c/strong\u003e：Score function期望为0（\u003ccode\u003e$\\mathbb{E}_{p(x|\\theta)}[s(\\theta|x)]=\\boldsymbol{0}$\u003c/code\u003e）.\u003c/p\u003e\n\u003cstyle type=\"text/css\"\u003e\n     \n    .notice {\n        --title-color: #fff;\n        --title-background-color: #6be;\n        --content-color: #444;\n        --content-background-color: #e7f2fa;\n    }\n\n    .notice.proof {\n        --title-background-color: rgb(130, 130, 130);\n        --content-background-color: #f7f7f7;\n    }\n\n    .notice.info {\n        --title-background-color: #fb7;\n        --content-background-color: #fec;\n    }\n\n    .notice.tip {\n        --title-background-color: #5a5;\n        --content-background-color: #efe;\n    }\n\n    .notice.warning {\n        --title-background-color: #c33;\n        --content-background-color: #fee;\n    }\n\n     \n\n    body.dark .notice {\n        --title-color: #fff;\n        --title-background-color: #069;\n        --content-color: #ddd;\n        --content-background-color: #023;\n    }\n\n    body.dark .notice.proof {\n        --title-background-color: rgb(129, 129, 129);\n        --content-background-color: rgb(41, 41, 41);\n    }\n\n    body.dark .notice.info {\n        --title-background-color: #a50;\n        --content-background-color: #420;\n    }\n\n    body.dark .notice.tip {\n        --title-background-color: #363;\n        --content-background-color: #121;\n    }\n\n    body.dark .notice.warning {\n        --title-background-color: #800;\n        --content-background-color: #400;\n    }\n\n     \n    .notice {\n        padding: 18px;\n        line-height: 24px;\n        margin-bottom: 24px;\n        border-radius: 4px;\n        color: var(--content-color);\n        background: var(--content-background-color);\n    }\n\n    .notice p:last-child {\n        margin-bottom: 0\n    }\n\n     \n    .notice-title {\n        margin: -18px -18px 12px;\n        padding: 4px 18px;\n        border-radius: 4px 4px 0 0;\n        font-weight: 700;\n        color: var(--title-color);\n        background: var(--title-background-color);\n    }\n\n     \n    .icon-notice {\n        display: inline-flex;\n        align-self: center;\n        margin-right: 8px;\n    }\n\n    .icon-notice img,\n    .icon-notice svg {\n        height: 1em;\n        width: 1em;\n        fill: currentColor;\n    }\n\n    .icon-notice img,\n    .icon-notice.baseline svg {\n        top: .125em;\n        position: relative;\n    }\n\u003c/style\u003e\u003cdiv class=\"notice proof\" \u003e\n    \u003cp class=\"notice-title\"\u003e\n        \u003cspan class=\"icon-notice baseline\"\u003e\n            \n        \u003c/span\u003eproof\u003c/p\u003e","title":"自然梯度（一）：Fisher信息矩阵作为黎曼度量"},{"content":"Tied embeddings，即将语言模型中的输入Embeddings权重与输出分类器的权重两组参数共享的操作，一度是语言建模和机器翻译任务的标准配置。在语言模型大规模化之后，这种设计在开源模型中愈发少见了。前几天看到@苏剑林 之前的一篇博客语言模型输出端共享Embedding的重新探索，为tied embeddings的消失提供了一种视角，但也还有值得商榷的地方，本文想从这篇文章出发做一点探讨。\n初始Loss的视角 这里先简要概括一下苏老师文章中的阐述框架1。在使用Transformer做语言建模的时候，可能会使用类似DeepNorm等初始化手段，从而使每一个Transformer Block接近于一个恒等映射2，同时由于词元表征是0均值的，因此LayerNorm可以看做与RMSNorm等价3。所以，假设每个残差分支都初始化为0，假设输入中某个位置的初始embedding是$\\boldsymbol{w}_i$（对应词表中的第$i$个词，维度是$d$），那么最终得到的表征满足\n$$ \\frac{\\boldsymbol{w}_i}{\\Vert\\boldsymbol{w}_i\\Vert \\big/\\sqrt{d}} \\approx \\frac{\\boldsymbol{w}_i}{\\sigma} $$\n假设在该位置的真实标签是词元$j$，则损失函数可以由如下逼近\n$$ \\begin{align}\\mathcal{L}\\triangleq -\\log p(j|i) \u0026amp;= \\log \\sum\\limits_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma} - \\boldsymbol{w}_i\\cdot \\boldsymbol{w}_j \\big/ \\sigma \\\\ \u0026amp;\\approx \\log \\sum_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma}\\\\ \u0026amp;=\\log \\left(e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_i / \\sigma} + \\sum\\limits_{k|k\\neq i} e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma}\\right)\\\\ \u0026amp;\\approx\\log \\left({\\color[rgb]{0, 0.5, 0.8}e^{d \\sigma}} + (n-1)\\right) \\end{align} $$\n其中$|n|$是词表大小。在常见的模型维度下，这里的第一项${\\color[rgb]{0, 0.5, 0.8}e^{d \\sigma}}$是比较大的。我们可以代入几个维度值看下第一项的大小，这里我们假设词表大小是32k，并考虑两种$\\sigma$取法，一种是比较常见的初始化超参数$\\sigma=0.02$，一种是取$\\sigma=1/\\sqrt{d}$。可以看到无论是哪种初始化方法，对应的${\\color[rgb]{0, 0.5, 0.8}e^{d \\sigma}}$都已经远远超过词表大小，响应地初始损失值也处于比较高的水平（按均匀分布的交叉熵是$\\log(n)\\approx 10.37$）。\n不同设定下的「初始损失值」\n以上是苏文中给出的关于语言建模中不再共享embedding的一个视角——tied embeddings会使语言模型的初始损失值很大。\n但这个问题实际上可以用一个rescale来解决，我们可以简单地将输出端乘以$1/\\sqrt{d}$，则损失函数可以做如下近似 $$ \\begin{align}\\mathcal{L} \u0026amp;= \\log \\sum\\limits_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / (\\sigma\\sqrt{d})} - \\boldsymbol{w}_i\\cdot \\boldsymbol{w}_j \\big/(\\sigma\\sqrt{d}) \\\\ \u0026amp;\\approx\\log \\left({\\color[rgb]{0, 0.5, 0.8}e^{\\sigma\\sqrt{d}}} + (n-1)\\right) \\end{align} $$\n这时候，对于常见的$\\sigma=1/\\sqrt{d}$或者$\\sigma=0.02$，${\\color[rgb]{0, 0.5, 0.8}e^{\\sigma\\sqrt{d}}}$这一项相对于词表大小都可以忽略不计了。\n事实上，早期共享embeddings的预训练模型T5的实现4中就使用了这个技巧：\nif self.shared_embedding_and_softmax_weights: logits = mtf.einsum( [x * (self.model_dim.size ** -0.5), embedding_weights], reduced_dims=[self.model_dim]) 另外，在一些公开的预训练实现中，一般残差项的初始化不会使用特别小的值，例如在OLMo-2中，就是对残差分支中的各个参数矩阵就是直接用了标准差为0.02的truncated normal初始化。\n我们可以使用Llama的模型结构做一个简单的实验，我们使用常见的正态分布初始化，在固定层数为12的情况下，测试不同embedding维度下的初始loss值，结果如下表所示。\n初始loss 768 1024 2048 4096 $\\log(n)$ 10.37 - - - untied 10.52 10.56 10.78 11.19 tied 10.53 10.58 10.77 11.24 untied+rescale 10.37 10.37 10.37 10.37 tied+rescale 10.37 10.37 10.37 10.37 可以看到，\n无论是否应用tied embeddings，初始loss都有略高于$\\log(n)$的情况； 在输出端应用rescale技巧，可以将初始loss控制在$\\log(n)$左右。 寻根溯源 笔者认为，初始Loss虽然是一个非常好的视角，但是不能解释当前tied embeddings的式微。讨论tied embeddings的应用，还得稍微追溯学术史，先看看他们是为何被提出的。\n在语言建模中引入tied embeddings技巧可以追溯到LSTM-LM时代的两篇工作：Inan 2016.和Press and Wolf 2017.。其中，Inan 2016.通过类似KD的框架构造出一种soft label\n$$ \\begin{aligned} \\boldsymbol{u}_t \u0026amp;= \\boldsymbol{L}\\boldsymbol{y}^{*}_t \\\\ \\tilde{\\boldsymbol{y}}_t \u0026amp;= \\text{softmax}(\\frac{\\boldsymbol{L}^\\top \\boldsymbol{u}_t}{\\tau}) \\end{aligned} $$\n这里$\\boldsymbol{L}, \\boldsymbol{y}^{*}_t$分别表示embedding权重和第$t$个位置的目标词元。作者论证了在一定的假设下，tied embeddings设定的语言模型相当于在隐式地学习这个soft label（而不是一般的one-hot目标）。\nPress and Wolf 2017.则是通过一系列实验论证了如下几个结论5：\nRNNLM使用tied embeddings时，embeddings的演进方式更接近与untied版本中输出端的embeddings； 使用tied embeddings可以有效降低语言建模中的PPL（PTB数据集），无论是否使用dropout均成立； 在不使用dropout的情况下，在输出embedding之前添加一个额外的投影$P$，并对$P$添加正则化loss，可以进一步降低PPL指标。 这篇文章还提出在机器翻译模型中，对于en-fr这样比较相似的语言，可以在两个语言的语料合集上联合训练一个tokenizer，共享encoder与decoder的embeddings（即encoder的输入、decoder的输入与输出共享一个参数矩阵），后来的Transformer(Vaswani 2017.)也沿用了这一做法。笔者认为这就是初期的很多预训练模型都不约而同地沿用tied embeddings的设定的原因。\n但是如今回顾这两篇文章的时候，我们注意到几点：\n当时的语言模型一般基于浅层的RNN，输入与输出的embeddings参数在模型中占比很大； 当时的实验基于PTB和WikiText数据集，相对于如今的预训练语料规模，可谓是非常小了，尤其是前者。 笔者认为，tied embeddings的有效性与数据和模型规模离不开关系。当数据与模型的规模比较有限时，tied embeddings可以作为一种很好的正则化手段（显著降低参数数量），从Press and Wolf 2017.的实验来看，在PTB这样的小数据集上，tied embeddings的语言模型在训练集上的PPL并不占优势，这表明它的作用可能有部分来自于过拟合风险的降低。\n现在的LLM模做规模化主要是通过加大隐藏层维度和模型层数，non-embedding部分的参数量按$\\mathcal{O}(Ld^2)$的级别增长，而embeddings的参数量只随着隐藏层维度线性增长，因此现有的LLM的embeddings所占参数比例已经非常小了，通过tied embeddings减少参数量的作用非常有限。另外，现在的预训练语料的词元规模也通常在万亿这个量级，与PTB这种训练集不到一百万词的数据集已经不能同日而语了。\n训练的不稳定、工程的限制 前面我们提到，tied embeddings是源于数据与模型规模都较小的LSTMLM时期的一种正则化方法，逐渐成为一项标准设定，在预训练的早期也被沿用了下来。如今在数据与模型规模化的趋势下，正则化的强问题意识已经逐渐不成立了，这种强正则甚至可能成为训练的负担。例如，在OLMo的talk中作者提到，tied embeddings在7B的模型中会造成训练的不稳定。\n除此之外，在语言模型规模化以后，模型的训练越来越依赖于各种跨节点并行计算方法。而使用tied embeddings实际上对并行方法的选择也有一定的限制。例如，使用流水线并行（Pipeline Parallelism）要求将模型纵向拆分部署在多个节点上，那么此时如果将输入与输出层看做两个不同的层，部署在不同的节点上，则首先这两部分参数共享不会节约任何的存储，还需要付出额外的通信成本来同步两个层的梯度。不过笔者觉得这个原因是次要的，如果收益是正向的，那么额外的同步步骤也是值得的。\n结语 本文从语言模型输出端共享Embedding的重新探索中的初始loss视角出发，拓展讨论了在语言建模规模化之后，tied embeddings操作不再作为标准设定的原因：模型与数据规模的变化使得正则化的问题意识不再，且从一些公开的实验来看，tied embeddings可能引发训练的不稳定6，此外tied embeddings也对并行方法的选型有一定限制。\n拓展阅读 Inan 2016. Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling Press and Wolf 2017. Using the Output Embedding to Improve Language Models 语言模型输出端共享Embedding的重新探索 详细内容请查看原文。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n除了DeepNorm，ReZero等优化也有类似的思想。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n在常见实现中，LayerNorm在初始化时，$\\gamma,\\beta$参数分别被初始化为1和0.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n这里略过关于embedding similarity测验的结论。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n在深度学习领域，经验结论很重要，尤其是对于LLM这样试错成本较高的应用中。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://nil9.net/posts/tied-embeddings-in-lm/","summary":"\u003cp\u003eTied embeddings，即将语言模型中的输入Embeddings权重与输出分类器的权重两组参数共享的操作，一度是语言建模和机器翻译任务的标准配置。在语言模型大规模化之后，这种设计在开源模型中愈发少见了。前几天看到\u003ca href=\"https://www.zhihu.com/people/su-jian-lin-22\"\u003e@苏剑林\u003c/a\u003e 之前的一篇博客\u003ca href=\"https://kexue.fm/archives/9698\"\u003e语言模型输出端共享Embedding的重新探索\u003c/a\u003e，为tied embeddings的消失提供了一种视角，但也还有值得商榷的地方，本文想从这篇文章出发做一点探讨。\u003c/p\u003e\n\u003ch1 id=\"初始loss的视角\"\u003e初始Loss的视角\u003c/h1\u003e\n\u003cp\u003e这里先简要概括一下苏老师文章中的阐述框架\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e。在使用Transformer做语言建模的时候，可能会使用类似DeepNorm等初始化手段，从而使每一个Transformer Block接近于一个恒等映射\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e，同时由于词元表征是0均值的，因此LayerNorm可以看做与RMSNorm等价\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e。所以，假设每个残差分支都初始化为0，假设输入中某个位置的初始embedding是\u003ccode\u003e$\\boldsymbol{w}_i$\u003c/code\u003e（对应词表中的第\u003ccode\u003e$i$\u003c/code\u003e个词，维度是\u003ccode\u003e$d$\u003c/code\u003e），那么最终得到的表征满足\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\frac{\\boldsymbol{w}_i}{\\Vert\\boldsymbol{w}_i\\Vert \\big/\\sqrt{d}} \\approx \\frac{\\boldsymbol{w}_i}{\\sigma} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e假设在该位置的真实标签是词元\u003ccode\u003e$j$\u003c/code\u003e，则损失函数可以由如下逼近\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{align}\\mathcal{L}\\triangleq -\\log p(j|i) \u0026amp;= \\log \\sum\\limits_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma} - \\boldsymbol{w}_i\\cdot \\boldsymbol{w}_j \\big/ \\sigma \\\\ \u0026amp;\\approx \\log \\sum_k e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma}\\\\ \u0026amp;=\\log \\left(e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_i / \\sigma} + \\sum\\limits_{k|k\\neq i} e^{\\boldsymbol{w}_i\\cdot \\boldsymbol{w}_k / \\sigma}\\right)\\\\ \u0026amp;\\approx\\log \\left({\\color[rgb]{0, 0.5, 0.8}e^{d \\sigma}} + (n-1)\\right) \\end{align} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中\u003ccode\u003e$|n|$\u003c/code\u003e是词表大小。在常见的模型维度下，这里的第一项\u003ccode\u003e${\\color[rgb]{0, 0.5, 0.8}e^{d \\sigma}}$\u003c/code\u003e是比较大的。我们可以代入几个维度值看下第一项的大小，这里我们假设词表大小是32k，并考虑两种\u003ccode\u003e$\\sigma$\u003c/code\u003e取法，一种是比较常见的初始化超参数\u003ccode\u003e$\\sigma=0.02$\u003c/code\u003e，一种是取\u003ccode\u003e$\\sigma=1/\\sqrt{d}$\u003c/code\u003e。可以看到无论是哪种初始化方法，对应的\u003ccode\u003e${\\color[rgb]{0, 0.5, 0.8}e^{d \\sigma}}$\u003c/code\u003e都已经远远超过词表大小，响应地初始损失值也处于比较高的水平（按均匀分布的交叉熵是\u003ccode\u003e$\\log(n)\\approx 10.37$\u003c/code\u003e）。\u003c/p\u003e\n\u003cp\u003e\u003cfigure\u003e\n  \u003cimg loading=\"lazy\" src=\"/images/tied-embeddings-in-lm/init_loss.png\" title=\"不同设定下的「初始损失值」\"\u003e\u003cfigcaption class=\"image-caption\"\u003e不同设定下的「初始损失值」\u003c/figcaption\u003e\u003c/figure\u003e\u003c/p\u003e\n\u003cp\u003e以上是苏文中给出的关于语言建模中不再共享embedding的一个视角——tied embeddings会使语言模型的初始损失值很大。\u003c/p\u003e","title":"关于语言建模中的Tied Embeddings的一点探讨"}]