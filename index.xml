<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>nil9.net</title>
    <link>https://nil9.net/</link>
    <description>Recent content on nil9.net</description>
    <image>
      <title>nil9.net</title>
      <url>https://nil9.net/images/android-chrome-512x512.png</url>
      <link>https://nil9.net/images/android-chrome-512x512.png</link>
    </image>
    <generator>Hugo -- 0.144.2</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 26 Feb 2025 00:00:41 +0000</lastBuildDate>
    <atom:link href="https://nil9.net/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>为什么LLM一般使用较大的权重衰减系数？</title>
      <link>https://nil9.net/posts/wd-model-precision/</link>
      <pubDate>Wed, 26 Feb 2025 00:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/wd-model-precision/</guid>
      <description>&lt;p&gt;最近在阅读&lt;a href=&#34;https://github.com/MoonshotAI/Moonlight/blob/master/Moonlight.pdf&#34;&gt;Muon is Scalable for LLM Training&lt;/a&gt;这篇文章的时候注意到他们使用无权重衰减（weight decay）版本的Muon优化LLM的时候，优化器的收敛优势会随着训练过程逐渐消失，又看到&lt;a href=&#34;https://www.zhihu.com/people/xiao-ming-tong-xue-86-81&#34;&gt;@小明同学&lt;/a&gt;提到的一个细节，很多开源的LLM在技术报告中都提到了使用0.1作为权重衰减的系数，觉得是个比较有意思的发现。结合Kimi的文章中关于bf16的简单陈述，笔者在本文中稍微展开讲下，权重衰减对于LLM的低精度训练中有什么作用。&lt;/p&gt;
&lt;p&gt;首先把结论放在前面：除了一般认知中的正则化作用，权重衰减也可能降低精度损失的风险——对于计算机的浮点数而言，&lt;strong&gt;绝对值越大，精度越低&lt;/strong&gt;。对于低精度/混合精度训练而言，使用权重衰减可以控制参数的绝对值范围，从而保证模型参数不落入低精度的数值区间。&lt;/p&gt;
&lt;h1 id=&#34;浮点数的存储与精度&#34;&gt;浮点数的存储与精度&lt;/h1&gt;
&lt;p&gt;上述结论主要与浮点数在计算机内的存储形式有关。学过计算机的一些基本课程的读者可能有印象，浮点数的存储是二进制的形式，分为符号位、指数位和尾数位三段。深度学习中常见的浮点数协议（fp32、fp16、bf16、tf32）的区别在于指数位和尾数位的比特数量不同。由于浮点数是一个「指数」的形式，&lt;strong&gt;因此它在实数空间的分布是不均匀的&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;这里我们考虑规范数的情形（指数位非全0），做一点分析。假设符号位、指数位、尾数位（mantissa）的二进制编码分别是&lt;code&gt;$S$&lt;/code&gt;、&lt;code&gt;$E$&lt;/code&gt;、&lt;code&gt;$M$&lt;/code&gt;，那么对应的浮点数为：
&lt;code&gt;$$ \text{value}=(-1)^S\times 1.M\times 2^{E-\text{bias}} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;在单精度fp32标准中，&lt;code&gt;$\text{bias}$&lt;/code&gt;取&lt;code&gt;${01111111}_{2}=127_{10}$&lt;/code&gt; .&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  &lt;img alt=&#34;fp32浮点数的一个例子&#34; loading=&#34;lazy&#34; src=&#34;https://nil9.net/images/wd-model-precision/ieee-fp32-example.png&#34; title=&#34;fp32浮点数的一个例子&#34;&gt;&lt;figcaption class=&#34;image-caption&#34;&gt;fp32浮点数的一个例子&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;例如在图中的例子中，&lt;code&gt;$S=0$&lt;/code&gt;，&lt;code&gt;$E=\underbrace{00\cdots0}_{7\ 0&#39;s}1$&lt;/code&gt;，&lt;code&gt;$M=\underbrace{00\cdots0}_{22\ 0&#39;s}1$&lt;/code&gt;，相应的值为
&lt;code&gt;$$ \begin{align} &amp;amp;{-1}^0\times 1.\underbrace{00\cdots0}_{22\ 0&#39;s}1_2\times 2^{00000001_2-{01111111}_{2}}\\ &amp;amp;\approx [1.175494490952134\times 10^{-38}]_{10} \end{align} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;现在我们来考虑不同的数值范围内的浮点数精度。对于区间范围&lt;code&gt;$[2^{x}, 2^{x+1}],\forall -126\le x\le127$&lt;/code&gt;（这里的x已经是经过-bias之后得到的最终指数），我们希望在给定任意浮点数&lt;code&gt;$y\in[2^{x}, 2^{x+1}]$&lt;/code&gt;的基础上增加一个最小量&lt;code&gt;$\varepsilon$&lt;/code&gt;（即区间内两个浮点数的最小间隔），这个增加的过程是通过操纵二进制编码实现的，那么最小间隔只能是通过在&lt;code&gt;$y$&lt;/code&gt;的尾数部分加上&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ 0.\underbrace{00\cdots0}_{22\ 0&#39;s}1 $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;来实现，对应的最小间隔是
&lt;code&gt;$$ \begin{align} \varepsilon &amp;amp;= 0.\underbrace{00\cdots0}_{22\text{ 0&#39;s}}1_2\times 2^{x}\\ &amp;amp;=2^{-23}\times2^{x}=2^{x-23}\\ \end{align} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;如果你将上面的公式带入不同的指数位，可以验证与&lt;a href=&#34;https://en.wikipedia.org/wiki/IEEE_754-1985&#34;&gt;Wikipedia&lt;/a&gt;中给出的这张表的Gap是吻合的：&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  &lt;img alt=&#34;不同指数位下的最小精度&#34; loading=&#34;lazy&#34; src=&#34;https://nil9.net/images/wd-model-precision/ieee-fp32-prec.png&#34; title=&#34;不同指数位下的最小精度，来源：Wikipedia&#34;&gt;&lt;figcaption class=&#34;image-caption&#34;&gt;不同指数位下的最小精度，来源：Wikipedia&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;这个计算可以拓展到其他的精度格式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于半精度格式fp16而言，其尾数位有10位，对应的最小间隔是&lt;code&gt;$2^{x-10}$&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;对于半精度格式bf16而言，其尾数位有7位，对应的最小间隔是&lt;code&gt;$2^{x-7}$&lt;/code&gt;；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从这里也可以看到，bf16相比fp16虽然拓宽了表示范围，但是减少了精度（同样数值范围内的最小间隔更宽了）。&lt;/p&gt;
&lt;p&gt;从这里我们得到了一个结论：计算机存储的浮点数之间的最小间隔随着浮点数绝对值数值增加，指数级地增大，换言之，&lt;strong&gt;浮点数（绝对值）数值越大，精度越低&lt;/strong&gt;。并且这个问题对于fp16或bf16格式的浮点数，问题要更加显著。&lt;/p&gt;
&lt;p&gt;这个结论的另一个引申的问题是&lt;strong&gt;舍入误差&lt;/strong&gt;，假如一个较大的浮点数和一个较小的浮点数相加，由于浮点数的加法（减法过程相当于取补码后相加，结论是类似的）过程需要先将两个数的指数位对齐，因此绝对值较小的数字的尾数的最后几位数字可能会在加法中丢失。这里我们举一个极端的例子来说明。&lt;/p&gt;
&lt;p&gt;假设浮点数存储为fp32格式（8位指数、23位尾数）。
&lt;code&gt;$$ \begin{align} x=(-1)^0\times 1.0_2\times 2^{-1}\\ y=(-1)^0\times 1.0_2\times 2^{-25}\\ \end{align} $$&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tensor Product Attention (TPA) 导读</title>
      <link>https://nil9.net/posts/tensor-product-attention/</link>
      <pubDate>Thu, 13 Feb 2025 00:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/tensor-product-attention/</guid>
      <description>&lt;p&gt;最近Deepseek比较出圈，连带着里面用到的MLA也被讨论得更多了。MLA无疑是一个非常出色的attention改进，但是由于它的KV cache设计，不能很好地兼容&lt;a href=&#34;https://kexue.fm/archives/8265&#34;&gt;RoPE&lt;/a&gt;，因此作者们使用了decoupled RoPE这样的「补丁」来引入位置关系，这无疑也增加了实现的复杂度。&lt;/p&gt;
&lt;p&gt;最近，修改注意力KV Cache这一线工作又增添了&lt;a href=&#34;https://arxiv.org/pdf/2501.06425&#34;&gt;TPA&lt;/a&gt;这个新成员，笔者觉得这篇文章比较有趣，因此希望写一篇简短的导读。之所以叫「导读」，是因为本文不打算太深入文章的formulation和实验，而是从笔者自己的视角出发介绍文章的一些重点贡献。。&lt;/p&gt;
&lt;h1 id=&#34;mha的拆解&#34;&gt;MHA的拆解&lt;/h1&gt;
&lt;p&gt;最标准的多头注意力（Vaswani的版本），大致可以拆解成3个步骤（这里默认讨论自注意力）&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;：
&lt;code&gt;$$ \begin{aligned} \text{step 1 }&amp;amp; \begin{cases} \boldsymbol{q}_i^{(h)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(h)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(h)}\in\mathbb{R}^{d\times d_k} \\  \boldsymbol{k}_i^{(h)} = \boldsymbol{x}_i\boldsymbol{W}_k^{(h)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{(h)}\in\mathbb{R}^{d\times d_k}\\  \boldsymbol{v}_i^{(h)} = \boldsymbol{x}_i\boldsymbol{W}_v^{(h)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(h)}\in\mathbb{R}^{d\times d_v}  \end{cases} \\ \text{step 2 }&amp;amp; \begin{cases} \boldsymbol{o}_t^{(h)} = \text{Attention}\left(\boldsymbol{q}_t^{(h)}, \boldsymbol{k}_{\leq t}^{(h)}, \boldsymbol{v}_{\leq t}^{(h)}\right)\triangleq\frac{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(h)} \boldsymbol{k}_i^{(h)}{}^{\top}\right)\boldsymbol{v}_i^{(h)}}{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(h)} \boldsymbol{k}_i^{(h)}{}^{\top}\right)} \\  \end{cases} \\ \text{step 3 }&amp;amp; \begin{cases} \boldsymbol{o}_t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(H)}\right]  \end{cases} \\ \end{aligned}\\ $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这里&lt;code&gt;$\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_l$&lt;/code&gt;是输入向量，上标&lt;code&gt;$h\in \{1,\ldots,H\}$&lt;/code&gt;表示注意力头，&lt;code&gt;$d,d_k,d_v$&lt;/code&gt;分别表示输入维度、key维度、value维度。在第一步中，我们将每个token的表征独立地线性投射到不同的自空间上，第二步是标准的点积注意力，第三步是拼接（后续再做一步线性变换）。&lt;/p&gt;
&lt;p&gt;我们重点来审视一下第一个步骤，这里由于每个token的表征是独立操作的（类似FFN），因此我们可以将每个token的表征看做一个样本点，这里做的事情就是将一个&lt;code&gt;$d$&lt;/code&gt;维度的向量，转化成3个矩阵&lt;code&gt;$\tilde{\boldsymbol{Q}}\in\mathbb{R}^{H\times d_k},\tilde{\boldsymbol{K}}\in\mathbb{R}^{H\times d_k},\tilde{\boldsymbol{V}}\in\mathbb{R}^{H\times d_v}$&lt;/code&gt;，每一个头对应这个矩阵中的一行。接着我们逐行计算每行对应的三组向量的点积注意力，就构成了标准的多头注意力。在标准实现中，这个变换是通过参数矩阵的线性变换+ reshape实现的。&lt;/p&gt;</description>
    </item>
    <item>
      <title>自然梯度（二）：黎曼距离下的最速下降</title>
      <link>https://nil9.net/posts/natural-gradient-descent/</link>
      <pubDate>Thu, 06 Feb 2025 12:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/natural-gradient-descent/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://nil9.net/posts/fisher-info-matrix/&#34;&gt;上篇文章&lt;/a&gt;中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在本篇文章中，我们利用上篇得到的结论，推导自然梯度中为何引入FIM来修正梯度方向，并介绍自然梯度的一些性质。&lt;/p&gt;
&lt;h1 id=&#34;梯度下降欧氏距离下的最速下降&#34;&gt;梯度下降：欧氏距离下的最速下降&lt;/h1&gt;
&lt;p&gt;考虑一个最优化任务（&lt;code&gt;$f:\Theta\to\mathbb{R}$&lt;/code&gt;）：
&lt;code&gt;$$ \underset{\theta}{\operatorname{min}} f(\theta) $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;最常见的一阶优化方法是梯度下降/steepest descent：
&lt;code&gt;$$ \theta^+=\theta-\eta\nabla_\theta f $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;code&gt;$\eta$&lt;/code&gt;是学习率。这里的「steepest」指的是在约束欧氏距离定义下的步长在极小范围内时，选取梯度的负方向能最大化一步之内目标函数下降的程度。
&lt;code&gt;$$ \lim_{\epsilon\to 0}\frac{1}{\epsilon}\left(\underset{\delta:\|\delta\|\le \epsilon}{\operatorname{argmin}}  f(\theta+\delta)\right)=-\frac{\nabla_\theta f}{\|\nabla_\theta f\|}\tag{1} $$&lt;/code&gt;&lt;/p&gt;
&lt;style type=&#34;text/css&#34;&gt;
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.proof {
        --title-background-color: rgb(130, 130, 130);
        --content-background-color: #f7f7f7;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.proof {
        --title-background-color: rgb(129, 129, 129);
        --content-background-color: rgb(41, 41, 41);
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
&lt;/style&gt;&lt;div class=&#34;notice proof&#34; &gt;
    &lt;p class=&#34;notice-title&#34;&gt;
        &lt;span class=&#34;icon-notice baseline&#34;&gt;
            
        &lt;/span&gt;proof&lt;/p&gt;</description>
    </item>
    <item>
      <title>自然梯度（一）：Fisher信息矩阵作为黎曼度量</title>
      <link>https://nil9.net/posts/fisher-info-matrix/</link>
      <pubDate>Wed, 05 Feb 2025 00:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/fisher-info-matrix/</guid>
      <description>&lt;p&gt;在一般的梯度下降中，我们认为目标函数梯度的负方向可以最小化一步更新后的目标函数值，这里隐含地假设了参数空间是欧氏空间，且参数构成了一组正交归一的坐标系统。在很多情况下，这一假设是不成立的，作为结果，优化过程的收敛效率可能受到影响。&lt;/p&gt;
&lt;p&gt;作为解决这一问题的一种思路，自然梯度使用Fisher信息矩阵（的逆）作为梯度的pre-conditioner来矫正梯度的方向。本文将分为两篇，在第一篇中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在第二篇中，我们推导自然梯度中为何引入FIM来修正梯度方向，以及自然梯度的一些性质。&lt;/p&gt;
&lt;h1 id=&#34;score-function与fim&#34;&gt;Score function与FIM&lt;/h1&gt;
&lt;p&gt;假设我们有一个由&lt;code&gt;$\theta$&lt;/code&gt;参数化的概率模型，模型分布为&lt;code&gt;$p(x|\theta)$&lt;/code&gt;，记对数似然函数为&lt;code&gt;$\ell(\theta|x):=\log p(x|\theta)$&lt;/code&gt;。与对数似然函数相关的有两个定义，score function和fisher information。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定义1（score function）&lt;/strong&gt;：score function &lt;code&gt;$s(\theta|x)$&lt;/code&gt;被定义为对数似然函数关于参数&lt;code&gt;$\theta$&lt;/code&gt;的梯度&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ s(\theta|x)=\nabla_\theta \ell(\theta|x) $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;一些文章会提到score function是用来为参数的好坏打分（score），这是不严谨的。score function中的「score」其实不是为参数打分，而是在Fisher研究的遗传统计问题中给基因异常家庭的「打分」(参见：&lt;a href=&#34;https://stats.stackexchange.com/questions/326091/interpretation-of-score&#34;&gt;Interpretation of &amp;ldquo;score&amp;rdquo;&lt;/a&gt;)。因此，score function只是约定俗成的一种名称，其实质就是似然函数的梯度，描述的是似然函数对于参数变化的敏感程度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;性质1&lt;/strong&gt;：Score function期望为0
&lt;code&gt;$$ \mathbb{E}_{p(x|\theta)}[s(\theta|x)]=\boldsymbol{0} $$&lt;/code&gt;&lt;/p&gt;
&lt;style type=&#34;text/css&#34;&gt;
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.proof {
        --title-background-color: rgb(130, 130, 130);
        --content-background-color: #f7f7f7;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.proof {
        --title-background-color: rgb(129, 129, 129);
        --content-background-color: rgb(41, 41, 41);
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
&lt;/style&gt;&lt;div class=&#34;notice proof&#34; &gt;
    &lt;p class=&#34;notice-title&#34;&gt;
        &lt;span class=&#34;icon-notice baseline&#34;&gt;
            
        &lt;/span&gt;proof&lt;/p&gt;</description>
    </item>
    <item>
      <title>关于语言建模中的Tied Embeddings的一点探讨</title>
      <link>https://nil9.net/posts/tied-embeddings-in-lm/</link>
      <pubDate>Sat, 18 Jan 2025 15:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/tied-embeddings-in-lm/</guid>
      <description>&lt;p&gt;Tied embeddings，即将语言模型中的输入Embeddings权重与输出分类器的权重两组参数共享的操作，一度是语言建模和机器翻译任务的标准配置。在语言模型大规模化之后，这种设计在开源模型中愈发少见了。前几天看到&lt;a href=&#34;https://www.zhihu.com/people/su-jian-lin-22&#34;&gt;@苏剑林&lt;/a&gt; 之前的一篇博客&lt;a href=&#34;https://kexue.fm/archives/9698&#34;&gt;语言模型输出端共享Embedding的重新探索&lt;/a&gt;，为tied embeddings的消失提供了一种视角，但也还有值得商榷的地方，本文想从这篇文章出发做一点探讨。&lt;/p&gt;
&lt;h1 id=&#34;初始loss的视角&#34;&gt;初始Loss的视角&lt;/h1&gt;
&lt;p&gt;这里先简要概括一下苏老师文章中的阐述框架&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。在使用Transformer做语言建模的时候，可能会使用类似DeepNorm等初始化手段，从而使每一个Transformer Block接近于一个恒等映射&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;，同时由于词元表征是0均值的，因此LayerNorm可以看做与RMSNorm等价&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;。所以，假设每个残差分支都初始化为0，假设输入中某个位置的初始embedding是&lt;code&gt;$\boldsymbol{w}_i$&lt;/code&gt;（对应词表中的第&lt;code&gt;$i$&lt;/code&gt;个词，维度是&lt;code&gt;$d$&lt;/code&gt;），那么最终得到的表征满足&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ \frac{\boldsymbol{w}_i}{\Vert\boldsymbol{w}_i\Vert \big/\sqrt{d}} \approx \frac{\boldsymbol{w}_i}{\sigma} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;假设在该位置的真实标签是词元&lt;code&gt;$j$&lt;/code&gt;，则损失函数可以由如下逼近&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ \begin{align}\mathcal{L}\triangleq -\log p(j|i) &amp;amp;= \log \sum\limits_k e^{\boldsymbol{w}_i\cdot \boldsymbol{w}_k / \sigma} - \boldsymbol{w}_i\cdot \boldsymbol{w}_j \big/ \sigma \\ &amp;amp;\approx \log \sum_k e^{\boldsymbol{w}_i\cdot \boldsymbol{w}_k / \sigma}\\ &amp;amp;=\log \left(e^{\boldsymbol{w}_i\cdot \boldsymbol{w}_i / \sigma} + \sum\limits_{k|k\neq i} e^{\boldsymbol{w}_i\cdot \boldsymbol{w}_k / \sigma}\right)\\ &amp;amp;\approx\log \left({\color[rgb]{0, 0.5, 0.8}e^{d \sigma}} + (n-1)\right) \end{align} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;code&gt;$|n|$&lt;/code&gt;是词表大小。在常见的模型维度下，这里的第一项&lt;code&gt;${\color[rgb]{0, 0.5, 0.8}e^{d \sigma}}$&lt;/code&gt;是比较大的。我们可以代入几个维度值看下第一项的大小，这里我们假设词表大小是32k，并考虑两种&lt;code&gt;$\sigma$&lt;/code&gt;取法，一种是比较常见的初始化超参数&lt;code&gt;$\sigma=0.02$&lt;/code&gt;，一种是取&lt;code&gt;$\sigma=1/\sqrt{d}$&lt;/code&gt;。可以看到无论是哪种初始化方法，对应的&lt;code&gt;${\color[rgb]{0, 0.5, 0.8}e^{d \sigma}}$&lt;/code&gt;都已经远远超过词表大小，响应地初始损失值也处于比较高的水平（按均匀分布的交叉熵是&lt;code&gt;$\log(n)\approx 10.37$&lt;/code&gt;）。&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  &lt;img loading=&#34;lazy&#34; src=&#34;https://nil9.net/images/tied-embeddings-in-lm/init_loss.png&#34; title=&#34;不同设定下的「初始损失值」&#34;&gt;&lt;figcaption class=&#34;image-caption&#34;&gt;不同设定下的「初始损失值」&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;以上是苏文中给出的关于语言建模中不再共享embedding的一个视角——tied embeddings会使语言模型的初始损失值很大。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
