<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Optimization on nil9.net</title>
    <link>https://nil9.net/tags/optimization/</link>
    <description>Recent content in Optimization on nil9.net</description>
    <image>
      <title>nil9.net</title>
      <url>https://nil9.net/images/android-chrome-512x512.png</url>
      <link>https://nil9.net/images/android-chrome-512x512.png</link>
    </image>
    <generator>Hugo -- 0.145.0</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sun, 02 Mar 2025 00:00:41 +0000</lastBuildDate>
    <atom:link href="https://nil9.net/tags/optimization/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>从约束视角看深度学习优化若干新进展</title>
      <link>https://nil9.net/posts/gd-norm-const/</link>
      <pubDate>Sun, 02 Mar 2025 00:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/gd-norm-const/</guid>
      <description>&lt;p&gt;在深度学习中最常用的优化方法是梯度下降方法及其变体。在过去很长一段时间中，Adam优化器是NLP社区的默认选择，在ViT出现之后，CV方面的工作也逐渐开始使用Adam和Adam的变体（在ViT之前，一种常见的观点是Adam不适用于Vision任务）。&lt;/p&gt;
&lt;p&gt;最近Muon优化器在Kimi的新工作带动下又火了一把，相较于Adam优化器需要同时维护一阶、二阶矩估计量，Muon只需要维护一份梯度的动量估计，因此在大规模训练中有很大优势。最近笔者顺着Muon的reference看了Jeremy Bernstein在优化的一些文章，觉得很有意思，因此写这篇文章梳理一下这一系列工作的思路。本文的核心论点是：使用诱导范数来约束梯度更新，可以推导出最近的一些新出现的优化器，这也可能是未来深度学习优化的一个有潜力的探索方向。&lt;/p&gt;
&lt;h1 id=&#34;梯度下降recap&#34;&gt;梯度下降（Recap）&lt;/h1&gt;
&lt;p&gt;当前深度学习优化算法的基石是梯度下降。之前笔者写过一篇拙文（&lt;a href=&#34;https://nil9.net/posts/natural-gradient-descent/&#34;&gt;自然梯度（二）：黎曼距离下的最速下降&lt;/a&gt;）整理过梯度下降的推导，核心的结论是：当我们假设参数空间是一个欧几里得空间、参数的距离可以用欧几里得距离来衡量时，我们在某个点约束&lt;code&gt;$\|\Delta\theta\|_2\le\epsilon(\epsilon&amp;gt;0)$&lt;/code&gt;时，&lt;code&gt;$\Delta\theta$&lt;/code&gt;取梯度的反方向时可以让目标函数下降最多（具体的证明请参阅上述引文）。&lt;/p&gt;
&lt;p&gt;使用梯度下降最大的问题是，它实际上&lt;strong&gt;忽略了模型的结构&lt;/strong&gt;。换句话说，梯度下降相当于将模型所有参数展平为1维向量，并且用向量2范数来衡量每次更新的「步长」。这种抽象是实用的，但是也存在一定的问题。两组参数有可能在欧几里得空间中距离很近，但是诱导的模型输出空间距离很远。造成的结果就是更新的方向实际上不是目标函数下降最快的方向。&lt;/p&gt;
&lt;p&gt;这个问题要如何解决呢？在&lt;a href=&#34;https://nil9.net/posts/natural-gradient-descent/&#34;&gt;自然梯度（二）：黎曼距离下的最速下降&lt;/a&gt;中，我们介绍了自然梯度方法，即使用Fisher信息矩阵的逆作为梯度的pre-conditioner来矫正梯度下降的方向，从原理上是使用参数更新前后引导的概率分布的KL散度作为每次更新的步长约束。但是对于常见的深度神经网络来说，这样做仍然是不切实际的，因为FIM是一个&lt;code&gt;$N\times N$&lt;/code&gt;的大矩阵（其中&lt;code&gt;$N$&lt;/code&gt;是参数量），对于这么大的矩阵存储或求逆都是很难做到的。&lt;/p&gt;
&lt;h1 id=&#34;诱导范数作为步长约束&#34;&gt;诱导范数作为步长约束&lt;/h1&gt;
&lt;p&gt;是否有一种更「廉价」的方法，可以考虑&lt;strong&gt;模型的参数结构&lt;/strong&gt;，同时将&lt;strong&gt;参数的变化对于输出的影响&lt;/strong&gt;作为约束呢？&lt;/p&gt;
&lt;p&gt;幸运的是，对于当下最流行的神经网络（e.g., Transformer）而言，模型往往可以拆解为很多小模块，其中最常见的是Linear模块（线性映射，这里忽略bias term）&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ f(\boldsymbol{x};\boldsymbol{W})=\boldsymbol{Wx},\ \boldsymbol{W}\in\mathbb{R}^{n\times m},\boldsymbol{x}\in \mathbb{R}^{m}\\ $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;在标准的Transformer中，Attention、FFN、LM分类器都是由Linear模块组成的，Embedding从数学原理上也是输入为one-hot encoding的线性映射。假设现在对于某个Linear模块的参数&lt;code&gt;$\boldsymbol{W}$&lt;/code&gt;做&lt;code&gt;$\Delta\boldsymbol{W}$&lt;/code&gt;的更新（&lt;code&gt;$\boldsymbol{W}&#39;\leftarrow \boldsymbol{W}+\Delta\boldsymbol{W}$&lt;/code&gt;），我们需要衡量这个更新对于最终输出的影响是多少（从而可以约束这个影响）。由于神经网络比较复杂，衡量&lt;code&gt;$\Delta\boldsymbol{W}$&lt;/code&gt;对于最终目标函数的影响是相对繁琐的，但我们可以退而求其次，衡量&lt;code&gt;$\Delta\boldsymbol{W}$&lt;/code&gt;对于这个Linear模块的输出&lt;code&gt;$\boldsymbol{Wx}$&lt;/code&gt;的影响。
考虑线性模块的输入与输出空间的距离都使用欧几里得范数&lt;code&gt;$\|\cdot\|_{\ell_2}$&lt;/code&gt;衡量，那么这个约束可以通过如下不等式实现&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ \|\Delta\boldsymbol{W}\boldsymbol{x}\|_{\ell_2} \le {\color[rgb]{0, 0.5, 0.8}{\|\Delta\boldsymbol{W}\|_{\ell_2\to\ell_2}}}\|\boldsymbol{x}\|_{\ell_2} $$&lt;/code&gt;
这里的&lt;code&gt;${\color[rgb]{0, 0.5, 0.8}{\|\Delta\boldsymbol{W}\|_{\ell_2\to\ell_2}}}$&lt;/code&gt;是矩阵2范数。这个不等式告诉我们，如果约束了参数更新量的谱范数（不等式右侧），也就约束了更新前后这个线性模块输出的变化量。&lt;/p&gt;
&lt;p&gt;假设现在需要优化的神经网络是由一系列的线性模块堆叠组成（e.g., MLP），我们可以参照梯度下降的推导构造如下的更新&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ \underset{\Delta\boldsymbol{W}_1,\ldots,\Delta\boldsymbol{W}_L}{\text{arg min}}\left[ \sum_{l=1}^L {\langle \boldsymbol{G}_l, \Delta\boldsymbol{W}_l \rangle}_F + \frac{\lambda}{2}\max_{l=1}^L{\color[rgb]{0, 0.5, 0.8}{\|\Delta\boldsymbol{W}_l\|^2_{\ell_2\to\ell_2}}} \right]\\ $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这里&lt;code&gt;$\boldsymbol{G}_l$&lt;/code&gt;表示&lt;code&gt;$\boldsymbol{W}_l$&lt;/code&gt;对应的梯度矩阵（布局与原参数矩阵相同），&lt;code&gt;${\langle \cdot, \cdot \rangle}_F$&lt;/code&gt;表示Frobenius内积（对矩阵而言，逐元素相乘求和）。这里之所以使用&lt;code&gt;$\max_{l=1}^L$&lt;/code&gt;（而不是直接求和），是因为我们引入这个约束时希望目标函数在&lt;code&gt;$\Delta\boldsymbol{W}_l$&lt;/code&gt;变化下，能够保持平滑的性质&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;，因此需要bound所有参数矩阵更新量的谱范数的最大值。&lt;/p&gt;
&lt;p&gt;我们来逐步推导这个最小值成立时的&lt;code&gt;$\Delta\boldsymbol{W}_1,\ldots,\Delta\boldsymbol{W}_L$&lt;/code&gt;取值&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;。为了方便，把每个&lt;code&gt;$\Delta\boldsymbol{W}_l$&lt;/code&gt;拆解成大小和方向两部分：&lt;code&gt;$\Delta\boldsymbol{W}_l=c_l\boldsymbol{T}_l(c_l\triangleq\|\Delta\boldsymbol{W}_l\|_{\ell_2\to\ell_2})$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;（为了可读性，下面的&lt;code&gt;$\|\cdots\|$&lt;/code&gt;均表示谱范数&lt;code&gt;$\|\cdot\|_{\ell_2\to\ell_2}$&lt;/code&gt;）&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ \begin{align} &amp;amp;\underset{\Delta\boldsymbol{W}_1,\ldots,\Delta\boldsymbol{W}_L}{\text{min}}\left[ \sum_{l=1}^L {\langle \boldsymbol{G}_l, \Delta\boldsymbol{W}_l \rangle}_F + \frac{\lambda}{2}\max_{l=1}^L\|\Delta\boldsymbol{W}_l\|^2 \right]\\ &amp;amp;=\underset{c_1,\ldots,c_L\ge 0}{\text{min}}\left[ \sum_{l=1}^L c_l\min_{\|\boldsymbol{T}_l\|=1}{\langle \boldsymbol{G}_l, \boldsymbol{T}_l \rangle}_F + \frac{\lambda}{2}\max_{l=1}^Lc_l^2 \right]\\ &amp;amp;=\underset{c_1,\ldots,c_L\ge 0}{\text{min}}\left[ -\sum_{l=1}^L c_l\max_{\|\boldsymbol{T}_l\|=1}{\langle \boldsymbol{G}_l, \boldsymbol{T}_l \rangle}_F + \frac{\lambda}{2}\max_{l=1}^Lc_l^2 \right]\\ &amp;amp;=\underset{c_1,\ldots,c_L\ge 0}{\text{min}}\left[ -\sum_{l=1}^L c_l \|\boldsymbol{G}_l\|_* + \frac{\lambda}{2}\max_{l=1}^Lc_l^2 \right]\quad\triangleright\|\cdot\|_*\text{表示核范数}\\ &amp;amp;=\underset{\eta\ge 0}{\text{min}}\left[ -\sum_{l=1}^L \eta\|\boldsymbol{G}_l\|_* + \frac{\lambda}{2}\max_{l=1}^L \eta^2 \right]\tag{1}\\ \end{align} $$&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>自然梯度（二）：黎曼距离下的最速下降</title>
      <link>https://nil9.net/posts/natural-gradient-descent/</link>
      <pubDate>Thu, 06 Feb 2025 12:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/natural-gradient-descent/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://nil9.net/posts/fisher-info-matrix/&#34;&gt;上篇文章&lt;/a&gt;中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在本篇文章中，我们利用上篇得到的结论，推导自然梯度中为何引入FIM来修正梯度方向，并介绍自然梯度的一些性质。&lt;/p&gt;
&lt;h1 id=&#34;梯度下降欧氏距离下的最速下降&#34;&gt;梯度下降：欧氏距离下的最速下降&lt;/h1&gt;
&lt;p&gt;考虑一个最优化任务（&lt;code&gt;$f:\Theta\to\mathbb{R}$&lt;/code&gt;）：
&lt;code&gt;$$ \underset{\theta}{\operatorname{min}} f(\theta) $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;最常见的一阶优化方法是梯度下降/steepest descent：
&lt;code&gt;$$ \theta^+=\theta-\eta\nabla_\theta f $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;code&gt;$\eta$&lt;/code&gt;是学习率。这里的「steepest」指的是在约束欧氏距离定义下的步长在极小范围内时，选取梯度的负方向能最大化一步之内目标函数下降的程度。
&lt;code&gt;$$ \lim_{\epsilon\to 0}\frac{1}{\epsilon}\left(\underset{\delta:\|\delta\|\le \epsilon}{\operatorname{argmin}}  f(\theta+\delta)\right)=-\frac{\nabla_\theta f}{\|\nabla_\theta f\|}\tag{1} $$&lt;/code&gt;&lt;/p&gt;
&lt;style type=&#34;text/css&#34;&gt;
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.proof {
        --title-background-color: rgb(130, 130, 130);
        --content-background-color: #f7f7f7;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.proof {
        --title-background-color: rgb(129, 129, 129);
        --content-background-color: rgb(41, 41, 41);
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
&lt;/style&gt;&lt;div class=&#34;notice proof&#34; &gt;
    &lt;p class=&#34;notice-title&#34;&gt;
        &lt;span class=&#34;icon-notice baseline&#34;&gt;
            
        &lt;/span&gt;proof&lt;/p&gt;</description>
    </item>
    <item>
      <title>自然梯度（一）：Fisher信息矩阵作为黎曼度量</title>
      <link>https://nil9.net/posts/fisher-info-matrix/</link>
      <pubDate>Wed, 05 Feb 2025 00:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/fisher-info-matrix/</guid>
      <description>&lt;p&gt;在一般的梯度下降中，我们认为目标函数梯度的负方向可以最小化一步更新后的目标函数值，这里隐含地假设了参数空间是欧氏空间，且参数构成了一组正交归一的坐标系统。在很多情况下，这一假设是不成立的，作为结果，优化过程的收敛效率可能受到影响。&lt;/p&gt;
&lt;p&gt;作为解决这一问题的一种思路，自然梯度使用Fisher信息矩阵（的逆）作为梯度的pre-conditioner来矫正梯度的方向。本文将分为两篇，在第一篇中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在第二篇中，我们推导自然梯度中为何引入FIM来修正梯度方向，以及自然梯度的一些性质。&lt;/p&gt;
&lt;h1 id=&#34;score-function与fim&#34;&gt;Score function与FIM&lt;/h1&gt;
&lt;p&gt;假设我们有一个由&lt;code&gt;$\theta$&lt;/code&gt;参数化的概率模型，模型分布为&lt;code&gt;$p(x|\theta)$&lt;/code&gt;，记对数似然函数为&lt;code&gt;$\ell(\theta|x):=\log p(x|\theta)$&lt;/code&gt;。与对数似然函数相关的有两个定义，score function和fisher information。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;定义1（score function）&lt;/strong&gt;：score function &lt;code&gt;$s(\theta|x)$&lt;/code&gt;被定义为对数似然函数关于参数&lt;code&gt;$\theta$&lt;/code&gt;的梯度&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ s(\theta|x)=\nabla_\theta \ell(\theta|x) $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;一些文章会提到score function是用来为参数的好坏打分（score），这是不严谨的。score function中的「score」其实不是为参数打分，而是在Fisher研究的遗传统计问题中给基因异常家庭的「打分」(参见：&lt;a href=&#34;https://stats.stackexchange.com/questions/326091/interpretation-of-score&#34;&gt;Interpretation of &amp;ldquo;score&amp;rdquo;&lt;/a&gt;)。因此，score function只是约定俗成的一种名称，其实质就是似然函数的梯度，描述的是似然函数对于参数变化的敏感程度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;性质1&lt;/strong&gt;：Score function期望为0
&lt;code&gt;$$ \mathbb{E}_{p(x|\theta)}[s(\theta|x)]=\boldsymbol{0} $$&lt;/code&gt;&lt;/p&gt;
&lt;style type=&#34;text/css&#34;&gt;
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.proof {
        --title-background-color: rgb(130, 130, 130);
        --content-background-color: #f7f7f7;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.proof {
        --title-background-color: rgb(129, 129, 129);
        --content-background-color: rgb(41, 41, 41);
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
&lt;/style&gt;&lt;div class=&#34;notice proof&#34; &gt;
    &lt;p class=&#34;notice-title&#34;&gt;
        &lt;span class=&#34;icon-notice baseline&#34;&gt;
            
        &lt;/span&gt;proof&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
