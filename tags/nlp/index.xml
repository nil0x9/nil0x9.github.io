<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>NLP on nil9.net</title>
    <link>https://nil9.net/tags/nlp/</link>
    <description>Recent content in NLP on nil9.net</description>
    <image>
      <title>nil9.net</title>
      <url>https://nil9.net/images/android-chrome-512x512.png</url>
      <link>https://nil9.net/images/android-chrome-512x512.png</link>
    </image>
    <generator>Hugo -- 0.141.0</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sat, 18 Jan 2025 12:05:31 +0000</lastBuildDate>
    <atom:link href="https://nil9.net/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>关于语言建模中的Tied Embeddings的一点探讨</title>
      <link>https://nil9.net/posts/tied-embeddings-in-lm/</link>
      <pubDate>Sat, 18 Jan 2025 12:05:31 +0000</pubDate>
      <guid>https://nil9.net/posts/tied-embeddings-in-lm/</guid>
      <description>&lt;p&gt;Tied embeddings，即将语言模型中的输入Embeddings权重与输出分类器的权重两组参数共享的操作，一度是语言建模和机器翻译任务的标准配置。在语言模型大规模化之后，这种设计在开源模型中愈发少见了。前几天看到&lt;a href=&#34;https://www.zhihu.com/people/su-jian-lin-22&#34;&gt;@苏剑林&lt;/a&gt; 之前的一篇博客&lt;a href=&#34;https://kexue.fm/archives/9698&#34;&gt;语言模型输出端共享Embedding的重新探索&lt;/a&gt;，为tied embeddings的消失提供了一种视角，但也还有值得商榷的地方，本文想从这篇文章出发做一点探讨。&lt;/p&gt;
&lt;h1 id=&#34;初始loss的视角&#34;&gt;初始Loss的视角&lt;/h1&gt;
&lt;p&gt;这里先简要概括一下苏老师文章中的阐述框架&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。在使用Transformer做语言建模的时候，可能会使用类似DeepNorm等初始化手段，从而使每一个Transformer Block接近于一个恒等映射&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;，同时由于词元表征是0均值的，因此LayerNorm可以看做与RMSNorm等价&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;。所以，假设每个残差分支都初始化为0，假设输入中某个位置的初始embedding是&lt;code&gt;$\boldsymbol{w}_i$&lt;/code&gt;（对应词表中的第&lt;code&gt;$i$&lt;/code&gt;个词，维度是&lt;code&gt;$d$&lt;/code&gt;），那么最终得到的表征满足&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ \frac{\boldsymbol{w}_i}{\Vert\boldsymbol{w}_i\Vert \big/\sqrt{d}} \approx \frac{\boldsymbol{w}_i}{\sigma} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;假设在该位置的真实标签是词元&lt;code&gt;$j$&lt;/code&gt;，则损失函数可以由如下逼近&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ \begin{align}\mathcal{L}\triangleq -\log p(j|i) &amp;amp;= \log \sum\limits_k e^{\boldsymbol{w}_i\cdot \boldsymbol{w}_k / \sigma} - \boldsymbol{w}_i\cdot \boldsymbol{w}_j \big/ \sigma \\ &amp;amp;\approx \log \sum_k e^{\boldsymbol{w}_i\cdot \boldsymbol{w}_k / \sigma}\\ &amp;amp;=\log \left(e^{\boldsymbol{w}_i\cdot \boldsymbol{w}_i / \sigma} + \sum\limits_{k|k\neq i} e^{\boldsymbol{w}_i\cdot \boldsymbol{w}_k / \sigma}\right)\\ &amp;amp;\approx\log \left({\color{blue}e^{d \sigma}} + (n-1)\right) \end{align} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;其中&lt;code&gt;$|n|$&lt;/code&gt;是词表大小。在常见的模型维度下，这里的第一项&lt;code&gt;${\color{blue}e^{d \sigma}}$&lt;/code&gt;是比较大的。我们可以代入几个维度值看下第一项的大小，这里我们假设词表大小是32k，并考虑两种&lt;code&gt;$\sigma$&lt;/code&gt;取法，一种是比较常见的初始化超参数&lt;code&gt;$\sigma=0.02$&lt;/code&gt;，一种是取&lt;code&gt;$\sigma=1/\sqrt{d}$&lt;/code&gt;。可以看到无论是哪种初始化方法，对应的&lt;code&gt;${\color{blue}e^{d \sigma}}$&lt;/code&gt;都已经远远超过词表大小，响应地初始损失值也处于比较高的水平（按均匀分布的交叉熵是&lt;code&gt;$\log(n)\approx 10.37$&lt;/code&gt;）。&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  &lt;img loading=&#34;lazy&#34; src=&#34;https://nil9.net/images/tied-embeddings-in-lm/init_loss.png&#34; title=&#34;不同设定下的「初始损失值」&#34;&gt;&lt;figcaption class=&#34;image-caption&#34;&gt;不同设定下的「初始损失值」&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;以上是苏文中给出的关于语言建模中不再共享embedding的一个视角——tied embeddings会使语言模型的初始损失值很大。&lt;/p&gt;
&lt;p&gt;但这个问题实际上可以用一个rescale来解决，我们可以简单地将输出端乘以&lt;code&gt;$1/\sqrt{d}$&lt;/code&gt;，则损失函数可以做如下近似
&lt;code&gt;$$ \begin{align}\mathcal{L} &amp;amp;= \log \sum\limits_k e^{\boldsymbol{w}_i\cdot \boldsymbol{w}_k / (\sigma\sqrt{d})} - \boldsymbol{w}_i\cdot \boldsymbol{w}_j  \big/(\sigma\sqrt{d}) \\ &amp;amp;\approx\log \left({\color{blue}e^{\sigma\sqrt{d}}} + (n-1)\right) \end{align} $$&lt;/code&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
