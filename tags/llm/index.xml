<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>LLM on nil9.net</title>
    <link>https://nil9.net/tags/llm/</link>
    <description>Recent content in LLM on nil9.net</description>
    <image>
      <title>nil9.net</title>
      <url>https://nil9.net/images/android-chrome-512x512.png</url>
      <link>https://nil9.net/images/android-chrome-512x512.png</link>
    </image>
    <generator>Hugo -- 0.144.2</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 26 Feb 2025 00:00:41 +0000</lastBuildDate>
    <atom:link href="https://nil9.net/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>为什么LLM一般使用较大的权重衰减系数？</title>
      <link>https://nil9.net/posts/wd-model-precision/</link>
      <pubDate>Wed, 26 Feb 2025 00:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/wd-model-precision/</guid>
      <description>&lt;p&gt;最近在阅读&lt;a href=&#34;https://github.com/MoonshotAI/Moonlight/blob/master/Moonlight.pdf&#34;&gt;Muon is Scalable for LLM Training&lt;/a&gt;这篇文章的时候注意到他们使用无权重衰减（weight decay）版本的Muon优化LLM的时候，优化器的收敛优势会随着训练过程逐渐消失，又看到&lt;a href=&#34;https://www.zhihu.com/people/xiao-ming-tong-xue-86-81&#34;&gt;@小明同学&lt;/a&gt;提到的一个细节，很多开源的LLM在技术报告中都提到了使用0.1作为权重衰减的系数，觉得是个比较有意思的发现。结合Kimi的文章中关于bf16的简单陈述，笔者在本文中稍微展开讲下，权重衰减对于LLM的低精度训练中有什么作用。&lt;/p&gt;
&lt;p&gt;首先把结论放在前面：除了一般认知中的正则化作用，权重衰减也可能降低精度损失的风险——对于计算机的浮点数而言，&lt;strong&gt;绝对值越大，精度越低&lt;/strong&gt;。对于低精度/混合精度训练而言，使用权重衰减可以控制参数的绝对值范围，从而保证模型参数不落入低精度的数值区间。&lt;/p&gt;
&lt;h1 id=&#34;浮点数的存储与精度&#34;&gt;浮点数的存储与精度&lt;/h1&gt;
&lt;p&gt;上述结论主要与浮点数在计算机内的存储形式有关。学过计算机的一些基本课程的读者可能有印象，浮点数的存储是二进制的形式，分为符号位、指数位和尾数位三段。深度学习中常见的浮点数协议（fp32、fp16、bf16、tf32）的区别在于指数位和尾数位的比特数量不同。由于浮点数是一个「指数」的形式，&lt;strong&gt;因此它在实数空间的分布是不均匀的&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;这里我们考虑规范数的情形（指数位非全0），做一点分析。假设符号位、指数位、尾数位（mantissa）的二进制编码分别是&lt;code&gt;$S$&lt;/code&gt;、&lt;code&gt;$E$&lt;/code&gt;、&lt;code&gt;$M$&lt;/code&gt;，那么对应的浮点数为：
&lt;code&gt;$$ \text{value}=(-1)^S\times 1.M\times 2^{E-\text{bias}} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;在单精度fp32标准中，&lt;code&gt;$\text{bias}$&lt;/code&gt;取&lt;code&gt;${01111111}_{2}=127_{10}$&lt;/code&gt; .&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  &lt;img alt=&#34;fp32浮点数的一个例子&#34; loading=&#34;lazy&#34; src=&#34;https://nil9.net/images/wd-model-precision/ieee-fp32-example.png&#34; title=&#34;fp32浮点数的一个例子&#34;&gt;&lt;figcaption class=&#34;image-caption&#34;&gt;fp32浮点数的一个例子&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;例如在图中的例子中，&lt;code&gt;$S=0$&lt;/code&gt;，&lt;code&gt;$E=\underbrace{00\cdots0}_{7\ 0&#39;s}1$&lt;/code&gt;，&lt;code&gt;$M=\underbrace{00\cdots0}_{22\ 0&#39;s}1$&lt;/code&gt;，相应的值为
&lt;code&gt;$$ \begin{align} &amp;amp;{-1}^0\times 1.\underbrace{00\cdots0}_{22\ 0&#39;s}1_2\times 2^{00000001_2-{01111111}_{2}}\\ &amp;amp;\approx [1.175494490952134\times 10^{-38}]_{10} \end{align} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;现在我们来考虑不同的数值范围内的浮点数精度。对于区间范围&lt;code&gt;$[2^{x}, 2^{x+1}],\forall -126\le x\le127$&lt;/code&gt;（这里的x已经是经过-bias之后得到的最终指数），我们希望在给定任意浮点数&lt;code&gt;$y\in[2^{x}, 2^{x+1}]$&lt;/code&gt;的基础上增加一个最小量&lt;code&gt;$\varepsilon$&lt;/code&gt;（即区间内两个浮点数的最小间隔），这个增加的过程是通过操纵二进制编码实现的，那么最小间隔只能是通过在&lt;code&gt;$y$&lt;/code&gt;的尾数部分加上&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ 0.\underbrace{00\cdots0}_{22\ 0&#39;s}1 $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;来实现，对应的最小间隔是
&lt;code&gt;$$ \begin{align} \varepsilon &amp;amp;= 0.\underbrace{00\cdots0}_{22\text{ 0&#39;s}}1_2\times 2^{x}\\ &amp;amp;=2^{-23}\times2^{x}=2^{x-23}\\ \end{align} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;如果你将上面的公式带入不同的指数位，可以验证与&lt;a href=&#34;https://en.wikipedia.org/wiki/IEEE_754-1985&#34;&gt;Wikipedia&lt;/a&gt;中给出的这张表的Gap是吻合的：&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  &lt;img alt=&#34;不同指数位下的最小精度&#34; loading=&#34;lazy&#34; src=&#34;https://nil9.net/images/wd-model-precision/ieee-fp32-prec.png&#34; title=&#34;不同指数位下的最小精度，来源：Wikipedia&#34;&gt;&lt;figcaption class=&#34;image-caption&#34;&gt;不同指数位下的最小精度，来源：Wikipedia&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;这个计算可以拓展到其他的精度格式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于半精度格式fp16而言，其尾数位有10位，对应的最小间隔是&lt;code&gt;$2^{x-10}$&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;对于半精度格式bf16而言，其尾数位有7位，对应的最小间隔是&lt;code&gt;$2^{x-7}$&lt;/code&gt;；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从这里也可以看到，bf16相比fp16虽然拓宽了表示范围，但是减少了精度（同样数值范围内的最小间隔更宽了）。&lt;/p&gt;
&lt;p&gt;从这里我们得到了一个结论：计算机存储的浮点数之间的最小间隔随着浮点数绝对值数值增加，指数级地增大，换言之，&lt;strong&gt;浮点数（绝对值）数值越大，精度越低&lt;/strong&gt;。并且这个问题对于fp16或bf16格式的浮点数，问题要更加显著。&lt;/p&gt;
&lt;p&gt;这个结论的另一个引申的问题是&lt;strong&gt;舍入误差&lt;/strong&gt;，假如一个较大的浮点数和一个较小的浮点数相加，由于浮点数的加法（减法过程相当于取补码后相加，结论是类似的）过程需要先将两个数的指数位对齐，因此绝对值较小的数字的尾数的最后几位数字可能会在加法中丢失。这里我们举一个极端的例子来说明。&lt;/p&gt;
&lt;p&gt;假设浮点数存储为fp32格式（8位指数、23位尾数）。
&lt;code&gt;$$ \begin{align} x=(-1)^0\times 1.0_2\times 2^{-1}\\ y=(-1)^0\times 1.0_2\times 2^{-25}\\ \end{align} $$&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tensor Product Attention (TPA) 导读</title>
      <link>https://nil9.net/posts/tensor-product-attention/</link>
      <pubDate>Thu, 13 Feb 2025 00:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/tensor-product-attention/</guid>
      <description>&lt;p&gt;最近Deepseek比较出圈，连带着里面用到的MLA也被讨论得更多了。MLA无疑是一个非常出色的attention改进，但是由于它的KV cache设计，不能很好地兼容&lt;a href=&#34;https://kexue.fm/archives/8265&#34;&gt;RoPE&lt;/a&gt;，因此作者们使用了decoupled RoPE这样的「补丁」来引入位置关系，这无疑也增加了实现的复杂度。&lt;/p&gt;
&lt;p&gt;最近，修改注意力KV Cache这一线工作又增添了&lt;a href=&#34;https://arxiv.org/pdf/2501.06425&#34;&gt;TPA&lt;/a&gt;这个新成员，笔者觉得这篇文章比较有趣，因此希望写一篇简短的导读。之所以叫「导读」，是因为本文不打算太深入文章的formulation和实验，而是从笔者自己的视角出发介绍文章的一些重点贡献。。&lt;/p&gt;
&lt;h1 id=&#34;mha的拆解&#34;&gt;MHA的拆解&lt;/h1&gt;
&lt;p&gt;最标准的多头注意力（Vaswani的版本），大致可以拆解成3个步骤（这里默认讨论自注意力）&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;：
&lt;code&gt;$$ \begin{aligned} \text{step 1 }&amp;amp; \begin{cases} \boldsymbol{q}_i^{(h)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(h)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(h)}\in\mathbb{R}^{d\times d_k} \\  \boldsymbol{k}_i^{(h)} = \boldsymbol{x}_i\boldsymbol{W}_k^{(h)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{(h)}\in\mathbb{R}^{d\times d_k}\\  \boldsymbol{v}_i^{(h)} = \boldsymbol{x}_i\boldsymbol{W}_v^{(h)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(h)}\in\mathbb{R}^{d\times d_v}  \end{cases} \\ \text{step 2 }&amp;amp; \begin{cases} \boldsymbol{o}_t^{(h)} = \text{Attention}\left(\boldsymbol{q}_t^{(h)}, \boldsymbol{k}_{\leq t}^{(h)}, \boldsymbol{v}_{\leq t}^{(h)}\right)\triangleq\frac{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(h)} \boldsymbol{k}_i^{(h)}{}^{\top}\right)\boldsymbol{v}_i^{(h)}}{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(h)} \boldsymbol{k}_i^{(h)}{}^{\top}\right)} \\  \end{cases} \\ \text{step 3 }&amp;amp; \begin{cases} \boldsymbol{o}_t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(H)}\right]  \end{cases} \\ \end{aligned}\\ $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这里&lt;code&gt;$\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_l$&lt;/code&gt;是输入向量，上标&lt;code&gt;$h\in \{1,\ldots,H\}$&lt;/code&gt;表示注意力头，&lt;code&gt;$d,d_k,d_v$&lt;/code&gt;分别表示输入维度、key维度、value维度。在第一步中，我们将每个token的表征独立地线性投射到不同的自空间上，第二步是标准的点积注意力，第三步是拼接（后续再做一步线性变换）。&lt;/p&gt;
&lt;p&gt;我们重点来审视一下第一个步骤，这里由于每个token的表征是独立操作的（类似FFN），因此我们可以将每个token的表征看做一个样本点，这里做的事情就是将一个&lt;code&gt;$d$&lt;/code&gt;维度的向量，转化成3个矩阵&lt;code&gt;$\tilde{\boldsymbol{Q}}\in\mathbb{R}^{H\times d_k},\tilde{\boldsymbol{K}}\in\mathbb{R}^{H\times d_k},\tilde{\boldsymbol{V}}\in\mathbb{R}^{H\times d_v}$&lt;/code&gt;，每一个头对应这个矩阵中的一行。接着我们逐行计算每行对应的三组向量的点积注意力，就构成了标准的多头注意力。在标准实现中，这个变换是通过参数矩阵的线性变换+ reshape实现的。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
