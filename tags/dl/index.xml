<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DL on nil9.net</title>
    <link>https://nil9.net/tags/dl/</link>
    <description>Recent content in DL on nil9.net</description>
    <image>
      <title>nil9.net</title>
      <url>https://nil9.net/images/android-chrome-512x512.png</url>
      <link>https://nil9.net/images/android-chrome-512x512.png</link>
    </image>
    <generator>Hugo -- 0.145.0</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sat, 08 Mar 2025 00:00:41 +0000</lastBuildDate>
    <atom:link href="https://nil9.net/tags/dl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>简记：Muon中设计Newton-Schulz迭代的系数？</title>
      <link>https://nil9.net/posts/newton-schulz-4muon/</link>
      <pubDate>Sat, 08 Mar 2025 00:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/newton-schulz-4muon/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://nil9.net/posts/gd-norm-const/&#34;&gt;上篇文章&lt;/a&gt;介绍了Muon等新兴深度学习优化器背后的原理，即约束参数矩阵的诱导范数下得到新的更新方向。&lt;/p&gt;
&lt;p&gt;在Muon对参数更新方向&lt;code&gt;$-\boldsymbol{U}\boldsymbol{V}^\top$&lt;/code&gt;的计算中用到了Newton-Schulz迭代方法，本质上是在寻找这样一个多项式函数
&lt;code&gt;$$ f(x)=ax+bx^3+cx^5+\ldots $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;使其满足对任意&lt;code&gt;$x\in(0, 1]$&lt;/code&gt;，对&lt;code&gt;$x$&lt;/code&gt;应用多次&lt;code&gt;$f(\cdot)$&lt;/code&gt;，都能收敛到1附近。这里我们尝试设计一个能work的参数组合。&lt;/p&gt;
&lt;p&gt;我的一个简单的想法是，设计一个多项式函数，使&lt;code&gt;$x=1$&lt;/code&gt;是它的一个&lt;strong&gt;吸引不动点&lt;/strong&gt;：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;定义1&lt;/strong&gt;（&lt;em&gt;不动点&lt;/em&gt;）当&lt;code&gt;$x_0$&lt;/code&gt;被函数&lt;code&gt;$f(\cdot)$&lt;/code&gt;映射到自身，即&lt;code&gt;$f(x_0)=x_0$&lt;/code&gt;时，称&lt;code&gt;$x_0$&lt;/code&gt;是函数&lt;code&gt;$f(\cdot)$&lt;/code&gt;的一个不动点。&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;定义2&lt;/strong&gt;（&lt;em&gt;吸引不动点&lt;/em&gt;）&lt;code&gt;$f$&lt;/code&gt;的吸引不动点是&lt;code&gt;$f$&lt;/code&gt;的不动点&lt;code&gt;$x_0$&lt;/code&gt;使得，对在足够接近&lt;code&gt;$x_0$&lt;/code&gt;的定义域中的任何&lt;code&gt;$x$&lt;/code&gt;值而言，迭代函数序列&lt;code&gt;$x,f(x),f(f(x)),f(f(f(x))),\ldots$&lt;/code&gt;收敛于&lt;code&gt;$x_0$&lt;/code&gt;。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;要令&lt;code&gt;$x=1$&lt;/code&gt;是&lt;code&gt;$f(x)$&lt;/code&gt;的一个吸引不动点，要满足如下的必要条件：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;$f(1)=1$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$|f&#39;(1)|&amp;lt;1$&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;使用这两个条件是无法确定具体的参数值&lt;code&gt;$a,b,\ldots$&lt;/code&gt;的，但是对于三阶（参数包括&lt;code&gt;$a,b$&lt;/code&gt;两个）或者五阶（参数包括&lt;code&gt;$a,b,c$&lt;/code&gt;三个）的Newton-Schulz迭代，可以大大缩小搜索的空间。下面展开看下。&lt;/p&gt;
&lt;h1 id=&#34;三阶迭代&#34;&gt;三阶迭代&lt;/h1&gt;
&lt;p&gt;先讨论三阶迭代的形式&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ f(x)=ax+bx^3 $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;代入上面的两个必要条件：
&lt;code&gt;$$ \begin{split} f(1)=a+b = 1\\ -1 &amp;lt; f&#39;(1)=a+3b &amp;lt; 1\\ \end{split} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;根据第一个条件，可以把&lt;code&gt;$b$&lt;/code&gt;用&lt;code&gt;$1-a$&lt;/code&gt;重参数化，然后就有可行的条件
&lt;code&gt;$$ 1 &amp;lt; a &amp;lt; 2 $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;我们记五次迭代后的函数&lt;code&gt;$\phi(x)=f(f(f(f(f(x)))))$&lt;/code&gt;，可视化看一下不同&lt;code&gt;$a$&lt;/code&gt;取值下对应的情况（理想情况下，对于&lt;code&gt;$(0,1]$&lt;/code&gt;区间内的&lt;code&gt;$x$&lt;/code&gt;，曲线要尽可能接近&lt;code&gt;$y=1$&lt;/code&gt;）&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  &lt;img alt=&#34;三阶迭代下，a取不同取值时对应的φ(x)&#34; loading=&#34;lazy&#34; src=&#34;https://nil9.net/images/newton-schulz-4muon/ns3.gif&#34; title=&#34;三阶迭代下，a取不同取值时对应的φ(x)&#34;&gt;&lt;figcaption class=&#34;image-caption&#34;&gt;三阶迭代下，a取不同取值时对应的φ(x)&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;注意到在&lt;code&gt;$a$&lt;/code&gt;接近1的时候，&lt;code&gt;$\phi(x)$&lt;/code&gt;收敛到1附近的邻域是比较窄的，随着&lt;code&gt;$a\to 2$&lt;/code&gt;，收敛到1附近的「邻域」范围逐渐拓宽，但在&lt;code&gt;$a=2$&lt;/code&gt;附近，曲线开始出现一定的抖动。对于优化器而言，这样的局部近似的方差是可以容忍的，因此我们可以选取一个比较接近2的值作为&lt;code&gt;$a$&lt;/code&gt;的参数，例如&lt;code&gt;$a=1.99,b=-0.99$&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;作为对比，在&lt;a href=&#34;https://arxiv.org/pdf/2410.21265&#34;&gt;Bernstein &amp;amp; Newhouse 2024.&lt;/a&gt;中，作者给出的参数是&lt;code&gt;$a=3/2,b=-1/2$&lt;/code&gt;。可以在下图中对照两种设定下的&lt;code&gt;$\phi(x)$&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  &lt;img alt=&#34;两种φ(x)对比&#34; loading=&#34;lazy&#34; src=&#34;https://nil9.net/images/newton-schulz-4muon/phi_curve_ns3.png&#34; title=&#34;两种φ(x)对比&#34;&gt;&lt;figcaption class=&#34;image-caption&#34;&gt;两种φ(x)对比&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;可以看到Bernstein给出的参数虽然更平滑地收敛于1，但是对于在0附近的初始&lt;code&gt;$x$&lt;/code&gt;，普遍无法收敛到1。也就是说对于较小的奇异值对应的&lt;code&gt;$\boldsymbol{u}_i, \boldsymbol{v}_i$&lt;/code&gt;，倾向于在更新中被忽略。&lt;/p&gt;
&lt;p&gt;在&lt;code&gt;$x=0$&lt;/code&gt;附近&lt;code&gt;$\phi(x)$&lt;/code&gt;能否快速接近1，主要取决于参数&lt;code&gt;$a$&lt;/code&gt;的大小，这是因为&lt;code&gt;$\phi&#39;(0)=a^5$&lt;/code&gt;。所以应该在尽可能保证&lt;code&gt;$\phi(x)\approx 1,\forall x\in(0,1]$&lt;/code&gt;的同时，让&lt;code&gt;$a$&lt;/code&gt;尽可能大。&lt;/p&gt;
&lt;h1 id=&#34;五阶迭代&#34;&gt;五阶迭代&lt;/h1&gt;
&lt;p&gt;现在来考虑五阶迭代的形式&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ f(x)=ax+bx^3+cx^5 $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;代入上面的两个必要条件：
&lt;code&gt;$$ \begin{split} f(1)=a+b+c = 1\\ -1 &amp;lt; f&#39;(1)=a+3b+5c &amp;lt; 1\\ \end{split} $$&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>从约束视角看深度学习优化若干新进展</title>
      <link>https://nil9.net/posts/gd-norm-const/</link>
      <pubDate>Wed, 05 Mar 2025 00:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/gd-norm-const/</guid>
      <description>&lt;p&gt;在深度学习中最常用的优化方法是梯度下降方法及其变体。在过去很长一段时间中，Adam优化器是NLP社区的默认选择，在ViT出现之后，CV方面的工作也逐渐开始使用Adam和Adam的变体（在ViT之前，一种常见的观点是Adam不适用于Vision任务）。&lt;/p&gt;
&lt;p&gt;最近Muon优化器在Kimi的新工作带动下又火了一把，相较于Adam优化器需要同时维护一阶、二阶矩估计量，Muon只需要维护一份梯度的动量估计，因此在大规模训练中有很大优势。最近笔者顺着Muon的reference看了Jeremy Bernstein在优化的一些文章，觉得很有意思，因此写这篇文章梳理一下这一系列工作的部分精要。本文的核心论点是：使用诱导范数来约束梯度更新，可以推导出最近的一些新出现的优化方法，这也可能是未来深度学习优化的一个有潜力的探索方向。&lt;/p&gt;
&lt;h1 id=&#34;梯度下降recap&#34;&gt;梯度下降（Recap）&lt;/h1&gt;
&lt;p&gt;当前深度学习优化算法的基石是梯度下降。之前笔者写过一篇拙文（&lt;a href=&#34;https://nil9.net/posts/natural-gradient-descent/&#34;&gt;自然梯度（二）：黎曼距离下的最速下降&lt;/a&gt;）整理过梯度下降的推导，核心的结论是：当我们假设参数空间是一个欧几里得空间、参数的距离可以用欧几里得距离来衡量时，我们在某个点约束&lt;code&gt;$\|\Delta\theta\|_2\le\epsilon(\epsilon&amp;gt;0)$&lt;/code&gt;时，&lt;code&gt;$\Delta\theta$&lt;/code&gt;取梯度的反方向时可以让目标函数下降最多（具体的证明请参阅上述引文）。&lt;/p&gt;
&lt;p&gt;使用梯度下降最大的问题是，它实际上&lt;strong&gt;忽略了模型的结构&lt;/strong&gt;。换句话说，梯度下降相当于将模型所有参数展平为1维向量，并且用向量2范数来衡量每次更新的「步长」。这种抽象是实用的，但是也存在一定的问题。两组参数有可能在欧几里得空间中距离很近，但是诱导的模型输出空间距离很远。造成的结果就是更新的方向实际上不是目标函数下降最快的方向。&lt;/p&gt;
&lt;p&gt;这个问题要如何解决呢？在&lt;a href=&#34;https://nil9.net/posts/natural-gradient-descent/&#34;&gt;自然梯度（二）：黎曼距离下的最速下降&lt;/a&gt;中，我们介绍了自然梯度方法，即使用Fisher信息矩阵的逆作为梯度的pre-conditioner来矫正梯度下降的方向，从原理上是使用参数更新前后引导的概率分布的KL散度作为每次更新的步长约束。但是对于常见的深度神经网络来说，这样做仍然是不切实际的，因为FIM是一个&lt;code&gt;$N\times N$&lt;/code&gt;的大矩阵（其中&lt;code&gt;$N$&lt;/code&gt;是参数量），对于这么大的矩阵存储或求逆都是很难做到的。&lt;/p&gt;
&lt;h1 id=&#34;诱导范数作为步长约束&#34;&gt;诱导范数作为步长约束&lt;/h1&gt;
&lt;p&gt;是否有一种更「廉价」的方法，可以考虑&lt;strong&gt;模型的参数结构&lt;/strong&gt;，同时将&lt;strong&gt;参数的变化对于输出的影响&lt;/strong&gt;作为约束呢？&lt;/p&gt;
&lt;p&gt;幸运的是，对于当下最流行的神经网络（e.g., Transformer）而言，模型往往可以拆解为很多小模块，其中最常见的是Linear模块（线性映射，这里忽略bias term）&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ f(\boldsymbol{x};\boldsymbol{W})=\boldsymbol{Wx},\ \boldsymbol{W}\in\mathbb{R}^{n\times m},\boldsymbol{x}\in \mathbb{R}^{m}\\ $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;在标准的Transformer中，Attention、FFN、LM分类器都是由Linear模块组成的，Embedding从数学原理上也是输入为one-hot encoding的线性映射。假设现在对于某个Linear模块的参数&lt;code&gt;$\boldsymbol{W}$&lt;/code&gt;做&lt;code&gt;$\Delta\boldsymbol{W}$&lt;/code&gt;的更新（&lt;code&gt;$\boldsymbol{W}&#39;\leftarrow \boldsymbol{W}+\Delta\boldsymbol{W}$&lt;/code&gt;），我们需要衡量这个更新对于最终输出的影响是多少（从而可以约束这个影响）。由于神经网络比较复杂，衡量&lt;code&gt;$\Delta\boldsymbol{W}$&lt;/code&gt;对于最终目标函数的影响是相对繁琐的，但我们可以退而求其次，衡量&lt;code&gt;$\Delta\boldsymbol{W}$&lt;/code&gt;对于这个Linear模块的输出&lt;code&gt;$\boldsymbol{Wx}$&lt;/code&gt;的影响。
考虑线性模块的输入与输出空间的距离都使用欧几里得范数&lt;code&gt;$\|\cdot\|_{\ell_2}$&lt;/code&gt;衡量，那么这个约束可以通过如下不等式实现&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ \|\Delta\boldsymbol{W}\boldsymbol{x}\|_{\ell_2} \le {\color[rgb]{0, 0.5, 0.8}{\|\Delta\boldsymbol{W}\|_{\ell_2\to\ell_2}}}\|\boldsymbol{x}\|_{\ell_2} $$&lt;/code&gt;
这里的&lt;code&gt;${\color[rgb]{0, 0.5, 0.8}{\|\Delta\boldsymbol{W}\|_{\ell_2\to\ell_2}}}$&lt;/code&gt;是矩阵2范数。这个不等式告诉我们，如果约束了参数更新量的谱范数（不等式右侧），也就约束了更新前后这个线性模块输出的变化量。&lt;/p&gt;
&lt;p&gt;假设现在需要优化的神经网络是由一系列的线性模块堆叠组成（e.g., MLP），我们可以参照梯度下降的推导构造如下的更新&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ \underset{\Delta\boldsymbol{W}_1,\ldots,\Delta\boldsymbol{W}_L}{\text{arg min}}\left[ \sum_{l=1}^L {\langle \boldsymbol{G}_l, \Delta\boldsymbol{W}_l \rangle}_F + \frac{\lambda}{2}\max_{l=1}^L{\color[rgb]{0, 0.5, 0.8}{\|\Delta\boldsymbol{W}_l\|^2_{\ell_2\to\ell_2}}} \right]\\ $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这里&lt;code&gt;$\boldsymbol{G}_l$&lt;/code&gt;表示&lt;code&gt;$\boldsymbol{W}_l$&lt;/code&gt;对应的梯度矩阵（布局与原参数矩阵相同），&lt;code&gt;${\langle \cdot, \cdot \rangle}_F$&lt;/code&gt;表示Frobenius内积（对矩阵而言，逐元素相乘求和）。这里之所以使用&lt;code&gt;$\max_{l=1}^L$&lt;/code&gt;（而不是直接求和），是因为我们引入这个约束时希望目标函数在&lt;code&gt;$\Delta\boldsymbol{W}_l$&lt;/code&gt;变化下，能够保持平滑的性质&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;，因此需要bound所有参数矩阵更新量的谱范数的最大值。&lt;/p&gt;
&lt;p&gt;我们来逐步推导这个最小值成立时的&lt;code&gt;$\Delta\boldsymbol{W}_1,\ldots,\Delta\boldsymbol{W}_L$&lt;/code&gt;取值&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;。为了方便，把每个&lt;code&gt;$\Delta\boldsymbol{W}_l$&lt;/code&gt;拆解成大小和方向两部分：&lt;code&gt;$\Delta\boldsymbol{W}_l=c_l\boldsymbol{T}_l(c_l\triangleq\|\Delta\boldsymbol{W}_l\|_{\ell_2\to\ell_2})$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;（为了可读性，下面的&lt;code&gt;$\|\cdots\|$&lt;/code&gt;均表示谱范数&lt;code&gt;$\|\cdot\|_{\ell_2\to\ell_2}$&lt;/code&gt;）&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ \begin{align} &amp;amp;\underset{\Delta\boldsymbol{W}_1,\ldots,\Delta\boldsymbol{W}_L}{\text{min}}\left[ \sum_{l=1}^L {\langle \boldsymbol{G}_l, \Delta\boldsymbol{W}_l \rangle}_F + \frac{\lambda}{2}\max_{l=1}^L\|\Delta\boldsymbol{W}_l\|^2 \right]\\ &amp;amp;=\underset{c_1,\ldots,c_L\ge 0}{\text{min}}\left[ \sum_{l=1}^L c_l\min_{\|\boldsymbol{T}_l\|=1}{\langle \boldsymbol{G}_l, \boldsymbol{T}_l \rangle}_F + \frac{\lambda}{2}\max_{l=1}^Lc_l^2 \right]\\ &amp;amp;=\underset{c_1,\ldots,c_L\ge 0}{\text{min}}\left[ -\sum_{l=1}^L c_l\max_{\|\boldsymbol{T}_l\|=1}{\langle \boldsymbol{G}_l, \boldsymbol{T}_l \rangle}_F + \frac{\lambda}{2}\max_{l=1}^Lc_l^2 \right]\\ &amp;amp;=\underset{c_1,\ldots,c_L\ge 0}{\text{min}}\left[ -\sum_{l=1}^L c_l \|\boldsymbol{G}_l\|_* + \frac{\lambda}{2}\max_{l=1}^Lc_l^2 \right]\quad\triangleright\|\cdot\|_*\text{表示核范数}\\ &amp;amp;=\underset{\eta\ge 0}{\text{min}}\left[ -\sum_{l=1}^L \eta\|\boldsymbol{G}_l\|_* + \frac{\lambda}{2}\max_{l=1}^L \eta^2 \right]\tag{1}\\ \end{align} $$&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>为什么LLM一般使用较大的权重衰减系数？</title>
      <link>https://nil9.net/posts/wd-model-precision/</link>
      <pubDate>Wed, 26 Feb 2025 00:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/wd-model-precision/</guid>
      <description>&lt;p&gt;最近在阅读&lt;a href=&#34;https://github.com/MoonshotAI/Moonlight/blob/master/Moonlight.pdf&#34;&gt;Muon is Scalable for LLM Training&lt;/a&gt;这篇文章的时候注意到他们使用无权重衰减（weight decay）版本的Muon优化LLM的时候，优化器的收敛优势会随着训练过程逐渐消失，又看到&lt;a href=&#34;https://www.zhihu.com/people/xiao-ming-tong-xue-86-81&#34;&gt;@小明同学&lt;/a&gt;在评论区提到的一个细节，很多开源的LLM在技术报告中都提到了使用0.1作为权重衰减的系数，觉得是个比较有意思的发现。结合Kimi的文章中关于bf16的简单陈述，笔者在本文中稍微展开讲下，权重衰减对于LLM的低精度训练中有什么作用。&lt;/p&gt;
&lt;p&gt;首先把结论放在前面：除了一般认知中的正则化作用，权重衰减也可能降低精度损失的风险——对于计算机的浮点数而言，&lt;strong&gt;绝对值越大，精度越低&lt;/strong&gt;。对于低精度/混合精度训练而言，使用权重衰减可以控制参数的绝对值范围，从而保证模型参数不落入低精度的数值区间。&lt;/p&gt;
&lt;h1 id=&#34;浮点数的存储与精度&#34;&gt;浮点数的存储与精度&lt;/h1&gt;
&lt;p&gt;上述结论主要与浮点数在计算机内的存储形式有关。学过计算机的一些基本课程的读者可能有印象，浮点数的存储是二进制的形式，分为符号位、指数位和尾数位三段。深度学习中常见的浮点数协议（fp32、fp16、bf16、tf32）的区别在于指数位和尾数位的比特数量不同。由于浮点数是一个「指数」的形式，&lt;strong&gt;因此它在实数空间的分布是不均匀的&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;这里我们考虑规范数的情形（指数位非全0），做一点分析。假设符号位、指数位、尾数位（mantissa）的二进制编码分别是&lt;code&gt;$S$&lt;/code&gt;、&lt;code&gt;$E$&lt;/code&gt;、&lt;code&gt;$M$&lt;/code&gt;，那么对应的浮点数为：
&lt;code&gt;$$ \text{value}=(-1)^S\times 1.M\times 2^{E-\text{bias}} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;在单精度fp32标准中，&lt;code&gt;$\text{bias}$&lt;/code&gt;取&lt;code&gt;${01111111}_{2}=127_{10}$&lt;/code&gt; .&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  &lt;img alt=&#34;fp32浮点数的一个例子&#34; loading=&#34;lazy&#34; src=&#34;https://nil9.net/images/wd-model-precision/ieee-fp32-example.png&#34; title=&#34;fp32浮点数的一个例子&#34;&gt;&lt;figcaption class=&#34;image-caption&#34;&gt;fp32浮点数的一个例子&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;例如在图中的例子中，&lt;code&gt;$S=0$&lt;/code&gt;，&lt;code&gt;$E=\underbrace{00\cdots0}_{7\ 0&#39;s}1$&lt;/code&gt;，&lt;code&gt;$M=\underbrace{00\cdots0}_{22\ 0&#39;s}1$&lt;/code&gt;，相应的值为
&lt;code&gt;$$ \begin{align} &amp;amp;{-1}^0\times 1.\underbrace{00\cdots0}_{22\ 0&#39;s}1_2\times 2^{00000001_2-{01111111}_{2}}\\ &amp;amp;\approx [1.175494490952134\times 10^{-38}]_{10} \end{align} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;现在我们来考虑不同的数值范围内的浮点数精度。对于区间范围&lt;code&gt;$[2^{x}, 2^{x+1}],\forall -126\le x\le127$&lt;/code&gt;（这里的x已经是经过-bias之后得到的最终指数），我们希望在给定任意浮点数&lt;code&gt;$y\in[2^{x}, 2^{x+1}]$&lt;/code&gt;的基础上增加一个最小量&lt;code&gt;$\varepsilon$&lt;/code&gt;（即区间内两个浮点数的最小间隔），这个增加的过程是通过操纵二进制编码实现的，那么最小间隔只能是通过在&lt;code&gt;$y$&lt;/code&gt;的尾数部分加上&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ 0.\underbrace{00\cdots0}_{22\ 0&#39;s}1 $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;来实现，对应的最小间隔是
&lt;code&gt;$$ \begin{align} \varepsilon &amp;amp;= 0.\underbrace{00\cdots0}_{22\text{ 0&#39;s}}1_2\times 2^{x}\\ &amp;amp;=2^{-23}\times2^{x}=2^{x-23}\\ \end{align} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;如果你将上面的公式带入不同的指数位，可以验证与&lt;a href=&#34;https://en.wikipedia.org/wiki/IEEE_754-1985&#34;&gt;Wikipedia&lt;/a&gt;中给出的这张表的Gap是吻合的：&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  &lt;img alt=&#34;不同指数位下的最小精度&#34; loading=&#34;lazy&#34; src=&#34;https://nil9.net/images/wd-model-precision/ieee-fp32-prec.png&#34; title=&#34;不同指数位下的最小精度，来源：Wikipedia&#34;&gt;&lt;figcaption class=&#34;image-caption&#34;&gt;不同指数位下的最小精度，来源：Wikipedia&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;这个计算可以拓展到其他的精度格式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于半精度格式fp16而言，其尾数位有10位，对应的最小间隔是&lt;code&gt;$2^{x-10}$&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;对于半精度格式bf16而言，其尾数位有7位，对应的最小间隔是&lt;code&gt;$2^{x-7}$&lt;/code&gt;；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从这里也可以看到，bf16相比fp16虽然拓宽了表示范围，但是减少了精度（同样数值范围内的最小间隔更宽了）。&lt;/p&gt;
&lt;p&gt;从这里我们得到了一个结论：计算机存储的浮点数之间的最小间隔随着浮点数绝对值数值增加，指数级地增大，换言之，&lt;strong&gt;浮点数（绝对值）数值越大，精度越低&lt;/strong&gt;。并且这个问题对于fp16或bf16格式的浮点数，问题要更加显著。&lt;/p&gt;
&lt;p&gt;这个结论的另一个引申的问题是&lt;strong&gt;舍入误差&lt;/strong&gt;，假如一个较大的浮点数和一个较小的浮点数相加，由于浮点数的加法（减法过程相当于取补码后相加，结论是类似的）过程需要先将两个数的指数位对齐，因此绝对值较小的数字的尾数的最后几位数字可能会在加法中丢失。这里我们举一个极端的例子来说明。&lt;/p&gt;
&lt;p&gt;假设浮点数存储为fp32格式（8位指数、23位尾数）。
&lt;code&gt;$$ \begin{align} x=(-1)^0\times 1.0_2\times 2^{-1}\\ y=(-1)^0\times 1.0_2\times 2^{-25}\\ \end{align} $$&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tensor Product Attention (TPA) 导读</title>
      <link>https://nil9.net/posts/tensor-product-attention/</link>
      <pubDate>Thu, 13 Feb 2025 00:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/tensor-product-attention/</guid>
      <description>&lt;p&gt;最近Deepseek比较出圈，连带着里面用到的MLA也被讨论得更多了。MLA无疑是一个非常出色的attention改进，但是由于它的KV cache设计，不能很好地兼容&lt;a href=&#34;https://kexue.fm/archives/8265&#34;&gt;RoPE&lt;/a&gt;，因此作者们使用了decoupled RoPE这样的「补丁」来引入位置关系，这无疑也增加了实现的复杂度。&lt;/p&gt;
&lt;p&gt;最近，修改注意力KV Cache这一线工作又增添了&lt;a href=&#34;https://arxiv.org/pdf/2501.06425&#34;&gt;TPA&lt;/a&gt;这个新成员，笔者觉得这篇文章比较有趣，因此希望写一篇简短的导读。之所以叫「导读」，是因为本文不打算太深入文章的formulation和实验，而是从笔者自己的视角出发介绍文章的一些重点贡献。。&lt;/p&gt;
&lt;h1 id=&#34;mha的拆解&#34;&gt;MHA的拆解&lt;/h1&gt;
&lt;p&gt;最标准的多头注意力（Vaswani的版本），大致可以拆解成3个步骤（这里默认讨论自注意力）&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;：
&lt;code&gt;$$ \begin{aligned} \text{step 1 }&amp;amp; \begin{cases} \boldsymbol{q}_i^{(h)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(h)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(h)}\in\mathbb{R}^{d\times d_k} \\  \boldsymbol{k}_i^{(h)} = \boldsymbol{x}_i\boldsymbol{W}_k^{(h)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{(h)}\in\mathbb{R}^{d\times d_k}\\  \boldsymbol{v}_i^{(h)} = \boldsymbol{x}_i\boldsymbol{W}_v^{(h)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(h)}\in\mathbb{R}^{d\times d_v}  \end{cases} \\ \text{step 2 }&amp;amp; \begin{cases} \boldsymbol{o}_t^{(h)} = \text{Attention}\left(\boldsymbol{q}_t^{(h)}, \boldsymbol{k}_{\leq t}^{(h)}, \boldsymbol{v}_{\leq t}^{(h)}\right)\triangleq\frac{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(h)} \boldsymbol{k}_i^{(h)}{}^{\top}\right)\boldsymbol{v}_i^{(h)}}{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(h)} \boldsymbol{k}_i^{(h)}{}^{\top}\right)} \\  \end{cases} \\ \text{step 3 }&amp;amp; \begin{cases} \boldsymbol{o}_t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(H)}\right]  \end{cases} \\ \end{aligned}\\ $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这里&lt;code&gt;$\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_l$&lt;/code&gt;是输入向量，上标&lt;code&gt;$h\in \{1,\ldots,H\}$&lt;/code&gt;表示注意力头，&lt;code&gt;$d,d_k,d_v$&lt;/code&gt;分别表示输入维度、key维度、value维度。在第一步中，我们将每个token的表征独立地线性投射到不同的自空间上，第二步是标准的点积注意力，第三步是拼接（后续再做一步线性变换）。&lt;/p&gt;
&lt;p&gt;我们重点来审视一下第一个步骤，这里由于每个token的表征是独立操作的（类似FFN），因此我们可以将每个token的表征看做一个样本点，这里做的事情就是将一个&lt;code&gt;$d$&lt;/code&gt;维度的向量，转化成3个矩阵&lt;code&gt;$\tilde{\boldsymbol{Q}}\in\mathbb{R}^{H\times d_k},\tilde{\boldsymbol{K}}\in\mathbb{R}^{H\times d_k},\tilde{\boldsymbol{V}}\in\mathbb{R}^{H\times d_v}$&lt;/code&gt;，每一个头对应这个矩阵中的一行。接着我们逐行计算每行对应的三组向量的点积注意力，就构成了标准的多头注意力。在标准实现中，这个变换是通过参数矩阵的线性变换+ reshape实现的。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
