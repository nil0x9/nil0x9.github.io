<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DL on nil9.net</title>
    <link>https://nil9.net/tags/dl/</link>
    <description>Recent content in DL on nil9.net</description>
    <image>
      <title>nil9.net</title>
      <url>https://nil9.net/images/android-chrome-512x512.png</url>
      <link>https://nil9.net/images/android-chrome-512x512.png</link>
    </image>
    <generator>Hugo -- 0.145.0</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 22 May 2025 00:00:41 +0000</lastBuildDate>
    <atom:link href="https://nil9.net/tags/dl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>简记：Attention Logits究竟如何放缩？</title>
      <link>https://nil9.net/posts/attn-logits-scale/</link>
      <pubDate>Thu, 22 May 2025 00:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/attn-logits-scale/</guid>
      <description>&lt;p&gt;最近在细读Tensor Programs的时候，发现一个有意思的细节，&lt;a href=&#34;https://arxiv.org/abs/2203.03466&#34;&gt;Greg 2022.&lt;/a&gt;在将mup应用到Transformer的时候，对Attention Logits使用&lt;code&gt;$1/d_k$&lt;/code&gt;进行放缩，而不是传统上的&lt;code&gt;$1/\sqrt{d_k}$&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;我不禁回想起很多年前在知乎的一个问题：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/339723385/answer/782509914&#34;&gt;transformer中的attention为什么scaled? - Nil-9的回答&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;当时笔者沿着&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Vaswani 2017.&lt;/a&gt;的一个脚注，展开写了这样一篇回答，其数学原理并不复杂，浓缩版本是这样的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;假设&lt;code&gt;$\boldsymbol{q}$&lt;/code&gt;,&lt;code&gt;$\boldsymbol{k}\in\mathbb{R}^{d_k}$&lt;/code&gt;是&lt;strong&gt;相互独立&lt;/strong&gt;的随机变量，均值和方差分别是0、1，则点积&lt;code&gt;$\langle\boldsymbol{q},\boldsymbol{k}\rangle$&lt;/code&gt;的量级是&lt;code&gt;$\Theta(\sqrt{d_k})$&lt;/code&gt;。为了防止点积过大导致softmax出现梯度消失问题，因此需要再点积基础上乘上&lt;code&gt;$1/\sqrt{d_k}$&lt;/code&gt;，将点积稳定在&lt;code&gt;$\Theta(1)$&lt;/code&gt;量级。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;这个问题后来也成为了某些公司的所谓「机器学习八股文」之一。但这个放缩是不是严谨的呢？我们来仔细思考一下。&lt;/p&gt;
&lt;p&gt;这里比较成问题的是「相互独立」的假设。我们先建立如下的基本的结论。&lt;/p&gt;
&lt;p&gt;假设&lt;code&gt;$X,Y$&lt;/code&gt;是两个随机变量，已知：
&lt;code&gt;$$ \mathbb{E}[X]= \mathbb{E}[Y] = 0\\ \text{Var}[X]=\text{Var}[Y] = 1 $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;那么乘积&lt;code&gt;$XY$&lt;/code&gt;满足：
&lt;code&gt;$$ \mathbb{E}[XY]= \rho $$&lt;/code&gt;
如果进一步地，&lt;code&gt;$X,Y$&lt;/code&gt;均为正态分布随机变量，则有
&lt;code&gt;$$ \text{Var}[XY]= 1 + \rho^2 $$&lt;/code&gt;
其中&lt;code&gt;$\rho$&lt;/code&gt;是&lt;code&gt;$X$&lt;/code&gt;和&lt;code&gt;$Y$&lt;/code&gt;的相关系数。&lt;/p&gt;
&lt;style type=&#34;text/css&#34;&gt;
     
    .notice {
        --title-color: #fff;
        --title-background-color: #6be;
        --content-color: #444;
        --content-background-color: #e7f2fa;
    }

    .notice.proof {
        --title-background-color: rgb(130, 130, 130);
        --content-background-color: #f7f7f7;
    }

    .notice.info {
        --title-background-color: #fb7;
        --content-background-color: #fec;
    }

    .notice.tip {
        --title-background-color: #5a5;
        --content-background-color: #efe;
    }

    .notice.warning {
        --title-background-color: #c33;
        --content-background-color: #fee;
    }

     

    body.dark .notice {
        --title-color: #fff;
        --title-background-color: #069;
        --content-color: #ddd;
        --content-background-color: #023;
    }

    body.dark .notice.proof {
        --title-background-color: rgb(129, 129, 129);
        --content-background-color: rgb(41, 41, 41);
    }

    body.dark .notice.info {
        --title-background-color: #a50;
        --content-background-color: #420;
    }

    body.dark .notice.tip {
        --title-background-color: #363;
        --content-background-color: #121;
    }

    body.dark .notice.warning {
        --title-background-color: #800;
        --content-background-color: #400;
    }

     
    .notice {
        padding: 18px;
        line-height: 24px;
        margin-bottom: 24px;
        border-radius: 4px;
        color: var(--content-color);
        background: var(--content-background-color);
    }

    .notice p:last-child {
        margin-bottom: 0
    }

     
    .notice-title {
        margin: -18px -18px 12px;
        padding: 4px 18px;
        border-radius: 4px 4px 0 0;
        font-weight: 700;
        color: var(--title-color);
        background: var(--title-background-color);
    }

     
    .icon-notice {
        display: inline-flex;
        align-self: center;
        margin-right: 8px;
    }

    .icon-notice img,
    .icon-notice svg {
        height: 1em;
        width: 1em;
        fill: currentColor;
    }

    .icon-notice img,
    .icon-notice.baseline svg {
        top: .125em;
        position: relative;
    }
&lt;/style&gt;&lt;div class=&#34;notice proof&#34; &gt;
    &lt;p class=&#34;notice-title&#34;&gt;
        &lt;span class=&#34;icon-notice baseline&#34;&gt;
            
        &lt;/span&gt;proof&lt;/p&gt;</description>
    </item>
    <item>
      <title>自定义CUDA kernel加速Muon优化器</title>
      <link>https://nil9.net/posts/flash-muon/</link>
      <pubDate>Fri, 25 Apr 2025 00:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/flash-muon/</guid>
      <description>&lt;h1 id=&#34;tldr&#34;&gt;TL;DR&lt;/h1&gt;
&lt;p&gt;笔者通过自定义&lt;code&gt;$XX^\top$&lt;/code&gt;算子，加速Muon优化器中的核心操作——Newton-Schulz迭代，算子单测在8192维度上相比原生实现计算时间降低约一半，端到端的Newton-Schulz迭代运行时间降低至原来的0.71倍。相关代码已经发布在：&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/nil0x9/flash-muon&#34;&gt;https://github.com/nil0x9/flash-muon&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;读者可以通过如下方式来试用优化版本的Muon实现或者核心算子。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git clone --recurse-submodules https://github.com/nil0x9/flash-muon.git
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pip install -e ./
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;具体做法&#34;&gt;具体做法&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://kellerjordan.github.io/posts/muon/&#34;&gt;Muon优化器&lt;/a&gt;的核心机制是通过Newton-Schulz迭代法来代替SVD分解，实现GPU友好的&lt;code&gt;$\boldsymbol{U}\boldsymbol{V}^T$&lt;/code&gt;计算（之前笔者写过&lt;a href=&#34;https://zhuanlan.zhihu.com/p/28236539668&#34;&gt;这篇blog&lt;/a&gt;介绍Muon背后的数学原理）。&lt;/p&gt;
&lt;p&gt;Newton-Schulz迭代法的核心公式如下所示，通过指定合适的多项式系数&lt;code&gt;$a,b,c$&lt;/code&gt;，将SVD分解的 &lt;code&gt;$\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^T$&lt;/code&gt;中的&lt;code&gt;$\boldsymbol{\Sigma}$&lt;/code&gt;在迭代中消除
&lt;code&gt;$$ \boldsymbol{X}\leftarrow a\boldsymbol{X} + b(\boldsymbol{X}\boldsymbol{X}^\top)\boldsymbol{X} + c(\boldsymbol{X}\boldsymbol{X}^\top)(\boldsymbol{X}\boldsymbol{X}^\top)\boldsymbol{X} $$&lt;/code&gt;
其中&lt;code&gt;$\boldsymbol{X}$&lt;/code&gt;是梯度矩阵或梯度的动量矩阵。在&lt;a href=&#34;https://github.com/KellerJordan/Muon/tree/master&#34;&gt;Jordan的实现&lt;/a&gt; 中这个迭代过程由如下的PyTorch代码实现：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _ &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(steps):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    A &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; X &lt;span style=&#34;color:#f92672&#34;&gt;@&lt;/span&gt; X&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;T
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    B &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; b &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; A &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; A &lt;span style=&#34;color:#f92672&#34;&gt;@&lt;/span&gt; A
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    X &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; a &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; X &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; B &lt;span style=&#34;color:#f92672&#34;&gt;@&lt;/span&gt; X
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意到这里提前计算了&lt;code&gt;$\boldsymbol{A}=\boldsymbol{X}\boldsymbol{X}^\top$&lt;/code&gt;来减少重复计算。由于&lt;code&gt;$\boldsymbol{A}$&lt;/code&gt;是一个对称矩阵，则循环内第二行的&lt;code&gt;$\boldsymbol{AA} = \boldsymbol{AA}^\top$&lt;/code&gt;。这里我们可以看到一个被使用了两次的运算&lt;code&gt;lambda x: matmul(x, x.T)&lt;/code&gt;，如果这个算子可以有比&lt;code&gt;matmul&lt;/code&gt;更高效的实现，就可以实现更快的Newton-Schulz迭代。&lt;/p&gt;
&lt;p&gt;幸运的是&lt;code&gt;$\boldsymbol{X}\boldsymbol{X}^\top$&lt;/code&gt;是一个对称矩阵，理论上我们可以只计算上三角部分，下三角部分可以直接复用下三角部分的结果。&lt;/p&gt;
&lt;p&gt;这一点在Laker Newhouse的这篇&lt;a href=&#34;https://www.lakernewhouse.com/assets/writing/faster-symmul-with-thunderkittens.pdf&#34;&gt;文章&lt;/a&gt;中有提及，但是可惜的是，他们并没有实现一个可用的kernel版本。&lt;/p&gt;
&lt;p&gt;笔者沿着这个思路实现了一个可用的CUDA kernel。它的设计思路其实非常直观（也可能比较clumsy，欢迎批评指正）——在一般的GEMM算子中，每个线程会从全局内存load计算一定分块的矩阵算元到自己的寄存器内，在计算完对应分块的结果矩阵后，将这部分的结果store到输出算元对应的全局内存空间。GEMM的相关分析教程实在是太多了，笔者就不赘述了。——而对于&lt;code&gt;$\boldsymbol{X}\boldsymbol{X}^\top$&lt;/code&gt;这种对称运算，笔者的让下三角对应的线程块在kernel一开始直接退出，&lt;!-- raw HTML omitted --&gt;上三角部分的线程在计算结束后做完一般的store操作后，根据线程和线程块的id，找到对应的下三角部分的全局内存空间，将当前线程对应的结果分块转置后store到下三角的分块&lt;!-- raw HTML omitted --&gt;。
用图像表达如下：&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  &lt;img alt=&#34;design of matmul_transpose kernel&#34; loading=&#34;lazy&#34; src=&#34;https://nil9.net/images/flash-muon/matmul_transpose_kernel.png&#34; title=&#34;matmul_transpose的kernel设计&#34;&gt;&lt;figcaption class=&#34;image-caption&#34;&gt;matmul_transpose的kernel设计&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>谱条件：如何衡量神经网络参数空间中的距离？</title>
      <link>https://nil9.net/posts/spectral-condition/</link>
      <pubDate>Mon, 31 Mar 2025 00:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/spectral-condition/</guid>
      <description>&lt;h1 id=&#34;prologue&#34;&gt;Prologue&lt;/h1&gt;
&lt;p&gt;如何衡量神经网络参数空间中的距离（i.e., 选取合适的范数）？这个问题我们之前已经在&lt;a href=&#34;https://nil9.net/posts/gd-norm-const/&#34;&gt;这篇文章&lt;/a&gt;中有所涉及，本文是更完整的拓展。&lt;/p&gt;
&lt;p&gt;首先应该明确为什么这个问题很重要？因为在梯度下降这样的优化算法框架下，总是依赖目标参数的某种距离的衡量。例如，当目标参数可以对应到欧几里得空间的坐标时，就可以得到局部目标函数下降最快的方向是梯度负方向。&lt;/p&gt;
&lt;p&gt;尽管在深度学习优化中，直接使用欧几里得范数（衡量参数空间的距离）非常吸引人，并且实际上也有效。但是，&lt;em&gt;&lt;strong&gt;这种做法会丢失结构信息&lt;/strong&gt;&lt;/em&gt;，因为它实质上是将参数看做一个扁平的向量。那么得到的优化方向就可能是相对低效的。&lt;/p&gt;
&lt;p&gt;对于常见的神经网络而言，对参数矩阵使用谱范数似乎是个更好的选择[1][2]。使用谱范数的相关约束，可以引出在现在的大规模模型训练中非常重要的两个工作：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;$\mu P$&lt;/code&gt; (Maximal Update Parameterization)：使用特定的初始化与学习率的参数化，可以做到在小模型上调节部分超参数，迁移到同构的大模型。&lt;/li&gt;
&lt;li&gt;Muon优化器的更新规则，即对梯度矩阵做SVD分解，消除奇异值矩阵后将&lt;code&gt;$-\boldsymbol{U}\boldsymbol{V}^\top$&lt;/code&gt;作为对应参数的更新方向。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;从而涵盖了给定神经网络结构下的参数初始化、参数更新的步长和方向三个方面（模型成功训练的三个基本要素）。&lt;/p&gt;
&lt;h1 id=&#34;feature-learning的谱条件&#34;&gt;feature learning的谱条件&lt;/h1&gt;
&lt;p&gt;为什么需要使用谱范数来衡量参数的大小和参数更新步长呢？在Greg Yang的系列工作中，考虑的主要是feature learning的问题。Yang在他的TP4中发现，在标准参数化或者NTK参数化的设定下，无穷宽的神经网络没有办法学习特征（一次更新后特征与初始化状态无异），这就与传统的linear transfer learning的智慧形成了悖论，因为如果没有feature learning，那么预训练就对后续的transfer没有任何增益。&lt;/p&gt;
&lt;p&gt;于是为了达成非平凡的feature learning，就需要对神经网络的特征&lt;code&gt;$\boldsymbol{h}_\ell(\boldsymbol{x})\in\mathbb{R}^{n_\ell}$&lt;/code&gt;的范数与一步更新的特征范数变化量&lt;code&gt;$\boldsymbol{h}_\ell(\boldsymbol{x})$&lt;/code&gt;做如下的约束[1]：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ \|\boldsymbol{h}_\ell\|_2=\Theta(\sqrt{n_\ell})\text{ and } \|\Delta\boldsymbol{h}_\ell\|_2=\Theta(\sqrt{n_\ell}), \text{ for }\ell=1,\ldots,L-1\tag{1} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这个条件意味着，对于中间的任何特征向量而言，每个元素平均下来的范数是常数阶的，一次更新后的变化量也是常数阶的（不会随着宽度&lt;code&gt;$n_\ell$&lt;/code&gt;趋近无穷而爆炸或者弥散）。&lt;/p&gt;
&lt;p&gt;对于常见的由一系列矩阵参数构成的神经网络而言，实现这样的约束，需要对参数矩阵做如下的谱条件约束：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ \|\boldsymbol{W}_\ell\|_2=\Theta\left(\sqrt{\frac{n_\ell}{n_{\ell-1}}}\right)\text{ and } \|\Delta\boldsymbol{W}_\ell\|_2=\Theta\left(\sqrt{\frac{n_\ell}{n_{\ell-1}}}\right), \text{ for }\ell=1,\ldots,L-1\tag{2} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这里的&lt;code&gt;$\|\cdot\|_2$&lt;/code&gt;是矩阵的谱范数，即从向量&lt;code&gt;$\ell_2$&lt;/code&gt;空间映射到向量&lt;code&gt;$\ell_2$&lt;/code&gt;空间的算子诱导范数&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ \|\boldsymbol{A}\|_2=\underset{\boldsymbol{x}\in\mathbb{R}^n}{\max} \frac{\|\boldsymbol{A}\boldsymbol{x}\|_2}{\|\boldsymbol{x}\|_2}\text{ for } \boldsymbol{A}\in\mathbb{R}^{m\times n} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这个谱条件能够保证公式&lt;code&gt;$(1)$&lt;/code&gt;成立，证明思路主要利用到谱范数的性质&lt;code&gt;$\|\boldsymbol{Av}\|_2\le\|\boldsymbol{A}\|_2\|\boldsymbol{v}\|_2$&lt;/code&gt;，证明出上述谱条件诱导的&lt;code&gt;$\|\boldsymbol{h}_\ell\|_2,\|\Delta\boldsymbol{h}_\ell\|_2$&lt;/code&gt;的上界均为&lt;code&gt;$\Theta(\sqrt{n_\ell})$&lt;/code&gt;，接着证明这个上界依概率总是取得。具体可以看&lt;a href=&#34;http://arxiv.org/abs/2310.17813&#34;&gt;这篇论文&lt;/a&gt;的第三节（相比Greg的TP系列，还算是很容易理解的）。&lt;/p&gt;
&lt;p&gt;公式&lt;code&gt;$(2)$&lt;/code&gt;中的第二个条件是很直观的，对于SGD优化器而言，直接按照每个参数矩阵的维度设定对应的学习率即可&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ \eta_\ell = \Theta\left(\sqrt{\frac{n_\ell}{n_{\ell-1}}}\right) $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;对于第一个条件&lt;code&gt;$\boldsymbol{W}_\ell = \Theta\left(\sqrt{\frac{n_\ell}{n_{\ell-1}}}\right)$&lt;/code&gt;，需要修改初始化的方式——我们可以从一个标准正态分布中i.i.d.采样一个权重矩阵&lt;code&gt;$\boldsymbol{W}&#39;\in\mathbb{R}^{n_\ell\times n_{\ell-1}}$&lt;/code&gt;，接着做缩放&lt;code&gt;$\boldsymbol{W} = \sigma\boldsymbol{W}&#39;$&lt;/code&gt;（这里略去下标&lt;code&gt;$\ell$&lt;/code&gt;）得到最终的初始化参数矩阵。&lt;/p&gt;
&lt;p&gt;这个&lt;code&gt;$\sigma$&lt;/code&gt;怎么确定呢？由于矩阵的stable rank按定义可以将矩阵的谱范数和Frobenius范数联系起来：
&lt;code&gt;$$ \text{stable-rank}(\boldsymbol{W}) = \frac{\|\boldsymbol{W}\|_F^2}{\|\boldsymbol{W}\|_2^2} $$&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>简记：Muon中设计Newton-Schulz迭代的系数？</title>
      <link>https://nil9.net/posts/newton-schulz-4muon/</link>
      <pubDate>Sat, 08 Mar 2025 00:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/newton-schulz-4muon/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://nil9.net/posts/gd-norm-const/&#34;&gt;上篇文章&lt;/a&gt;介绍了Muon等新兴深度学习优化器背后的原理，即约束参数矩阵的诱导范数下得到新的更新方向。&lt;/p&gt;
&lt;p&gt;在Muon对参数更新方向&lt;code&gt;$-\boldsymbol{U}\boldsymbol{V}^\top$&lt;/code&gt;的计算中用到了Newton-Schulz迭代方法，本质上是在寻找这样一个多项式函数
&lt;code&gt;$$ f(x)=ax+bx^3+cx^5+\ldots $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;使其满足对任意&lt;code&gt;$x\in(0, 1]$&lt;/code&gt;，对&lt;code&gt;$x$&lt;/code&gt;应用多次&lt;code&gt;$f(\cdot)$&lt;/code&gt;，都能收敛到1附近。这里我们尝试设计一个能work的参数组合。&lt;/p&gt;
&lt;p&gt;我的一个简单的想法是，设计一个多项式函数，使&lt;code&gt;$x=1$&lt;/code&gt;是它的一个&lt;strong&gt;吸引不动点&lt;/strong&gt;：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;定义1&lt;/strong&gt;（&lt;em&gt;不动点&lt;/em&gt;）当&lt;code&gt;$x_0$&lt;/code&gt;被函数&lt;code&gt;$f(\cdot)$&lt;/code&gt;映射到自身，即&lt;code&gt;$f(x_0)=x_0$&lt;/code&gt;时，称&lt;code&gt;$x_0$&lt;/code&gt;是函数&lt;code&gt;$f(\cdot)$&lt;/code&gt;的一个不动点。&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;定义2&lt;/strong&gt;（&lt;em&gt;吸引不动点&lt;/em&gt;）&lt;code&gt;$f$&lt;/code&gt;的吸引不动点是&lt;code&gt;$f$&lt;/code&gt;的不动点&lt;code&gt;$x_0$&lt;/code&gt;使得，对在足够接近&lt;code&gt;$x_0$&lt;/code&gt;的定义域中的任何&lt;code&gt;$x$&lt;/code&gt;值而言，迭代函数序列&lt;code&gt;$x,f(x),f(f(x)),f(f(f(x))),\ldots$&lt;/code&gt;收敛于&lt;code&gt;$x_0$&lt;/code&gt;。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;要令&lt;code&gt;$x=1$&lt;/code&gt;是&lt;code&gt;$f(x)$&lt;/code&gt;的一个吸引不动点，要满足如下的必要条件：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;$f(1)=1$&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$|f&#39;(1)|&amp;lt;1$&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;使用这两个条件是无法确定具体的参数值&lt;code&gt;$a,b,\ldots$&lt;/code&gt;的，但是对于三阶（参数包括&lt;code&gt;$a,b$&lt;/code&gt;两个）或者五阶（参数包括&lt;code&gt;$a,b,c$&lt;/code&gt;三个）的Newton-Schulz迭代，可以大大缩小搜索的空间。下面展开看下。&lt;/p&gt;
&lt;h1 id=&#34;三阶迭代&#34;&gt;三阶迭代&lt;/h1&gt;
&lt;p&gt;先讨论三阶迭代的形式&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ f(x)=ax+bx^3 $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;代入上面的两个必要条件：
&lt;code&gt;$$ \begin{split} f(1)=a+b = 1\\ -1 &amp;lt; f&#39;(1)=a+3b &amp;lt; 1\\ \end{split} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;根据第一个条件，可以把&lt;code&gt;$b$&lt;/code&gt;用&lt;code&gt;$1-a$&lt;/code&gt;重参数化，然后就有可行的条件
&lt;code&gt;$$ 1 &amp;lt; a &amp;lt; 2 $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;我们记五次迭代后的函数&lt;code&gt;$\phi(x)=f(f(f(f(f(x)))))$&lt;/code&gt;，可视化看一下不同&lt;code&gt;$a$&lt;/code&gt;取值下对应的情况（理想情况下，对于&lt;code&gt;$(0,1]$&lt;/code&gt;区间内的&lt;code&gt;$x$&lt;/code&gt;，曲线要尽可能接近&lt;code&gt;$y=1$&lt;/code&gt;）&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  &lt;img alt=&#34;三阶迭代下，a取不同取值时对应的φ(x)&#34; loading=&#34;lazy&#34; src=&#34;https://nil9.net/images/newton-schulz-4muon/ns3.gif&#34; title=&#34;三阶迭代下，a取不同取值时对应的φ(x)&#34;&gt;&lt;figcaption class=&#34;image-caption&#34;&gt;三阶迭代下，a取不同取值时对应的φ(x)&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;注意到在&lt;code&gt;$a$&lt;/code&gt;接近1的时候，&lt;code&gt;$\phi(x)$&lt;/code&gt;收敛到1附近的邻域是比较窄的，随着&lt;code&gt;$a\to 2$&lt;/code&gt;，收敛到1附近的「邻域」范围逐渐拓宽，但在&lt;code&gt;$a=2$&lt;/code&gt;附近，曲线开始出现一定的抖动。对于优化器而言，这样的局部近似的方差是可以容忍的，因此我们可以选取一个比较接近2的值作为&lt;code&gt;$a$&lt;/code&gt;的参数，例如&lt;code&gt;$a=1.99,b=-0.99$&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;作为对比，在&lt;a href=&#34;https://arxiv.org/pdf/2410.21265&#34;&gt;Bernstein &amp;amp; Newhouse 2024.&lt;/a&gt;中，作者给出的参数是&lt;code&gt;$a=3/2,b=-1/2$&lt;/code&gt;。可以在下图中对照两种设定下的&lt;code&gt;$\phi(x)$&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  &lt;img alt=&#34;两种φ(x)对比&#34; loading=&#34;lazy&#34; src=&#34;https://nil9.net/images/newton-schulz-4muon/phi_curve_ns3.png&#34; title=&#34;两种φ(x)对比&#34;&gt;&lt;figcaption class=&#34;image-caption&#34;&gt;两种φ(x)对比&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;可以看到Bernstein给出的参数虽然更平滑地收敛于1，但是对于在0附近的初始&lt;code&gt;$x$&lt;/code&gt;，普遍无法收敛到1。也就是说对于较小的奇异值对应的&lt;code&gt;$\boldsymbol{u}_i, \boldsymbol{v}_i$&lt;/code&gt;，倾向于在更新中被忽略。&lt;/p&gt;
&lt;p&gt;在&lt;code&gt;$x=0$&lt;/code&gt;附近&lt;code&gt;$\phi(x)$&lt;/code&gt;能否快速接近1，主要取决于参数&lt;code&gt;$a$&lt;/code&gt;的大小，这是因为&lt;code&gt;$\phi&#39;(0)=a^5$&lt;/code&gt;。所以应该在尽可能保证&lt;code&gt;$\phi(x)\approx 1,\forall x\in(0,1]$&lt;/code&gt;的同时，让&lt;code&gt;$a$&lt;/code&gt;尽可能大。&lt;/p&gt;
&lt;h1 id=&#34;五阶迭代&#34;&gt;五阶迭代&lt;/h1&gt;
&lt;p&gt;现在来考虑五阶迭代的形式&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ f(x)=ax+bx^3+cx^5 $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;代入上面的两个必要条件：
&lt;code&gt;$$ \begin{split} f(1)=a+b+c = 1\\ -1 &amp;lt; f&#39;(1)=a+3b+5c &amp;lt; 1\\ \end{split} $$&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>从约束视角看深度学习优化若干新进展</title>
      <link>https://nil9.net/posts/gd-norm-const/</link>
      <pubDate>Wed, 05 Mar 2025 00:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/gd-norm-const/</guid>
      <description>&lt;p&gt;在深度学习中最常用的优化方法是梯度下降方法及其变体。在过去很长一段时间中，Adam优化器是NLP社区的默认选择，在ViT出现之后，CV方面的工作也逐渐开始使用Adam和Adam的变体（在ViT之前，一种常见的观点是Adam不适用于Vision任务）。&lt;/p&gt;
&lt;p&gt;最近Muon优化器在Kimi的新工作带动下又火了一把，相较于Adam优化器需要同时维护一阶、二阶矩估计量，Muon只需要维护一份梯度的动量估计，因此在大规模训练中有很大优势。最近笔者顺着Muon的reference看了Jeremy Bernstein在优化的一些文章，觉得很有意思，因此写这篇文章梳理一下这一系列工作的部分精要。本文的核心论点是：使用诱导范数来约束梯度更新，可以推导出最近的一些新出现的优化方法，这也可能是未来深度学习优化的一个有潜力的探索方向。&lt;/p&gt;
&lt;h1 id=&#34;梯度下降recap&#34;&gt;梯度下降（Recap）&lt;/h1&gt;
&lt;p&gt;当前深度学习优化算法的基石是梯度下降。之前笔者写过一篇拙文（&lt;a href=&#34;https://nil9.net/posts/natural-gradient-descent/&#34;&gt;自然梯度（二）：黎曼距离下的最速下降&lt;/a&gt;）整理过梯度下降的推导，核心的结论是：当我们假设参数空间是一个欧几里得空间、参数的距离可以用欧几里得距离来衡量时，我们在某个点约束&lt;code&gt;$\|\Delta\theta\|_2\le\epsilon(\epsilon&amp;gt;0)$&lt;/code&gt;时，&lt;code&gt;$\Delta\theta$&lt;/code&gt;取梯度的反方向时可以让目标函数下降最多（具体的证明请参阅上述引文）。&lt;/p&gt;
&lt;p&gt;使用梯度下降最大的问题是，它实际上&lt;strong&gt;忽略了模型的结构&lt;/strong&gt;。换句话说，梯度下降相当于将模型所有参数展平为1维向量，并且用向量2范数来衡量每次更新的「步长」。这种抽象是实用的，但是也存在一定的问题。两组参数有可能在欧几里得空间中距离很近，但是诱导的模型输出空间距离很远。造成的结果就是更新的方向实际上不是目标函数下降最快的方向。&lt;/p&gt;
&lt;p&gt;这个问题要如何解决呢？在&lt;a href=&#34;https://nil9.net/posts/natural-gradient-descent/&#34;&gt;自然梯度（二）：黎曼距离下的最速下降&lt;/a&gt;中，我们介绍了自然梯度方法，即使用Fisher信息矩阵的逆作为梯度的pre-conditioner来矫正梯度下降的方向，从原理上是使用参数更新前后引导的概率分布的KL散度作为每次更新的步长约束。但是对于常见的深度神经网络来说，这样做仍然是不切实际的，因为FIM是一个&lt;code&gt;$N\times N$&lt;/code&gt;的大矩阵（其中&lt;code&gt;$N$&lt;/code&gt;是参数量），对于这么大的矩阵存储或求逆都是很难做到的。&lt;/p&gt;
&lt;h1 id=&#34;诱导范数作为步长约束&#34;&gt;诱导范数作为步长约束&lt;/h1&gt;
&lt;p&gt;是否有一种更「廉价」的方法，可以考虑&lt;strong&gt;模型的参数结构&lt;/strong&gt;，同时将&lt;strong&gt;参数的变化对于输出的影响&lt;/strong&gt;作为约束呢？&lt;/p&gt;
&lt;p&gt;幸运的是，对于当下最流行的神经网络（e.g., Transformer）而言，模型往往可以拆解为很多小模块，其中最常见的是Linear模块（线性映射，这里忽略bias term）&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ f(\boldsymbol{x};\boldsymbol{W})=\boldsymbol{Wx},\ \boldsymbol{W}\in\mathbb{R}^{n\times m},\boldsymbol{x}\in \mathbb{R}^{m}\\ $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;在标准的Transformer中，Attention、FFN、LM分类器都是由Linear模块组成的，Embedding从数学原理上也是输入为one-hot encoding的线性映射。假设现在对于某个Linear模块的参数&lt;code&gt;$\boldsymbol{W}$&lt;/code&gt;做&lt;code&gt;$\Delta\boldsymbol{W}$&lt;/code&gt;的更新（&lt;code&gt;$\boldsymbol{W}&#39;\leftarrow \boldsymbol{W}+\Delta\boldsymbol{W}$&lt;/code&gt;），我们需要衡量这个更新对于最终输出的影响是多少（从而可以约束这个影响）。由于神经网络比较复杂，衡量&lt;code&gt;$\Delta\boldsymbol{W}$&lt;/code&gt;对于最终目标函数的影响是相对繁琐的，但我们可以退而求其次，衡量&lt;code&gt;$\Delta\boldsymbol{W}$&lt;/code&gt;对于这个Linear模块的输出&lt;code&gt;$\boldsymbol{Wx}$&lt;/code&gt;的影响。
考虑线性模块的输入与输出空间的距离都使用欧几里得范数&lt;code&gt;$\|\cdot\|_{\ell_2}$&lt;/code&gt;衡量，那么这个约束可以通过如下不等式实现&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ \|\Delta\boldsymbol{W}\boldsymbol{x}\|_{\ell_2} \le {\color[rgb]{0, 0.5, 0.8}{\|\Delta\boldsymbol{W}\|_{\ell_2\to\ell_2}}}\|\boldsymbol{x}\|_{\ell_2} $$&lt;/code&gt;
这里的&lt;code&gt;${\color[rgb]{0, 0.5, 0.8}{\|\Delta\boldsymbol{W}\|_{\ell_2\to\ell_2}}}$&lt;/code&gt;是矩阵2范数。这个不等式告诉我们，如果约束了参数更新量的谱范数（不等式右侧），也就约束了更新前后这个线性模块输出的变化量。&lt;/p&gt;
&lt;p&gt;假设现在需要优化的神经网络是由一系列的线性模块堆叠组成（e.g., MLP），我们可以参照梯度下降的推导构造如下的更新&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ \underset{\Delta\boldsymbol{W}_1,\ldots,\Delta\boldsymbol{W}_L}{\text{arg min}}\left[ \sum_{l=1}^L {\langle \boldsymbol{G}_l, \Delta\boldsymbol{W}_l \rangle}_F + \frac{\lambda}{2}\max_{l=1}^L{\color[rgb]{0, 0.5, 0.8}{\|\Delta\boldsymbol{W}_l\|^2_{\ell_2\to\ell_2}}} \right]\\ $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这里&lt;code&gt;$\boldsymbol{G}_l$&lt;/code&gt;表示&lt;code&gt;$\boldsymbol{W}_l$&lt;/code&gt;对应的梯度矩阵（布局与原参数矩阵相同），&lt;code&gt;${\langle \cdot, \cdot \rangle}_F$&lt;/code&gt;表示Frobenius内积（对矩阵而言，逐元素相乘求和）。这里之所以使用&lt;code&gt;$\max_{l=1}^L$&lt;/code&gt;（而不是直接求和），是因为我们引入这个约束时希望目标函数在&lt;code&gt;$\Delta\boldsymbol{W}_l$&lt;/code&gt;变化下，能够保持平滑的性质&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;，因此需要bound所有参数矩阵更新量的谱范数的最大值。&lt;/p&gt;
&lt;p&gt;我们来逐步推导这个最小值成立时的&lt;code&gt;$\Delta\boldsymbol{W}_1,\ldots,\Delta\boldsymbol{W}_L$&lt;/code&gt;取值&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;。为了方便，把每个&lt;code&gt;$\Delta\boldsymbol{W}_l$&lt;/code&gt;拆解成大小和方向两部分：&lt;code&gt;$\Delta\boldsymbol{W}_l=c_l\boldsymbol{T}_l(c_l\triangleq\|\Delta\boldsymbol{W}_l\|_{\ell_2\to\ell_2})$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;（为了可读性，下面的&lt;code&gt;$\|\cdots\|$&lt;/code&gt;均表示谱范数&lt;code&gt;$\|\cdot\|_{\ell_2\to\ell_2}$&lt;/code&gt;）&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ \begin{align} &amp;amp;\underset{\Delta\boldsymbol{W}_1,\ldots,\Delta\boldsymbol{W}_L}{\text{min}}\left[ \sum_{l=1}^L {\langle \boldsymbol{G}_l, \Delta\boldsymbol{W}_l \rangle}_F + \frac{\lambda}{2}\max_{l=1}^L\|\Delta\boldsymbol{W}_l\|^2 \right]\\ &amp;amp;=\underset{c_1,\ldots,c_L\ge 0}{\text{min}}\left[ \sum_{l=1}^L c_l\min_{\|\boldsymbol{T}_l\|=1}{\langle \boldsymbol{G}_l, \boldsymbol{T}_l \rangle}_F + \frac{\lambda}{2}\max_{l=1}^Lc_l^2 \right]\\ &amp;amp;=\underset{c_1,\ldots,c_L\ge 0}{\text{min}}\left[ -\sum_{l=1}^L c_l\max_{\|\boldsymbol{T}_l\|=1}{\langle \boldsymbol{G}_l, \boldsymbol{T}_l \rangle}_F + \frac{\lambda}{2}\max_{l=1}^Lc_l^2 \right]\\ &amp;amp;=\underset{c_1,\ldots,c_L\ge 0}{\text{min}}\left[ -\sum_{l=1}^L c_l \|\boldsymbol{G}_l\|_* + \frac{\lambda}{2}\max_{l=1}^Lc_l^2 \right]\quad\triangleright\|\cdot\|_*\text{表示核范数}\\ &amp;amp;=\underset{\eta\ge 0}{\text{min}}\left[ -\sum_{l=1}^L \eta\|\boldsymbol{G}_l\|_* + \frac{\lambda}{2}\max_{l=1}^L \eta^2 \right]\tag{1}\\ \end{align} $$&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>为什么LLM一般使用较大的权重衰减系数？</title>
      <link>https://nil9.net/posts/wd-model-precision/</link>
      <pubDate>Wed, 26 Feb 2025 00:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/wd-model-precision/</guid>
      <description>&lt;p&gt;最近在阅读&lt;a href=&#34;https://github.com/MoonshotAI/Moonlight/blob/master/Moonlight.pdf&#34;&gt;Muon is Scalable for LLM Training&lt;/a&gt;这篇文章的时候注意到他们使用无权重衰减（weight decay）版本的Muon优化LLM的时候，优化器的收敛优势会随着训练过程逐渐消失，又看到&lt;a href=&#34;https://www.zhihu.com/people/xiao-ming-tong-xue-86-81&#34;&gt;@小明同学&lt;/a&gt;在评论区提到的一个细节，很多开源的LLM在技术报告中都提到了使用0.1作为权重衰减的系数，觉得是个比较有意思的发现。结合Kimi的文章中关于bf16的简单陈述，笔者在本文中稍微展开讲下，权重衰减对于LLM的低精度训练中有什么作用。&lt;/p&gt;
&lt;p&gt;首先把结论放在前面：除了一般认知中的正则化作用，权重衰减也可能降低精度损失的风险——对于计算机的浮点数而言，&lt;strong&gt;绝对值越大，精度越低&lt;/strong&gt;。对于低精度/混合精度训练而言，使用权重衰减可以控制参数的绝对值范围，从而保证模型参数不落入低精度的数值区间。&lt;/p&gt;
&lt;h1 id=&#34;浮点数的存储与精度&#34;&gt;浮点数的存储与精度&lt;/h1&gt;
&lt;p&gt;上述结论主要与浮点数在计算机内的存储形式有关。学过计算机的一些基本课程的读者可能有印象，浮点数的存储是二进制的形式，分为符号位、指数位和尾数位三段。深度学习中常见的浮点数协议（fp32、fp16、bf16、tf32）的区别在于指数位和尾数位的比特数量不同。由于浮点数是一个「指数」的形式，&lt;strong&gt;因此它在实数空间的分布是不均匀的&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;这里我们考虑规范数的情形（指数位非全0），做一点分析。假设符号位、指数位、尾数位（mantissa）的二进制编码分别是&lt;code&gt;$S$&lt;/code&gt;、&lt;code&gt;$E$&lt;/code&gt;、&lt;code&gt;$M$&lt;/code&gt;，那么对应的浮点数为：
&lt;code&gt;$$ \text{value}=(-1)^S\times 1.M\times 2^{E-\text{bias}} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;在单精度fp32标准中，&lt;code&gt;$\text{bias}$&lt;/code&gt;取&lt;code&gt;${01111111}_{2}=127_{10}$&lt;/code&gt; .&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  &lt;img alt=&#34;fp32浮点数的一个例子&#34; loading=&#34;lazy&#34; src=&#34;https://nil9.net/images/wd-model-precision/ieee-fp32-example.png&#34; title=&#34;fp32浮点数的一个例子&#34;&gt;&lt;figcaption class=&#34;image-caption&#34;&gt;fp32浮点数的一个例子&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;例如在图中的例子中，&lt;code&gt;$S=0$&lt;/code&gt;，&lt;code&gt;$E=\underbrace{00\cdots0}_{7\ 0&#39;s}1$&lt;/code&gt;，&lt;code&gt;$M=\underbrace{00\cdots0}_{22\ 0&#39;s}1$&lt;/code&gt;，相应的值为
&lt;code&gt;$$ \begin{align} &amp;amp;{-1}^0\times 1.\underbrace{00\cdots0}_{22\ 0&#39;s}1_2\times 2^{00000001_2-{01111111}_{2}}\\ &amp;amp;\approx [1.175494490952134\times 10^{-38}]_{10} \end{align} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;现在我们来考虑不同的数值范围内的浮点数精度。对于区间范围&lt;code&gt;$[2^{x}, 2^{x+1}],\forall -126\le x\le127$&lt;/code&gt;（这里的x已经是经过-bias之后得到的最终指数），我们希望在给定任意浮点数&lt;code&gt;$y\in[2^{x}, 2^{x+1}]$&lt;/code&gt;的基础上增加一个最小量&lt;code&gt;$\varepsilon$&lt;/code&gt;（即区间内两个浮点数的最小间隔），这个增加的过程是通过操纵二进制编码实现的，那么最小间隔只能是通过在&lt;code&gt;$y$&lt;/code&gt;的尾数部分加上&lt;/p&gt;
&lt;p&gt;&lt;code&gt;$$ 0.\underbrace{00\cdots0}_{22\ 0&#39;s}1 $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;来实现，对应的最小间隔是
&lt;code&gt;$$ \begin{align} \varepsilon &amp;amp;= 0.\underbrace{00\cdots0}_{22\text{ 0&#39;s}}1_2\times 2^{x}\\ &amp;amp;=2^{-23}\times2^{x}=2^{x-23}\\ \end{align} $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;如果你将上面的公式带入不同的指数位，可以验证与&lt;a href=&#34;https://en.wikipedia.org/wiki/IEEE_754-1985&#34;&gt;Wikipedia&lt;/a&gt;中给出的这张表的Gap是吻合的：&lt;/p&gt;
&lt;p&gt;&lt;figure&gt;
  &lt;img alt=&#34;不同指数位下的最小精度&#34; loading=&#34;lazy&#34; src=&#34;https://nil9.net/images/wd-model-precision/ieee-fp32-prec.png&#34; title=&#34;不同指数位下的最小精度，来源：Wikipedia&#34;&gt;&lt;figcaption class=&#34;image-caption&#34;&gt;不同指数位下的最小精度，来源：Wikipedia&lt;/figcaption&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;这个计算可以拓展到其他的精度格式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于半精度格式fp16而言，其尾数位有10位，对应的最小间隔是&lt;code&gt;$2^{x-10}$&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;对于半精度格式bf16而言，其尾数位有7位，对应的最小间隔是&lt;code&gt;$2^{x-7}$&lt;/code&gt;；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从这里也可以看到，bf16相比fp16虽然拓宽了表示范围，但是减少了精度（同样数值范围内的最小间隔更宽了）。&lt;/p&gt;
&lt;p&gt;从这里我们得到了一个结论：计算机存储的浮点数之间的最小间隔随着浮点数绝对值数值增加，指数级地增大，换言之，&lt;strong&gt;浮点数（绝对值）数值越大，精度越低&lt;/strong&gt;。并且这个问题对于fp16或bf16格式的浮点数，问题要更加显著。&lt;/p&gt;
&lt;p&gt;这个结论的另一个引申的问题是&lt;strong&gt;舍入误差&lt;/strong&gt;，假如一个较大的浮点数和一个较小的浮点数相加，由于浮点数的加法（减法过程相当于取补码后相加，结论是类似的）过程需要先将两个数的指数位对齐，因此绝对值较小的数字的尾数的最后几位数字可能会在加法中丢失。这里我们举一个极端的例子来说明。&lt;/p&gt;
&lt;p&gt;假设浮点数存储为fp32格式（8位指数、23位尾数）。
&lt;code&gt;$$ \begin{align} x=(-1)^0\times 1.0_2\times 2^{-1}\\ y=(-1)^0\times 1.0_2\times 2^{-25}\\ \end{align} $$&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tensor Product Attention (TPA) 导读</title>
      <link>https://nil9.net/posts/tensor-product-attention/</link>
      <pubDate>Thu, 13 Feb 2025 00:00:41 +0000</pubDate>
      <guid>https://nil9.net/posts/tensor-product-attention/</guid>
      <description>&lt;p&gt;最近Deepseek比较出圈，连带着里面用到的MLA也被讨论得更多了。MLA无疑是一个非常出色的attention改进，但是由于它的KV cache设计，不能很好地兼容&lt;a href=&#34;https://kexue.fm/archives/8265&#34;&gt;RoPE&lt;/a&gt;，因此作者们使用了decoupled RoPE这样的「补丁」来引入位置关系，这无疑也增加了实现的复杂度。&lt;/p&gt;
&lt;p&gt;最近，修改注意力KV Cache这一线工作又增添了&lt;a href=&#34;https://arxiv.org/pdf/2501.06425&#34;&gt;TPA&lt;/a&gt;这个新成员，笔者觉得这篇文章比较有趣，因此希望写一篇简短的导读。之所以叫「导读」，是因为本文不打算太深入文章的formulation和实验，而是从笔者自己的视角出发介绍文章的一些重点贡献。。&lt;/p&gt;
&lt;h1 id=&#34;mha的拆解&#34;&gt;MHA的拆解&lt;/h1&gt;
&lt;p&gt;最标准的多头注意力（Vaswani的版本），大致可以拆解成3个步骤（这里默认讨论自注意力）&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;：
&lt;code&gt;$$ \begin{aligned} \text{step 1 }&amp;amp; \begin{cases} \boldsymbol{q}_i^{(h)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(h)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(h)}\in\mathbb{R}^{d\times d_k} \\  \boldsymbol{k}_i^{(h)} = \boldsymbol{x}_i\boldsymbol{W}_k^{(h)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{(h)}\in\mathbb{R}^{d\times d_k}\\  \boldsymbol{v}_i^{(h)} = \boldsymbol{x}_i\boldsymbol{W}_v^{(h)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(h)}\in\mathbb{R}^{d\times d_v}  \end{cases} \\ \text{step 2 }&amp;amp; \begin{cases} \boldsymbol{o}_t^{(h)} = \text{Attention}\left(\boldsymbol{q}_t^{(h)}, \boldsymbol{k}_{\leq t}^{(h)}, \boldsymbol{v}_{\leq t}^{(h)}\right)\triangleq\frac{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(h)} \boldsymbol{k}_i^{(h)}{}^{\top}\right)\boldsymbol{v}_i^{(h)}}{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(h)} \boldsymbol{k}_i^{(h)}{}^{\top}\right)} \\  \end{cases} \\ \text{step 3 }&amp;amp; \begin{cases} \boldsymbol{o}_t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(H)}\right]  \end{cases} \\ \end{aligned}\\ $$&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这里&lt;code&gt;$\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_l$&lt;/code&gt;是输入向量，上标&lt;code&gt;$h\in \{1,\ldots,H\}$&lt;/code&gt;表示注意力头，&lt;code&gt;$d,d_k,d_v$&lt;/code&gt;分别表示输入维度、key维度、value维度。在第一步中，我们将每个token的表征独立地线性投射到不同的自空间上，第二步是标准的点积注意力，第三步是拼接（后续再做一步线性变换）。&lt;/p&gt;
&lt;p&gt;我们重点来审视一下第一个步骤，这里由于每个token的表征是独立操作的（类似FFN），因此我们可以将每个token的表征看做一个样本点，这里做的事情就是将一个&lt;code&gt;$d$&lt;/code&gt;维度的向量，转化成3个矩阵&lt;code&gt;$\tilde{\boldsymbol{Q}}\in\mathbb{R}^{H\times d_k},\tilde{\boldsymbol{K}}\in\mathbb{R}^{H\times d_k},\tilde{\boldsymbol{V}}\in\mathbb{R}^{H\times d_v}$&lt;/code&gt;，每一个头对应这个矩阵中的一行。接着我们逐行计算每行对应的三组向量的点积注意力，就构成了标准的多头注意力。在标准实现中，这个变换是通过参数矩阵的线性变换+ reshape实现的。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
