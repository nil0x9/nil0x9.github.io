<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>DL | nil9.net</title>
<meta name=keywords content><meta name=description content="nil9.net"><meta name=author content="Tianyang Lin"><link rel=canonical href=https://nil9.net/tags/dl/><link crossorigin=anonymous href=/assets/css/stylesheet.71aa4c4f8e8b273084883bf486ff4976c30116bece69acc9b3d1b1e97b7d13ee.css integrity="sha256-capMT46LJzCEiDv0hv9JdsMBFr7OaazJs9Gx6Xt9E+4=" rel="preload stylesheet" as=style><link rel=icon href=https://nil9.net/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nil9.net/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nil9.net/favicon-32x32.png><link rel=apple-touch-icon href=https://nil9.net/apple-touch-icon.png><link rel=mask-icon href=https://nil9.net/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=manifest href=https://nil9.net/site.webmanifest><link rel=alternate type=application/rss+xml href=https://nil9.net/tags/dl/index.xml><link rel=alternate hreflang=en href=https://nil9.net/tags/dl/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=//cdn.jsdelivr.net/npm/@xiee/utils/js/math-code.min.js defer></script><script defer src=https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js></script><script>function adjustMathJaxFontSize(){document.querySelectorAll(".mjx-svg, .mjx-chtml").forEach(function(e){const t=e.parentElement;let n=t.offsetWidth;t.classList.contains("isplay")&&(n=t.parentElement.offsetWidth);const s=e.offsetWidth;if(s>n){const t=n/s*100;e.style.fontSize=`${t}%`}})}MathJax.startup.promise.then(()=>{adjustMathJaxFontSize()}),window.addEventListener("resize",()=>{adjustMathJaxFontSize()})</script><script defer src=https://vercount.one/js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js></script><meta property="og:url" content="https://nil9.net/tags/dl/"><meta property="og:site_name" content="nil9.net"><meta property="og:title" content="DL"><meta property="og:description" content="nil9.net"><meta property="og:locale" content="zh-CN"><meta property="og:type" content="website"><meta property="og:image" content="https://nil9.net/images/android-chrome-512x512.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nil9.net/images/android-chrome-512x512.png"><meta name=twitter:title content="DL"><meta name=twitter:description content="nil9.net"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nil9.net/ accesskey=h title="nil9.net (Alt + H)">nil9.net</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nil9.net/archives/ title=archives><span>archives</span></a></li><li><a href=https://nil9.net/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://nil9.net/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://nil9.net/>Home</a>&nbsp;»&nbsp;<a href=https://nil9.net/tags/>Tags</a></div><h1>DL
<a href=/tags/dl/index.xml title=RSS aria-label=RSS><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>谱条件：如何衡量神经网络参数空间中的距离？</h2></header><div class=entry-content><p>Prologue 如何衡量神经网络参数空间中的距离（i.e., 选取合适的范数）？这个问题我们之前已经在这篇文章中有所涉及，本文是更完整的拓展。
首先应该明确为什么这个问题很重要？因为在梯度下降这样的优化算法框架下，总是依赖目标参数的某种距离的衡量。例如，当目标参数可以对应到欧几里得空间的坐标时，就可以得到局部目标函数下降最快的方向是梯度负方向。
尽管在深度学习优化中，直接使用欧几里得范数（衡量参数空间的距离）非常吸引人，并且实际上也有效。但是，这种做法会丢失结构信息，因为它实质上是将参数看做一个扁平的向量。那么得到的优化方向就可能是相对低效的。
对于常见的神经网络而言，对参数矩阵使用谱范数似乎是个更好的选择[1][2]。使用谱范数的相关约束，可以引出在现在的大规模模型训练中非常重要的两个工作：
$\mu P$ (Maximal Update Parameterization)：使用特定的初始化与学习率的参数化，可以做到在小模型上调节部分超参数，迁移到同构的大模型。 Muon优化器的更新规则，即对梯度矩阵做SVD分解，消除奇异值矩阵后将$-\boldsymbol{U}\boldsymbol{V}^\top$作为对应参数的更新方向。 从而涵盖了给定神经网络结构下的参数初始化、参数更新的步长和方向三个方面（模型成功训练的三个基本要素）。
feature learning的谱条件 为什么需要使用谱范数来衡量参数的大小和参数更新步长呢？在Greg Yang的系列工作中，考虑的主要是feature learning的问题。Yang在他的TP4中发现，在标准参数化或者NTK参数化的设定下，无穷宽的神经网络没有办法学习特征（一次更新后特征与初始化状态无异），这就与传统的linear transfer learning的智慧形成了悖论，因为如果没有feature learning，那么预训练就对后续的transfer没有任何增益。
于是为了达成非平凡的feature learning，就需要对神经网络的特征$\boldsymbol{h}_\ell(\boldsymbol{x})\in\mathbb{R}^{n_\ell}$的范数与一步更新的特征范数变化量$\boldsymbol{h}_\ell(\boldsymbol{x})$做如下的约束[1]：
$$ \|\boldsymbol{h}_\ell\|_2=\Theta(\sqrt{n_\ell})\text{ and } \|\Delta\boldsymbol{h}_\ell\|_2=\Theta(\sqrt{n_\ell}), \text{ for }\ell=1,\ldots,L-1\tag{1} $$
这个条件意味着，对于中间的任何特征向量而言，每个元素平均下来的范数是常数阶的，一次更新后的变化量也是常数阶的（不会随着宽度$n_\ell$趋近无穷而爆炸或者弥散）。
对于常见的由一系列矩阵参数构成的神经网络而言，实现这样的约束，需要对参数矩阵做如下的谱条件约束：
$$ \|\boldsymbol{W}_\ell\|_2=\Theta\left(\sqrt{\frac{n_\ell}{n_{\ell-1}}}\right)\text{ and } \|\Delta\boldsymbol{W}_\ell\|_2=\Theta\left(\sqrt{\frac{n_\ell}{n_{\ell-1}}}\right), \text{ for }\ell=1,\ldots,L-1 $$
这里的$\|\cdot\|_2$是矩阵的谱范数，即从向量$\ell_2$空间映射到向量$\ell_2$空间的算子诱导范数
$$ \|\boldsymbol{A}\|_2=\underset{\boldsymbol{x}\in\mathbb{R}^n}{\max} \frac{\|\boldsymbol{A}\boldsymbol{x}\|_2}{\|\boldsymbol{x}\|_2}\text{ for } \boldsymbol{A}\in\mathbb{R}^{m\times n} $$
这个谱条件能够保证公式$(1)$成立，证明思路主要利用到谱范数的性质$\|\boldsymbol{Av}\|_2\le\|\boldsymbol{A}\|_2\|\boldsymbol{v}\|_2$，证明出上述谱条件诱导的$\|\boldsymbol{h}_\ell\|_2,\|\Delta\boldsymbol{h}_\ell\|_2$的上界均为$\Theta(\sqrt{n_\ell})$，接着证明这个上界依概率总是取得。具体可以看这篇论文的第三节（相比Greg的TP系列，还算是很容易理解的）。
其中第二个条件是很直观的，对于SGD优化器而言，直接按照每个参数矩阵的维度设定对应的学习率即可
$$ \eta_\ell = \Theta\left(\sqrt{\frac{n_\ell}{n_{\ell-1}}}\right) $$
但是对于第一个条件$\boldsymbol{W}_\ell = \Theta\left(\sqrt{\frac{n_\ell}{n_{\ell-1}}}\right)$，需要修改初始化的方式，我们可以从一个标准正态分布中i.i.d.采样一个权重矩阵$\boldsymbol{W}'\in\mathbb{R}^{n_\ell\times n_{\ell-1}}$，接着做缩放$\boldsymbol{W} = \sigma\boldsymbol{W}'$（这里略去下标$\ell$）得到最终的初始化参数矩阵。
这个$\sigma$怎么确定呢？由于矩阵的stable rank按定义可以将矩阵的谱范数和Frobenius范数联系起来： $$ \text{stable-rank}(\boldsymbol{W}) = \frac{\|\boldsymbol{W}\|_F^2}{\|\boldsymbol{W}\|_2^2} $$
...</p></div><footer class=entry-footer><span title='2025-03-31 00:00:41 +0000 UTC'>2025-03-31</span>&nbsp;·&nbsp;Tianyang Lin</footer><a class=entry-link aria-label="post link to 谱条件：如何衡量神经网络参数空间中的距离？" href=https://nil9.net/posts/spectral-condition/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>简记：Muon中设计Newton-Schulz迭代的系数？</h2></header><div class=entry-content><p>上篇文章介绍了Muon等新兴深度学习优化器背后的原理，即约束参数矩阵的诱导范数下得到新的更新方向。
在Muon对参数更新方向$-\boldsymbol{U}\boldsymbol{V}^\top$的计算中用到了Newton-Schulz迭代方法，本质上是在寻找这样一个多项式函数 $$ f(x)=ax+bx^3+cx^5+\ldots $$
使其满足对任意$x\in(0, 1]$，对$x$应用多次$f(\cdot)$，都能收敛到1附近。这里我们尝试设计一个能work的参数组合。
我的一个简单的想法是，设计一个多项式函数，使$x=1$是它的一个吸引不动点：
定义1（不动点）当$x_0$被函数$f(\cdot)$映射到自身，即$f(x_0)=x_0$时，称$x_0$是函数$f(\cdot)$的一个不动点。
定义2（吸引不动点）$f$的吸引不动点是$f$的不动点$x_0$使得，对在足够接近$x_0$的定义域中的任何$x$值而言，迭代函数序列$x,f(x),f(f(x)),f(f(f(x))),\ldots$收敛于$x_0$。
要令$x=1$是$f(x)$的一个吸引不动点，要满足如下的必要条件：
$f(1)=1$ $|f'(1)|&lt;1$ 使用这两个条件是无法确定具体的参数值$a,b,\ldots$的，但是对于三阶（参数包括$a,b$两个）或者五阶（参数包括$a,b,c$三个）的Newton-Schulz迭代，可以大大缩小搜索的空间。下面展开看下。
三阶迭代 先讨论三阶迭代的形式
$$ f(x)=ax+bx^3 $$
代入上面的两个必要条件： $$ \begin{split} f(1)=a+b = 1\\ -1 &lt; f'(1)=a+3b &lt; 1\\ \end{split} $$
根据第一个条件，可以把$b$用$1-a$重参数化，然后就有可行的条件 $$ 1 &lt; a &lt; 2 $$
我们记五次迭代后的函数$\phi(x)=f(f(f(f(f(x)))))$，可视化看一下不同$a$取值下对应的情况（理想情况下，对于$(0,1]$区间内的$x$，曲线要尽可能接近$y=1$）
三阶迭代下，a取不同取值时对应的φ(x)
注意到在$a$接近1的时候，$\phi(x)$收敛到1附近的邻域是比较窄的，随着$a\to 2$，收敛到1附近的「邻域」范围逐渐拓宽，但在$a=2$附近，曲线开始出现一定的抖动。对于优化器而言，这样的局部近似的方差是可以容忍的，因此我们可以选取一个比较接近2的值作为$a$的参数，例如$a=1.99,b=-0.99$。
作为对比，在Bernstein & Newhouse 2024.中，作者给出的参数是$a=3/2,b=-1/2$。可以在下图中对照两种设定下的$\phi(x)$.
两种φ(x)对比
可以看到Bernstein给出的参数虽然更平滑地收敛于1，但是对于在0附近的初始$x$，普遍无法收敛到1。也就是说对于较小的奇异值对应的$\boldsymbol{u}_i, \boldsymbol{v}_i$，倾向于在更新中被忽略。
在$x=0$附近$\phi(x)$能否快速接近1，主要取决于参数$a$的大小，这是因为$\phi'(0)=a^5$。所以应该在尽可能保证$\phi(x)\approx 1,\forall x\in(0,1]$的同时，让$a$尽可能大。
五阶迭代 现在来考虑五阶迭代的形式
$$ f(x)=ax+bx^3+cx^5 $$
代入上面的两个必要条件： $$ \begin{split} f(1)=a+b+c = 1\\ -1 &lt; f'(1)=a+3b+5c &lt; 1\\ \end{split} $$
...</p></div><footer class=entry-footer><span title='2025-03-08 00:00:41 +0000 UTC'>2025-03-08</span>&nbsp;·&nbsp;Tianyang Lin</footer><a class=entry-link aria-label="post link to 简记：Muon中设计Newton-Schulz迭代的系数？" href=https://nil9.net/posts/newton-schulz-4muon/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>从约束视角看深度学习优化若干新进展</h2></header><div class=entry-content><p>在深度学习中最常用的优化方法是梯度下降方法及其变体。在过去很长一段时间中，Adam优化器是NLP社区的默认选择，在ViT出现之后，CV方面的工作也逐渐开始使用Adam和Adam的变体（在ViT之前，一种常见的观点是Adam不适用于Vision任务）。
最近Muon优化器在Kimi的新工作带动下又火了一把，相较于Adam优化器需要同时维护一阶、二阶矩估计量，Muon只需要维护一份梯度的动量估计，因此在大规模训练中有很大优势。最近笔者顺着Muon的reference看了Jeremy Bernstein在优化的一些文章，觉得很有意思，因此写这篇文章梳理一下这一系列工作的部分精要。本文的核心论点是：使用诱导范数来约束梯度更新，可以推导出最近的一些新出现的优化方法，这也可能是未来深度学习优化的一个有潜力的探索方向。
梯度下降（Recap） 当前深度学习优化算法的基石是梯度下降。之前笔者写过一篇拙文（自然梯度（二）：黎曼距离下的最速下降）整理过梯度下降的推导，核心的结论是：当我们假设参数空间是一个欧几里得空间、参数的距离可以用欧几里得距离来衡量时，我们在某个点约束$\|\Delta\theta\|_2\le\epsilon(\epsilon>0)$时，$\Delta\theta$取梯度的反方向时可以让目标函数下降最多（具体的证明请参阅上述引文）。
使用梯度下降最大的问题是，它实际上忽略了模型的结构。换句话说，梯度下降相当于将模型所有参数展平为1维向量，并且用向量2范数来衡量每次更新的「步长」。这种抽象是实用的，但是也存在一定的问题。两组参数有可能在欧几里得空间中距离很近，但是诱导的模型输出空间距离很远。造成的结果就是更新的方向实际上不是目标函数下降最快的方向。
这个问题要如何解决呢？在自然梯度（二）：黎曼距离下的最速下降中，我们介绍了自然梯度方法，即使用Fisher信息矩阵的逆作为梯度的pre-conditioner来矫正梯度下降的方向，从原理上是使用参数更新前后引导的概率分布的KL散度作为每次更新的步长约束。但是对于常见的深度神经网络来说，这样做仍然是不切实际的，因为FIM是一个$N\times N$的大矩阵（其中$N$是参数量），对于这么大的矩阵存储或求逆都是很难做到的。
诱导范数作为步长约束 是否有一种更「廉价」的方法，可以考虑模型的参数结构，同时将参数的变化对于输出的影响作为约束呢？
幸运的是，对于当下最流行的神经网络（e.g., Transformer）而言，模型往往可以拆解为很多小模块，其中最常见的是Linear模块（线性映射，这里忽略bias term）
$$ f(\boldsymbol{x};\boldsymbol{W})=\boldsymbol{Wx},\ \boldsymbol{W}\in\mathbb{R}^{n\times m},\boldsymbol{x}\in \mathbb{R}^{m}\\ $$
在标准的Transformer中，Attention、FFN、LM分类器都是由Linear模块组成的，Embedding从数学原理上也是输入为one-hot encoding的线性映射。假设现在对于某个Linear模块的参数$\boldsymbol{W}$做$\Delta\boldsymbol{W}$的更新（$\boldsymbol{W}'\leftarrow \boldsymbol{W}+\Delta\boldsymbol{W}$），我们需要衡量这个更新对于最终输出的影响是多少（从而可以约束这个影响）。由于神经网络比较复杂，衡量$\Delta\boldsymbol{W}$对于最终目标函数的影响是相对繁琐的，但我们可以退而求其次，衡量$\Delta\boldsymbol{W}$对于这个Linear模块的输出$\boldsymbol{Wx}$的影响。 考虑线性模块的输入与输出空间的距离都使用欧几里得范数$\|\cdot\|_{\ell_2}$衡量，那么这个约束可以通过如下不等式实现
$$ \|\Delta\boldsymbol{W}\boldsymbol{x}\|_{\ell_2} \le {\color[rgb]{0, 0.5, 0.8}{\|\Delta\boldsymbol{W}\|_{\ell_2\to\ell_2}}}\|\boldsymbol{x}\|_{\ell_2} $$ 这里的${\color[rgb]{0, 0.5, 0.8}{\|\Delta\boldsymbol{W}\|_{\ell_2\to\ell_2}}}$是矩阵2范数。这个不等式告诉我们，如果约束了参数更新量的谱范数（不等式右侧），也就约束了更新前后这个线性模块输出的变化量。
假设现在需要优化的神经网络是由一系列的线性模块堆叠组成（e.g., MLP），我们可以参照梯度下降的推导构造如下的更新1
$$ \underset{\Delta\boldsymbol{W}_1,\ldots,\Delta\boldsymbol{W}_L}{\text{arg min}}\left[ \sum_{l=1}^L {\langle \boldsymbol{G}_l, \Delta\boldsymbol{W}_l \rangle}_F + \frac{\lambda}{2}\max_{l=1}^L{\color[rgb]{0, 0.5, 0.8}{\|\Delta\boldsymbol{W}_l\|^2_{\ell_2\to\ell_2}}} \right]\\ $$
这里$\boldsymbol{G}_l$表示$\boldsymbol{W}_l$对应的梯度矩阵（布局与原参数矩阵相同），${\langle \cdot, \cdot \rangle}_F$表示Frobenius内积（对矩阵而言，逐元素相乘求和）。这里之所以使用$\max_{l=1}^L$（而不是直接求和），是因为我们引入这个约束时希望目标函数在$\Delta\boldsymbol{W}_l$变化下，能够保持平滑的性质2，因此需要bound所有参数矩阵更新量的谱范数的最大值。
我们来逐步推导这个最小值成立时的$\Delta\boldsymbol{W}_1,\ldots,\Delta\boldsymbol{W}_L$取值3。为了方便，把每个$\Delta\boldsymbol{W}_l$拆解成大小和方向两部分：$\Delta\boldsymbol{W}_l=c_l\boldsymbol{T}_l(c_l\triangleq\|\Delta\boldsymbol{W}_l\|_{\ell_2\to\ell_2})$
（为了可读性，下面的$\|\cdots\|$均表示谱范数$\|\cdot\|_{\ell_2\to\ell_2}$）
$$ \begin{align} &\underset{\Delta\boldsymbol{W}_1,\ldots,\Delta\boldsymbol{W}_L}{\text{min}}\left[ \sum_{l=1}^L {\langle \boldsymbol{G}_l, \Delta\boldsymbol{W}_l \rangle}_F + \frac{\lambda}{2}\max_{l=1}^L\|\Delta\boldsymbol{W}_l\|^2 \right]\\ &=\underset{c_1,\ldots,c_L\ge 0}{\text{min}}\left[ \sum_{l=1}^L c_l\min_{\|\boldsymbol{T}_l\|=1}{\langle \boldsymbol{G}_l, \boldsymbol{T}_l \rangle}_F + \frac{\lambda}{2}\max_{l=1}^Lc_l^2 \right]\\ &=\underset{c_1,\ldots,c_L\ge 0}{\text{min}}\left[ -\sum_{l=1}^L c_l\max_{\|\boldsymbol{T}_l\|=1}{\langle \boldsymbol{G}_l, \boldsymbol{T}_l \rangle}_F + \frac{\lambda}{2}\max_{l=1}^Lc_l^2 \right]\\ &=\underset{c_1,\ldots,c_L\ge 0}{\text{min}}\left[ -\sum_{l=1}^L c_l \|\boldsymbol{G}_l\|_* + \frac{\lambda}{2}\max_{l=1}^Lc_l^2 \right]\quad\triangleright\|\cdot\|_*\text{表示核范数}\\ &=\underset{\eta\ge 0}{\text{min}}\left[ -\sum_{l=1}^L \eta\|\boldsymbol{G}_l\|_* + \frac{\lambda}{2}\max_{l=1}^L \eta^2 \right]\tag{1}\\ \end{align} $$
...</p></div><footer class=entry-footer><span title='2025-03-05 00:00:41 +0000 UTC'>2025-03-05</span>&nbsp;·&nbsp;Tianyang Lin</footer><a class=entry-link aria-label="post link to 从约束视角看深度学习优化若干新进展" href=https://nil9.net/posts/gd-norm-const/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>为什么LLM一般使用较大的权重衰减系数？</h2></header><div class=entry-content><p>最近在阅读Muon is Scalable for LLM Training这篇文章的时候注意到他们使用无权重衰减（weight decay）版本的Muon优化LLM的时候，优化器的收敛优势会随着训练过程逐渐消失，又看到@小明同学在评论区提到的一个细节，很多开源的LLM在技术报告中都提到了使用0.1作为权重衰减的系数，觉得是个比较有意思的发现。结合Kimi的文章中关于bf16的简单陈述，笔者在本文中稍微展开讲下，权重衰减对于LLM的低精度训练中有什么作用。
首先把结论放在前面：除了一般认知中的正则化作用，权重衰减也可能降低精度损失的风险——对于计算机的浮点数而言，绝对值越大，精度越低。对于低精度/混合精度训练而言，使用权重衰减可以控制参数的绝对值范围，从而保证模型参数不落入低精度的数值区间。
浮点数的存储与精度 上述结论主要与浮点数在计算机内的存储形式有关。学过计算机的一些基本课程的读者可能有印象，浮点数的存储是二进制的形式，分为符号位、指数位和尾数位三段。深度学习中常见的浮点数协议（fp32、fp16、bf16、tf32）的区别在于指数位和尾数位的比特数量不同。由于浮点数是一个「指数」的形式，因此它在实数空间的分布是不均匀的。
这里我们考虑规范数的情形（指数位非全0），做一点分析。假设符号位、指数位、尾数位（mantissa）的二进制编码分别是$S$、$E$、$M$，那么对应的浮点数为： $$ \text{value}=(-1)^S\times 1.M\times 2^{E-\text{bias}} $$
在单精度fp32标准中，$\text{bias}$取${01111111}_{2}=127_{10}$ .
fp32浮点数的一个例子
例如在图中的例子中，$S=0$，$E=\underbrace{00\cdots0}_{7\ 0's}1$，$M=\underbrace{00\cdots0}_{22\ 0's}1$，相应的值为 $$ \begin{align} &{-1}^0\times 1.\underbrace{00\cdots0}_{22\ 0's}1_2\times 2^{00000001_2-{01111111}_{2}}\\ &\approx [1.175494490952134\times 10^{-38}]_{10} \end{align} $$
现在我们来考虑不同的数值范围内的浮点数精度。对于区间范围$[2^{x}, 2^{x+1}],\forall -126\le x\le127$（这里的x已经是经过-bias之后得到的最终指数），我们希望在给定任意浮点数$y\in[2^{x}, 2^{x+1}]$的基础上增加一个最小量$\varepsilon$（即区间内两个浮点数的最小间隔），这个增加的过程是通过操纵二进制编码实现的，那么最小间隔只能是通过在$y$的尾数部分加上
$$ 0.\underbrace{00\cdots0}_{22\ 0's}1 $$
来实现，对应的最小间隔是 $$ \begin{align} \varepsilon &= 0.\underbrace{00\cdots0}_{22\text{ 0's}}1_2\times 2^{x}\\ &=2^{-23}\times2^{x}=2^{x-23}\\ \end{align} $$
如果你将上面的公式带入不同的指数位，可以验证与Wikipedia中给出的这张表的Gap是吻合的：
不同指数位下的最小精度，来源：Wikipedia
这个计算可以拓展到其他的精度格式：
对于半精度格式fp16而言，其尾数位有10位，对应的最小间隔是$2^{x-10}$； 对于半精度格式bf16而言，其尾数位有7位，对应的最小间隔是$2^{x-7}$； 从这里也可以看到，bf16相比fp16虽然拓宽了表示范围，但是减少了精度（同样数值范围内的最小间隔更宽了）。
从这里我们得到了一个结论：计算机存储的浮点数之间的最小间隔随着浮点数绝对值数值增加，指数级地增大，换言之，浮点数（绝对值）数值越大，精度越低。并且这个问题对于fp16或bf16格式的浮点数，问题要更加显著。
这个结论的另一个引申的问题是舍入误差，假如一个较大的浮点数和一个较小的浮点数相加，由于浮点数的加法（减法过程相当于取补码后相加，结论是类似的）过程需要先将两个数的指数位对齐，因此绝对值较小的数字的尾数的最后几位数字可能会在加法中丢失。这里我们举一个极端的例子来说明。
假设浮点数存储为fp32格式（8位指数、23位尾数）。 $$ \begin{align} x=(-1)^0\times 1.0_2\times 2^{-1}\\ y=(-1)^0\times 1.0_2\times 2^{-25}\\ \end{align} $$
...</p></div><footer class=entry-footer><span title='2025-02-26 00:00:41 +0000 UTC'>2025-02-26</span>&nbsp;·&nbsp;Tianyang Lin</footer><a class=entry-link aria-label="post link to 为什么LLM一般使用较大的权重衰减系数？" href=https://nil9.net/posts/wd-model-precision/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Tensor Product Attention (TPA) 导读</h2></header><div class=entry-content><p>最近Deepseek比较出圈，连带着里面用到的MLA也被讨论得更多了。MLA无疑是一个非常出色的attention改进，但是由于它的KV cache设计，不能很好地兼容RoPE，因此作者们使用了decoupled RoPE这样的「补丁」来引入位置关系，这无疑也增加了实现的复杂度。
最近，修改注意力KV Cache这一线工作又增添了TPA这个新成员，笔者觉得这篇文章比较有趣，因此希望写一篇简短的导读。之所以叫「导读」，是因为本文不打算太深入文章的formulation和实验，而是从笔者自己的视角出发介绍文章的一些重点贡献。。
MHA的拆解 最标准的多头注意力（Vaswani的版本），大致可以拆解成3个步骤（这里默认讨论自注意力）1： $$ \begin{aligned} \text{step 1 }& \begin{cases} \boldsymbol{q}_i^{(h)} = \boldsymbol{x}_i\boldsymbol{W}_q^{(h)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_q^{(h)}\in\mathbb{R}^{d\times d_k} \\ \boldsymbol{k}_i^{(h)} = \boldsymbol{x}_i\boldsymbol{W}_k^{(h)}\in\mathbb{R}^{d_k},\quad \boldsymbol{W}_k^{(h)}\in\mathbb{R}^{d\times d_k}\\ \boldsymbol{v}_i^{(h)} = \boldsymbol{x}_i\boldsymbol{W}_v^{(h)}\in\mathbb{R}^{d_v},\quad \boldsymbol{W}_v^{(h)}\in\mathbb{R}^{d\times d_v} \end{cases} \\ \text{step 2 }& \begin{cases} \boldsymbol{o}_t^{(h)} = \text{Attention}\left(\boldsymbol{q}_t^{(h)}, \boldsymbol{k}_{\leq t}^{(h)}, \boldsymbol{v}_{\leq t}^{(h)}\right)\triangleq\frac{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(h)} \boldsymbol{k}_i^{(h)}{}^{\top}\right)\boldsymbol{v}_i^{(h)}}{\sum_{i\leq t}\exp\left(\boldsymbol{q}_t^{(h)} \boldsymbol{k}_i^{(h)}{}^{\top}\right)} \\ \end{cases} \\ \text{step 3 }& \begin{cases} \boldsymbol{o}_t = \left[\boldsymbol{o}_t^{(1)}, \boldsymbol{o}_t^{(2)}, \cdots, \boldsymbol{o}_t^{(H)}\right] \end{cases} \\ \end{aligned}\\ $$
这里$\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_l$是输入向量，上标$h\in \{1,\ldots,H\}$表示注意力头，$d,d_k,d_v$分别表示输入维度、key维度、value维度。在第一步中，我们将每个token的表征独立地线性投射到不同的自空间上，第二步是标准的点积注意力，第三步是拼接（后续再做一步线性变换）。
我们重点来审视一下第一个步骤，这里由于每个token的表征是独立操作的（类似FFN），因此我们可以将每个token的表征看做一个样本点，这里做的事情就是将一个$d$维度的向量，转化成3个矩阵$\tilde{\boldsymbol{Q}}\in\mathbb{R}^{H\times d_k},\tilde{\boldsymbol{K}}\in\mathbb{R}^{H\times d_k},\tilde{\boldsymbol{V}}\in\mathbb{R}^{H\times d_v}$，每一个头对应这个矩阵中的一行。接着我们逐行计算每行对应的三组向量的点积注意力，就构成了标准的多头注意力。在标准实现中，这个变换是通过参数矩阵的线性变换+ reshape实现的。
...</p></div><footer class=entry-footer><span title='2025-02-13 00:00:41 +0000 UTC'>2025-02-13</span>&nbsp;·&nbsp;Tianyang Lin</footer><a class=entry-link aria-label="post link to Tensor Product Attention (TPA) 导读" href=https://nil9.net/posts/tensor-product-attention/></a></article></main><footer class=footer><span>&copy; 2025 <a href=https://nil9.net/>nil9.net</a></span> ·
All rights reserved ·</footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>[...document.getElementsByTagName("code")].forEach(e=>{if(e.parentNode.tagName==="PRE"||e.childElementCount>0||e.classList.contains("nolatex"))return;let t=e.textContent;/^\$[^$]/.test(t)&&/[^$]\$$/.test(t)&&(t=t.replace(/^\$/,"\\(").replace(/\$$/,"\\)"),e.textContent=t),(/^\\\((.|\s)+\\\)$/.test(t)||/^\\\[(.|\s)+\\\]$/.test(t)||/^\$(.|\s)+\$$/.test(t)||/^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(t))&&(e.outerHTML=e.innerHTML)})</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("td .highlight-marker");e.forEach(e=>{const t=e.parentElement;e.remove(),t.classList.add("highlight")})})</script><script>hljs.highlightAll()</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>