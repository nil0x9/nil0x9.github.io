<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>自然梯度（一）：Fisher信息矩阵作为黎曼度量 | nil9.net</title>
<meta name=keywords content="Optimization"><meta name=description content="在一般的梯度下降中，我们认为目标函数梯度的负方向可以最小化一步更新后的目标函数值，这里隐含地假设了参数空间是欧氏空间，且参数构成了一组正交归一的坐标系统。在很多情况下，这一假设是不成立的，作为结果，优化过程的收敛效率可能受到影响。
作为解决这一问题的一种思路，自然梯度使用Fisher信息矩阵（的逆）作为梯度的pre-conditioner来矫正梯度的方向。本文将分为两篇，在第一篇中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在第二篇中，我们推导自然梯度中为何引入FIM来修正梯度方向，以及自然梯度的一些性质。
Score function与FIM
假设我们有一个由$\theta$参数化的概率模型，模型分布为$p(x|\theta)$，记对数似然函数为$\ell(\theta|x):=\log p(x|\theta)$。与对数似然函数相关的有两个定义，score function和fisher information。
定义1（score function）：score function $s(\theta|x)$被定义为对数似然函数关于参数$\theta$的梯度
$$ s(\theta|x)=\nabla_\theta \ell(\theta|x) $$
一些文章会提到score function是用来为参数的好坏打分（score），这是不严谨的。score function中的「score」其实不是为参数打分，而是在Fisher研究的遗传统计问题中给基因异常家庭的「打分」(参见：Interpretation of &ldquo;score&rdquo;)。因此，score function只是约定俗成的一种名称，其实质就是似然函数的梯度，描述的是似然函数对于参数变化的敏感程度。
性质1：Score function期望为0
$$ \mathbb{E}_{p(x|\theta)}[s(\theta|x)]=\boldsymbol{0} $$

    
        
            
        proof"><meta name=author content="Tianyang Lin"><link rel=canonical href=https://nil9.net/posts/fisher-info-matrix/><link crossorigin=anonymous href=/assets/css/stylesheet.b196917648943f010579b20d605de47b3a58da0ffc6d6aafefc10f04dcd48f27.css integrity="sha256-sZaRdkiUPwEFebINYF3kezpY2g/8bWqv78EPBNzUjyc=" rel="preload stylesheet" as=style><link rel=icon href=https://nil9.net/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nil9.net/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nil9.net/favicon-32x32.png><link rel=apple-touch-icon href=https://nil9.net/apple-touch-icon.png><link rel=mask-icon href=https://nil9.net/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=manifest href=https://nil9.net/site.webmanifest><link rel=alternate hreflang=en href=https://nil9.net/posts/fisher-info-matrix/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=//cdn.jsdelivr.net/npm/@xiee/utils/js/math-code.min.js defer></script><script defer src=https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js></script><script>function adjustMathJaxFontSize(){document.querySelectorAll(".mjx-svg, .mjx-chtml").forEach(function(e){const t=e.parentElement;let n=t.offsetWidth;t.classList.contains("isplay")&&(n=t.parentElement.offsetWidth);const s=e.offsetWidth;if(s>n){const t=n/s*100;e.style.fontSize=`${t}%`}})}MathJax.startup.promise.then(()=>{adjustMathJaxFontSize()}),window.addEventListener("resize",()=>{adjustMathJaxFontSize()})</script><script defer src=https://vercount.one/js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js></script><meta property="og:url" content="https://nil9.net/posts/fisher-info-matrix/"><meta property="og:site_name" content="nil9.net"><meta property="og:title" content="自然梯度（一）：Fisher信息矩阵作为黎曼度量"><meta property="og:description" content="在一般的梯度下降中，我们认为目标函数梯度的负方向可以最小化一步更新后的目标函数值，这里隐含地假设了参数空间是欧氏空间，且参数构成了一组正交归一的坐标系统。在很多情况下，这一假设是不成立的，作为结果，优化过程的收敛效率可能受到影响。
作为解决这一问题的一种思路，自然梯度使用Fisher信息矩阵（的逆）作为梯度的pre-conditioner来矫正梯度的方向。本文将分为两篇，在第一篇中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在第二篇中，我们推导自然梯度中为何引入FIM来修正梯度方向，以及自然梯度的一些性质。
Score function与FIM 假设我们有一个由$\theta$参数化的概率模型，模型分布为$p(x|\theta)$，记对数似然函数为$\ell(\theta|x):=\log p(x|\theta)$。与对数似然函数相关的有两个定义，score function和fisher information。
定义1（score function）：score function $s(\theta|x)$被定义为对数似然函数关于参数$\theta$的梯度
$$ s(\theta|x)=\nabla_\theta \ell(\theta|x) $$
一些文章会提到score function是用来为参数的好坏打分（score），这是不严谨的。score function中的「score」其实不是为参数打分，而是在Fisher研究的遗传统计问题中给基因异常家庭的「打分」(参见：Interpretation of “score”)。因此，score function只是约定俗成的一种名称，其实质就是似然函数的梯度，描述的是似然函数对于参数变化的敏感程度。
性质1：Score function期望为0 $$ \mathbb{E}_{p(x|\theta)}[s(\theta|x)]=\boldsymbol{0} $$
proof"><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-05T00:00:41+00:00"><meta property="article:modified_time" content="2025-02-05T00:00:41+00:00"><meta property="article:tag" content="Optimization"><meta property="og:image" content="https://nil9.net/images/android-chrome-512x512.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nil9.net/images/android-chrome-512x512.png"><meta name=twitter:title content="自然梯度（一）：Fisher信息矩阵作为黎曼度量"><meta name=twitter:description content="在一般的梯度下降中，我们认为目标函数梯度的负方向可以最小化一步更新后的目标函数值，这里隐含地假设了参数空间是欧氏空间，且参数构成了一组正交归一的坐标系统。在很多情况下，这一假设是不成立的，作为结果，优化过程的收敛效率可能受到影响。
作为解决这一问题的一种思路，自然梯度使用Fisher信息矩阵（的逆）作为梯度的pre-conditioner来矫正梯度的方向。本文将分为两篇，在第一篇中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在第二篇中，我们推导自然梯度中为何引入FIM来修正梯度方向，以及自然梯度的一些性质。
Score function与FIM
假设我们有一个由$\theta$参数化的概率模型，模型分布为$p(x|\theta)$，记对数似然函数为$\ell(\theta|x):=\log p(x|\theta)$。与对数似然函数相关的有两个定义，score function和fisher information。
定义1（score function）：score function $s(\theta|x)$被定义为对数似然函数关于参数$\theta$的梯度
$$ s(\theta|x)=\nabla_\theta \ell(\theta|x) $$
一些文章会提到score function是用来为参数的好坏打分（score），这是不严谨的。score function中的「score」其实不是为参数打分，而是在Fisher研究的遗传统计问题中给基因异常家庭的「打分」(参见：Interpretation of &ldquo;score&rdquo;)。因此，score function只是约定俗成的一种名称，其实质就是似然函数的梯度，描述的是似然函数对于参数变化的敏感程度。
性质1：Score function期望为0
$$ \mathbb{E}_{p(x|\theta)}[s(\theta|x)]=\boldsymbol{0} $$

    
        
            
        proof"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nil9.net/posts/"},{"@type":"ListItem","position":2,"name":"自然梯度（一）：Fisher信息矩阵作为黎曼度量","item":"https://nil9.net/posts/fisher-info-matrix/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"自然梯度（一）：Fisher信息矩阵作为黎曼度量","name":"自然梯度（一）：Fisher信息矩阵作为黎曼度量","description":"在一般的梯度下降中，我们认为目标函数梯度的负方向可以最小化一步更新后的目标函数值，这里隐含地假设了参数空间是欧氏空间，且参数构成了一组正交归一的坐标系统。在很多情况下，这一假设是不成立的，作为结果，优化过程的收敛效率可能受到影响。\n作为解决这一问题的一种思路，自然梯度使用Fisher信息矩阵（的逆）作为梯度的pre-conditioner来矫正梯度的方向。本文将分为两篇，在第一篇中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在第二篇中，我们推导自然梯度中为何引入FIM来修正梯度方向，以及自然梯度的一些性质。\nScore function与FIM 假设我们有一个由$\\theta$参数化的概率模型，模型分布为$p(x|\\theta)$，记对数似然函数为$\\ell(\\theta|x):=\\log p(x|\\theta)$。与对数似然函数相关的有两个定义，score function和fisher information。\n定义1（score function）：score function $s(\\theta|x)$被定义为对数似然函数关于参数$\\theta$的梯度\n$$ s(\\theta|x)=\\nabla_\\theta \\ell(\\theta|x) $$\n一些文章会提到score function是用来为参数的好坏打分（score），这是不严谨的。score function中的「score」其实不是为参数打分，而是在Fisher研究的遗传统计问题中给基因异常家庭的「打分」(参见：Interpretation of \u0026ldquo;score\u0026rdquo;)。因此，score function只是约定俗成的一种名称，其实质就是似然函数的梯度，描述的是似然函数对于参数变化的敏感程度。\n性质1：Score function期望为0 $$ \\mathbb{E}_{p(x|\\theta)}[s(\\theta|x)]=\\boldsymbol{0} $$\nproof\n","keywords":["Optimization"],"articleBody":"在一般的梯度下降中，我们认为目标函数梯度的负方向可以最小化一步更新后的目标函数值，这里隐含地假设了参数空间是欧氏空间，且参数构成了一组正交归一的坐标系统。在很多情况下，这一假设是不成立的，作为结果，优化过程的收敛效率可能受到影响。\n作为解决这一问题的一种思路，自然梯度使用Fisher信息矩阵（的逆）作为梯度的pre-conditioner来矫正梯度的方向。本文将分为两篇，在第一篇中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在第二篇中，我们推导自然梯度中为何引入FIM来修正梯度方向，以及自然梯度的一些性质。\nScore function与FIM 假设我们有一个由$\\theta$参数化的概率模型，模型分布为$p(x|\\theta)$，记对数似然函数为$\\ell(\\theta|x):=\\log p(x|\\theta)$。与对数似然函数相关的有两个定义，score function和fisher information。\n定义1（score function）：score function $s(\\theta|x)$被定义为对数似然函数关于参数$\\theta$的梯度\n$$ s(\\theta|x)=\\nabla_\\theta \\ell(\\theta|x) $$\n一些文章会提到score function是用来为参数的好坏打分（score），这是不严谨的。score function中的「score」其实不是为参数打分，而是在Fisher研究的遗传统计问题中给基因异常家庭的「打分」(参见：Interpretation of “score”)。因此，score function只是约定俗成的一种名称，其实质就是似然函数的梯度，描述的是似然函数对于参数变化的敏感程度。\n性质1：Score function期望为0 $$ \\mathbb{E}_{p(x|\\theta)}[s(\\theta|x)]=\\boldsymbol{0} $$\nproof\n$$ \\begin{align} \\mathbb{E}_{p(x|\\theta)}[s(\\theta|x)] \u0026=\\mathbb{E}_{p(x|\\theta)}[\\nabla_\\theta \\ell(\\theta|x)]\\\\ \u0026=\\int_x p(x|\\theta)\\nabla_\\theta \\log p(x|\\theta) dx\\\\ \u0026=\\int_x p(x|\\theta)\\frac{\\nabla_\\theta p(x|\\theta)}{p(x|\\theta)} dx\\\\ \u0026=\\int_x \\nabla_\\theta p(x|\\theta) dx\\\\ \u0026=\\nabla_\\theta\\int_x p(x|\\theta) dx\\\\ \u0026=\\nabla_\\theta 1 = \\boldsymbol{0}\\\\ \\end{align} $$\n通过性质1可以很顺利地引出Fisher信息矩阵的定义，由于score function是「零均值」的，因此协方差可以直接定义为score function的外积的期望。\n定义2（Fisher information matrix）：Fisher矩阵是score function的方差-协方差矩阵： $$ \\begin{align} \\boldsymbol{F}(\\theta) \u0026:= \\mathbb{E}_{p(x|\\theta)}\\left[ \\left(s(\\theta|x)-\\mathbb{E}_{p(x|\\theta)}[s(\\theta|x)]\\right)\\left(s(\\theta|x)-\\mathbb{E}_{p(x|\\theta)}[s(\\theta|x)]\\right)^\\top \\right]\\\\ \u0026=\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla_\\theta \\ell(\\theta|x)\\nabla_\\theta \\ell(\\theta|x)^\\top \\right]\\quad\\triangleright\\text{性质1} \\end{align} $$\nFisher信息的应用有很多，但我们主要关心它在优化方法中的应用。我们下面会证明，Fisher矩阵是分布$p(x|\\theta)$和$p(x|\\theta')$的KL散度的Hessian在$\\theta'=\\theta$处的取值。这点结论之所以重要，是因为这提供了衡量概率模型参数在函数流形上的距离的一种方法，也就是自然梯度方法的基础。\nFIM与KL散度近似 下面我们尝试通过如下路径建立FIM与KL散度的关系：首先我们证明，FIM与似然函数的Hessian的期望的负值相等（性质2），接着我们利用这一性质，进一步得到：FIM是分布$p(x|\\theta)$和$p(x|\\theta')$的KL散度的Hessian在$\\theta'=\\theta$处的取值（性质3）。通过对KL散度做二阶泰勒展开，可以使用FIM局部近似KL散度，衡量参数在概率模型函数流形上的距离，后面我们会用这个近似关系来推导自然梯度。\n性质2：FIM与似然函数的Hessian的期望的负值相等\n$$ \\boldsymbol{F}(\\theta) = -\\mathbb{E}_{p(x|\\theta)}[\\boldsymbol{H}(\\ell(\\theta|x))] $$\nproof\n$$ \\begin{align} \\boldsymbol{H}(\\ell(\\theta|x)) \u0026=\\nabla^2_\\theta\\left(\\log p(x|\\theta)\\right)\\\\ \u0026=\\nabla_\\theta\\left(\\frac{\\nabla_\\theta p(x|\\theta)}{p(x|\\theta)}\\right)\\\\ \u0026=\\frac{p(x|\\theta)\\nabla^2_\\theta p(x|\\theta)-\\nabla_\\theta p(x|\\theta)\\nabla_\\theta p(x|\\theta)^\\top}{p(x|\\theta)^2}\\\\ \u0026=\\frac{\\nabla^2_\\theta p(x|\\theta)}{p(x|\\theta)}-\\frac{\\nabla_\\theta p(x|\\theta)\\nabla_\\theta p(x|\\theta)^\\top}{p(x|\\theta)^2}\\\\ \u0026=\\frac{\\boldsymbol{H}(p(x|\\theta))}{p(x|\\theta)}-\\nabla_\\theta\\log p(x|\\theta)\\nabla_\\theta\\log p(x|\\theta)^\\top\\\\ \u0026=\\frac{\\boldsymbol{H}(p(x|\\theta))}{p(x|\\theta)}-\\nabla_\\theta\\ell(\\theta|x)\\nabla_\\theta\\ell(\\theta|x)^\\top\\\\ \\end{align} $$\n$$ \\begin{align} \\mathbb{E}_{p(x|\\theta)}[\\boldsymbol{H}(\\ell(\\theta|x))] \u0026=\\mathbb{E}_{p(x|\\theta)}\\left[\\frac{\\boldsymbol{H}(p(x|\\theta))}{p(x|\\theta)}\\right]-\\mathbb{E}_{p(x|\\theta)}\\left[\\nabla_\\theta\\ell(\\theta|x)\\nabla_\\theta\\ell(\\theta|x)^\\top\\right]\\\\ \u0026=\\int_x p(x|\\theta)\\frac{\\boldsymbol{H}(p(x|\\theta))}{p(x|\\theta)}dx-\\boldsymbol{F}(\\theta)\\\\ \u0026=\\int_x \\boldsymbol{H}(p(x|\\theta))dx-\\boldsymbol{F}(\\theta)\\\\ \u0026=\\boldsymbol{H}\\left(\\int_xp(x|\\theta)dx\\right)-\\boldsymbol{F}(\\theta)\\\\ \u0026=\\boldsymbol{H}(1)-\\boldsymbol{F}(\\theta)\\\\ \u0026=0-\\boldsymbol{F}(\\theta)\\\\ \u0026=-\\boldsymbol{F}(\\theta)\\\\ \\end{align} $$\n性质3：FIM是分布$p(x|\\theta)$和$p(x|\\theta')$的KL散度的Hessian在$\\theta'=\\theta$处的取值。\n$$ \\boldsymbol{F}(\\theta)=\\nabla_{\\theta'}^2 D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)|_{\\theta'=\\theta} $$\nproof\n首先将KL散度展开： $$ \\begin{align} D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right) \u0026=\\mathbb{E}_{p(x|\\theta)}\\left[ \\log\\left( \\frac{p(x|\\theta)}{p(x|\\theta')} \\right) \\right]\\\\ \u0026=\\mathbb{E}_{p(x|\\theta)}\\left[ \\log\\left(p(x|\\theta) \\right) \\right]-\\mathbb{E}_{p(x|\\theta)}\\left[ \\log\\left(p(x|\\theta') \\right) \\right]\\\\ \\end{align} $$\n接着，求一阶梯度：\n$$ \\begin{align} \\nabla_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right) \u0026=\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla_{\\theta'}\\log\\left(p(x|\\theta) \\right) \\right]-\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla_{\\theta'}\\log\\left(p(x|\\theta') \\right) \\right]\\\\ \u0026=0-\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla_{\\theta'}\\log\\left(p(x|\\theta') \\right) \\right]\\\\ \u0026=-\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla_{\\theta'}\\log\\left(p(x|\\theta') \\right) \\right]\\\\ \\end{align} $$\n继续求导：\n$$ \\begin{align} \\nabla^2_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right) \u0026=-\\mathbb{E}_{p(x|\\theta)}\\left[ \\nabla^2_{\\theta'}\\log\\left(p(x|\\theta') \\right) \\right]\\\\ \u0026=-\\mathbb{E}_{p(x|\\theta)}\\left[ \\boldsymbol{H}(\\ell(\\theta'|x)) \\right]\\\\ \\end{align} $$\n代入$\\theta'=\\theta$： $$ \\begin{align} \\nabla^2_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)|_{\\theta'=\\theta}\u0026=-\\mathbb{E}_{p(x|\\theta)}\\left[ \\boldsymbol{H}(\\ell(\\theta|x)) \\right]\\\\ \u0026=-\\boldsymbol{F}(\\theta) \\quad\\triangleright\\text{性质2} \\end{align} $$\n由于FIM是KL散度的Hessian，可以将其用于KL的近似。\n性质4：FIM可以用于KL散度的局部二阶近似： $$ D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta+\\Delta_\\theta)\\right)\\approx \\frac{1}{2}\\Delta_\\theta^\\top\\boldsymbol{F}(\\theta)\\Delta_\\theta $$\nproof\n我们记$\\theta'=\\theta+\\Delta_\\theta$ $$ \\begin{align} D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)\u0026\\approx D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)|_{\\theta'=\\theta} + \\Delta_\\theta^\\top\\left(\\nabla_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)\\right)|_{\\theta'=\\theta}\\\\ \u0026+\\frac{1}{2}\\Delta_\\theta^\\top\\left(\\nabla^2_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)\\right)|_{\\theta'=\\theta}\\Delta_\\theta\\\\ \u0026=0 + 0 + \\frac{1}{2}\\Delta_\\theta^\\top\\boldsymbol{F}(\\theta)\\Delta_\\theta\\\\ \u0026=\\frac{1}{2}\\Delta_\\theta^\\top\\boldsymbol{F}(\\theta)\\Delta_\\theta\\\\ \\end{align} $$\n其中二阶导到FIM的转化直接利用了性质3；一阶项为0是因为：\n$$ \\begin{align} \\Delta_\\theta^\\top\\left(\\nabla_{\\theta'}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta')\\right)\\right)|_{\\theta'=\\theta} \u0026= \\Delta_\\theta^\\top\\left.\\nabla_{\\theta'}\\mathbb{E}_{p(x|\\theta)}\\left[\\log{\\frac{p(x|\\theta)}{p(x|\\theta')}}\\right]\\right|_{\\theta'=\\theta} \\\\ \u0026= \\Delta_\\theta^\\top\\left.\\mathbb{E}_{p(x|\\theta)}\\left[\\nabla_{\\theta'}{\\log\\frac{p(x|\\theta)}{p(x|\\theta')}}\\right]\\right|_{\\theta'=\\theta}\\\\ \u0026= \\Delta_\\theta^\\top\\left.\\mathbb{E}_{p(x|\\theta)}\\left[\\nabla_{\\theta'}{-\\log p(x|\\theta')}\\right]\\right|_{\\theta'=\\theta}\\\\ \u0026= -\\Delta_\\theta^\\top\\left.\\mathbb{E}_{p(x|\\theta)}\\left[\\nabla_{\\theta'}{\\log p(x|\\theta')}\\right]\\right|_{\\theta'=\\theta} \\\\ \u0026= 0\\quad\\triangleright\\text{性质1} \\end{align} $$\nFIM作为黎曼度量 在建立了FIM可用于KL散度近似的结论之后，我们将其放到一个新的框架里来审视——这种近似关系实际上定义了一种黎曼度量。\n在黎曼几何中，度量张量（metric tensor）提供了一种将坐标值转化为距离（或内积）的工具。假设在流形上的给定点$p$的切空间$T_pM$上定义了一组基向量$\\{e_1,\\cdots,e_n\\}$，黎曼度量可以定义为基向量两两之间的内积： $$ g_{ij} = \\langle \\boldsymbol{e}_i, \\boldsymbol{e}_j\\rangle $$\n对于任意切向量$\\boldsymbol{u},\\boldsymbol{v}\\in T_pM$，可以利用这个黎曼度量来定义切向量的内积：\n$$ \\begin{align} \\langle \\boldsymbol{u}, \\boldsymbol{v}\\rangle _p \u0026= \\langle \\sum_i u_i\\boldsymbol{e}_i, \\sum_j v_j\\boldsymbol{e}_j\\rangle _p\\\\ \u0026=\\sum_{ij}u_i v_j \\langle \\boldsymbol{e}_i, \\boldsymbol{e}_j\\rangle\\\\ \u0026=\\sum_{ij}u_i v_j g_{ij}\\\\ \u0026= \\boldsymbol{u}^\\top \\boldsymbol{G}\\boldsymbol{v} \\end{align} $$\n其中$(\\boldsymbol{G})_{ij}=g_{ij}$，即将度量组织成一个矩阵的形式。基于这个内积可以衍生出$p$点附近的微小距离（线元）的计算，考虑$p$点上的一个微小位移$\\boldsymbol{\\delta}$，则对应的线元为： $$ |\\boldsymbol{\\delta}|^2 = \\boldsymbol{\\delta}^\\top\\boldsymbol{G}\\boldsymbol{\\delta} $$\n作为一个特例，考虑常见的欧氏空间，由于基向量都是规范正交的，任意两个不同基向量的内积是0，且基向量与自身的内积为1，因此对应的度量张量是$\\boldsymbol{G}=\\boldsymbol{I}$，这一点对于任意点都成立。因此欧氏空间对应的内积总是$\\boldsymbol{u}^\\top \\boldsymbol{v}$，线元总是$|\\boldsymbol{\\delta}|^2 = \\boldsymbol{\\delta}^\\top\\boldsymbol{\\delta}$。\n回顾上一节推导得到的性质4（$D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta+\\Delta_\\theta)\\right)\\approx \\frac{1}{2}\\Delta_\\theta^\\top\\boldsymbol{F}(\\theta)\\Delta_\\theta$），通过对比上述的线元的计算，可以发现二者的形式是一致的。\n现在，如果我们将不同参数$\\theta$实例化的分布族$\\mathcal{F}=\\{p_\\theta:\\theta\\in \\Theta\\}$拓展为一个流形$\\mathcal{M}$，使得流形$\\mathcal{M}$上的点与分布族$\\mathcal{F}$中的分布构成双射（一一对应关系）。这样，分布中的参数$\\theta$可以看做流形上的坐标。如果我们将参数的距离定义为参数引导的概率分布的KL散度1，则由性质4的近似，$\\boldsymbol{F}(\\theta)$可以看做点$\\theta$的黎曼度量张量。\n在下篇文章中，我们会通过泛化标准梯度下降的距离约束条件为黎曼距离，从而得到自然梯度方法的更新公式，并给出这种新的约束下对应的一些性质。\n这里的距离是广义而言的，因为KL散度不满足距离的约定。 ↩︎\n","wordCount":"287","inLanguage":"en","image":"https://nil9.net/images/android-chrome-512x512.png","datePublished":"2025-02-05T00:00:41Z","dateModified":"2025-02-05T00:00:41Z","author":{"@type":"Person","name":"Tianyang Lin"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nil9.net/posts/fisher-info-matrix/"},"publisher":{"@type":"Organization","name":"nil9.net","logo":{"@type":"ImageObject","url":"https://nil9.net/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nil9.net/ accesskey=h title="nil9.net (Alt + H)">nil9.net</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nil9.net/archives/ title=archives><span>archives</span></a></li><li><a href=https://nil9.net/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://nil9.net/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nil9.net/>Home</a>&nbsp;»&nbsp;<a href=https://nil9.net/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">自然梯度（一）：Fisher信息矩阵作为黎曼度量</h1><div class=post-meta><span title='2025-02-05 00:00:41 +0000 UTC'>2025-02-05</span>&nbsp;·&nbsp;Tianyang Lin<div class=meta-item>&nbsp·&nbsp
        <svg t="1737096748489" class="icon" style="vertical-align:middle" viewBox="0 0 1024 1024" p-id="2420" width="1em" height="1em"><path d="M512 928c-17.7.0-32-14.3-32-32V192c0-17.7 14.3-32 32-32s32 14.3 32 32v704c0 17.7-14.3 32-32 32z" fill="#243154" p-id="2421"/><path d="M893.2 928H130.8C76.3 928 32 883.7 32 829.2V194.8C32 140.3 76.3 96 130.8 96h247.5c54.8.0 103.5 26.8 133.7 67.9C542.2 122.8 590.9 96 645.7 96h247.5c54.5.0 98.8 44.3 98.8 98.8v634.3c0 54.6-44.3 98.9-98.8 98.9zM130.8 160c-19.2.0-34.8 15.6-34.8 34.8v634.3c0 19.2 15.6 34.8 34.8 34.8h762.3c19.2.0 34.8-15.6 34.8-34.8V194.8c0-19.2-15.6-34.8-34.8-34.8H645.7C589.6 160 544 205.6 544 261.7c0 17.7-14.3 32-32 32s-32-14.3-32-32c0-56.1-45.6-101.7-101.7-101.7H130.8z" fill="var(--secondary)" p-id="2422"/></svg>
&#8201;<span id=vercount_value_page_pv>-</span></div><div class=meta-item>&nbsp·&nbsp
                <a href=https://nil9.net/tags/optimization/>Optimization</a></div></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#score-function%e4%b8%8efim aria-label="Score function与FIM">Score function与FIM</a></li><li><a href=#fim%e4%b8%8ekl%e6%95%a3%e5%ba%a6%e8%bf%91%e4%bc%bc aria-label=FIM与KL散度近似>FIM与KL散度近似</a></li><li><a href=#fim%e4%bd%9c%e4%b8%ba%e9%bb%8e%e6%9b%bc%e5%ba%a6%e9%87%8f aria-label=FIM作为黎曼度量>FIM作为黎曼度量</a></li></ul></div></details></div><div class=post-content><p>在一般的梯度下降中，我们认为目标函数梯度的负方向可以最小化一步更新后的目标函数值，这里隐含地假设了参数空间是欧氏空间，且参数构成了一组正交归一的坐标系统。在很多情况下，这一假设是不成立的，作为结果，优化过程的收敛效率可能受到影响。</p><p>作为解决这一问题的一种思路，自然梯度使用Fisher信息矩阵（的逆）作为梯度的pre-conditioner来矫正梯度的方向。本文将分为两篇，在第一篇中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在第二篇中，我们推导自然梯度中为何引入FIM来修正梯度方向，以及自然梯度的一些性质。</p><h1 id=score-function与fim>Score function与FIM<a hidden class=anchor aria-hidden=true href=#score-function与fim>#</a></h1><p>假设我们有一个由<code>$\theta$</code>参数化的概率模型，模型分布为<code>$p(x|\theta)$</code>，记对数似然函数为<code>$\ell(\theta|x):=\log p(x|\theta)$</code>。与对数似然函数相关的有两个定义，score function和fisher information。</p><p><strong>定义1（score function）</strong>：score function <code>$s(\theta|x)$</code>被定义为对数似然函数关于参数<code>$\theta$</code>的梯度</p><p><code>$$ s(\theta|x)=\nabla_\theta \ell(\theta|x) $$</code></p><p>一些文章会提到score function是用来为参数的好坏打分（score），这是不严谨的。score function中的「score」其实不是为参数打分，而是在Fisher研究的遗传统计问题中给基因异常家庭的「打分」(参见：<a href=https://stats.stackexchange.com/questions/326091/interpretation-of-score>Interpretation of &ldquo;score&rdquo;</a>)。因此，score function只是约定俗成的一种名称，其实质就是似然函数的梯度，描述的是似然函数对于参数变化的敏感程度。</p><p><strong>性质1</strong>：Score function期望为0
<code>$$ \mathbb{E}_{p(x|\theta)}[s(\theta|x)]=\boldsymbol{0} $$</code></p><style type=text/css>.notice{--title-color:#fff;--title-background-color:#6be;--content-color:#444;--content-background-color:#e7f2fa}.notice.proof{--title-background-color:rgb(130, 130, 130);--content-background-color:#f7f7f7}.notice.info{--title-background-color:#fb7;--content-background-color:#fec}.notice.tip{--title-background-color:#5a5;--content-background-color:#efe}.notice.warning{--title-background-color:#c33;--content-background-color:#fee}body.dark .notice{--title-color:#fff;--title-background-color:#069;--content-color:#ddd;--content-background-color:#023}body.dark .notice.proof{--title-background-color:rgb(129, 129, 129);--content-background-color:rgb(41, 41, 41)}body.dark .notice.info{--title-background-color:#a50;--content-background-color:#420}body.dark .notice.tip{--title-background-color:#363;--content-background-color:#121}body.dark .notice.warning{--title-background-color:#800;--content-background-color:#400}.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:var(--content-color);background:var(--content-background-color)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background-color)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative}</style><div class="notice proof"><p class=notice-title><span class="icon-notice baseline"></span>proof</p><p><code>$$ \begin{align} \mathbb{E}_{p(x|\theta)}[s(\theta|x)] &=\mathbb{E}_{p(x|\theta)}[\nabla_\theta \ell(\theta|x)]\\ &=\int_x p(x|\theta)\nabla_\theta \log p(x|\theta) dx\\ &=\int_x p(x|\theta)\frac{\nabla_\theta p(x|\theta)}{p(x|\theta)} dx\\ &=\int_x \nabla_\theta p(x|\theta) dx\\ &=\nabla_\theta\int_x p(x|\theta) dx\\ &=\nabla_\theta 1 = \boldsymbol{0}\\ \end{align} $$</code></p></div><p>通过性质1可以很顺利地引出Fisher信息矩阵的定义，由于score function是「零均值」的，因此协方差可以直接定义为score function的外积的期望。</p><p><strong>定义2（Fisher information matrix）</strong>：Fisher矩阵是score function的方差-协方差矩阵：
<code>$$ \begin{align} \boldsymbol{F}(\theta) &:= \mathbb{E}_{p(x|\theta)}\left[ \left(s(\theta|x)-\mathbb{E}_{p(x|\theta)}[s(\theta|x)]\right)\left(s(\theta|x)-\mathbb{E}_{p(x|\theta)}[s(\theta|x)]\right)^\top \right]\\ &=\mathbb{E}_{p(x|\theta)}\left[ \nabla_\theta \ell(\theta|x)\nabla_\theta \ell(\theta|x)^\top \right]\quad\triangleright\text{性质1} \end{align} $$</code></p><p>Fisher信息的应用有很多，但我们主要关心它在优化方法中的应用。我们下面会证明，Fisher矩阵是分布<code>$p(x|\theta)$</code>和<code>$p(x|\theta')$</code>的KL散度的Hessian在<code>$\theta'=\theta$</code>处的取值。<strong>这点结论之所以重要，是因为这提供了衡量概率模型参数在函数流形上的距离的一种方法，也就是自然梯度方法的基础</strong>。</p><h1 id=fim与kl散度近似>FIM与KL散度近似<a hidden class=anchor aria-hidden=true href=#fim与kl散度近似>#</a></h1><p>下面我们尝试通过如下路径建立FIM与KL散度的关系：首先我们证明，FIM与似然函数的Hessian的期望的负值相等（性质2），接着我们利用这一性质，进一步得到：FIM是分布<code>$p(x|\theta)$</code>和<code>$p(x|\theta')$</code>的KL散度的Hessian在<code>$\theta'=\theta$</code>处的取值（性质3）。通过对KL散度做二阶泰勒展开，可以使用FIM局部近似KL散度，衡量参数在概率模型函数流形上的距离，后面我们会用这个近似关系来推导自然梯度。</p><p><strong>性质2</strong>：FIM与似然函数的Hessian的期望的负值相等</p><p><code>$$ \boldsymbol{F}(\theta) = -\mathbb{E}_{p(x|\theta)}[\boldsymbol{H}(\ell(\theta|x))] $$</code></p><div class="notice proof"><p class=notice-title><span class="icon-notice baseline"></span>proof</p><p><code>$$ \begin{align} \boldsymbol{H}(\ell(\theta|x)) &=\nabla^2_\theta\left(\log p(x|\theta)\right)\\ &=\nabla_\theta\left(\frac{\nabla_\theta p(x|\theta)}{p(x|\theta)}\right)\\ &=\frac{p(x|\theta)\nabla^2_\theta p(x|\theta)-\nabla_\theta p(x|\theta)\nabla_\theta p(x|\theta)^\top}{p(x|\theta)^2}\\ &=\frac{\nabla^2_\theta p(x|\theta)}{p(x|\theta)}-\frac{\nabla_\theta p(x|\theta)\nabla_\theta p(x|\theta)^\top}{p(x|\theta)^2}\\ &=\frac{\boldsymbol{H}(p(x|\theta))}{p(x|\theta)}-\nabla_\theta\log p(x|\theta)\nabla_\theta\log p(x|\theta)^\top\\ &=\frac{\boldsymbol{H}(p(x|\theta))}{p(x|\theta)}-\nabla_\theta\ell(\theta|x)\nabla_\theta\ell(\theta|x)^\top\\ \end{align} $$</code></p><p><code>$$ \begin{align} \mathbb{E}_{p(x|\theta)}[\boldsymbol{H}(\ell(\theta|x))] &=\mathbb{E}_{p(x|\theta)}\left[\frac{\boldsymbol{H}(p(x|\theta))}{p(x|\theta)}\right]-\mathbb{E}_{p(x|\theta)}\left[\nabla_\theta\ell(\theta|x)\nabla_\theta\ell(\theta|x)^\top\right]\\ &=\int_x p(x|\theta)\frac{\boldsymbol{H}(p(x|\theta))}{p(x|\theta)}dx-\boldsymbol{F}(\theta)\\ &=\int_x \boldsymbol{H}(p(x|\theta))dx-\boldsymbol{F}(\theta)\\ &=\boldsymbol{H}\left(\int_xp(x|\theta)dx\right)-\boldsymbol{F}(\theta)\\ &=\boldsymbol{H}(1)-\boldsymbol{F}(\theta)\\ &=0-\boldsymbol{F}(\theta)\\ &=-\boldsymbol{F}(\theta)\\ \end{align} $$</code></p></div><p><strong>性质3</strong>：FIM是分布<code>$p(x|\theta)$</code>和<code>$p(x|\theta')$</code>的KL散度的Hessian在<code>$\theta'=\theta$</code>处的取值。</p><p><code>$$ \boldsymbol{F}(\theta)=\nabla_{\theta'}^2 D_{\text{KL}}\left(p(x|\theta)\|p(x|\theta')\right)|_{\theta'=\theta} $$</code></p><div class="notice proof"><p class=notice-title><span class="icon-notice baseline"></span>proof</p><p>首先将KL散度展开：
<code>$$ \begin{align} D_{\text{KL}}\left(p(x|\theta)\|p(x|\theta')\right) &=\mathbb{E}_{p(x|\theta)}\left[ \log\left( \frac{p(x|\theta)}{p(x|\theta')} \right) \right]\\ &=\mathbb{E}_{p(x|\theta)}\left[ \log\left(p(x|\theta) \right) \right]-\mathbb{E}_{p(x|\theta)}\left[ \log\left(p(x|\theta') \right) \right]\\ \end{align} $$</code></p><p>接着，求一阶梯度：</p><p><code>$$ \begin{align} \nabla_{\theta'}D_{\text{KL}}\left(p(x|\theta)\|p(x|\theta')\right) &=\mathbb{E}_{p(x|\theta)}\left[ \nabla_{\theta'}\log\left(p(x|\theta) \right) \right]-\mathbb{E}_{p(x|\theta)}\left[ \nabla_{\theta'}\log\left(p(x|\theta') \right) \right]\\ &=0-\mathbb{E}_{p(x|\theta)}\left[ \nabla_{\theta'}\log\left(p(x|\theta') \right) \right]\\ &=-\mathbb{E}_{p(x|\theta)}\left[ \nabla_{\theta'}\log\left(p(x|\theta') \right) \right]\\ \end{align} $$</code></p><p>继续求导：</p><p><code>$$ \begin{align} \nabla^2_{\theta'}D_{\text{KL}}\left(p(x|\theta)\|p(x|\theta')\right) &=-\mathbb{E}_{p(x|\theta)}\left[ \nabla^2_{\theta'}\log\left(p(x|\theta') \right) \right]\\ &=-\mathbb{E}_{p(x|\theta)}\left[ \boldsymbol{H}(\ell(\theta'|x)) \right]\\ \end{align} $$</code></p><p>代入<code>$\theta'=\theta$</code>：
<code>$$ \begin{align} \nabla^2_{\theta'}D_{\text{KL}}\left(p(x|\theta)\|p(x|\theta')\right)|_{\theta'=\theta}&=-\mathbb{E}_{p(x|\theta)}\left[ \boldsymbol{H}(\ell(\theta|x)) \right]\\ &=-\boldsymbol{F}(\theta) \quad\triangleright\text{性质2} \end{align} $$</code></p></div><p>由于FIM是KL散度的Hessian，可以将其用于KL的近似。</p><p><strong>性质4</strong>：FIM可以用于KL散度的局部二阶近似：
<code>$$ D_{\text{KL}}\left(p(x|\theta)\|p(x|\theta+\Delta_\theta)\right)\approx \frac{1}{2}\Delta_\theta^\top\boldsymbol{F}(\theta)\Delta_\theta $$</code></p><div class="notice proof"><p class=notice-title><span class="icon-notice baseline"></span>proof</p><p>我们记<code>$\theta'=\theta+\Delta_\theta$</code>
<code>$$ \begin{align} D_{\text{KL}}\left(p(x|\theta)\|p(x|\theta')\right)&\approx D_{\text{KL}}\left(p(x|\theta)\|p(x|\theta')\right)|_{\theta'=\theta} + \Delta_\theta^\top\left(\nabla_{\theta'}D_{\text{KL}}\left(p(x|\theta)\|p(x|\theta')\right)\right)|_{\theta'=\theta}\\ &+\frac{1}{2}\Delta_\theta^\top\left(\nabla^2_{\theta'}D_{\text{KL}}\left(p(x|\theta)\|p(x|\theta')\right)\right)|_{\theta'=\theta}\Delta_\theta\\ &=0 + 0 + \frac{1}{2}\Delta_\theta^\top\boldsymbol{F}(\theta)\Delta_\theta\\ &=\frac{1}{2}\Delta_\theta^\top\boldsymbol{F}(\theta)\Delta_\theta\\ \end{align} $$</code></p><p>其中二阶导到FIM的转化直接利用了性质3；一阶项为0是因为：</p><p><code>$$ \begin{align} \Delta_\theta^\top\left(\nabla_{\theta'}D_{\text{KL}}\left(p(x|\theta)\|p(x|\theta')\right)\right)|_{\theta'=\theta} &= \Delta_\theta^\top\left.\nabla_{\theta'}\mathbb{E}_{p(x|\theta)}\left[\log{\frac{p(x|\theta)}{p(x|\theta')}}\right]\right|_{\theta'=\theta} \\ &= \Delta_\theta^\top\left.\mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta'}{\log\frac{p(x|\theta)}{p(x|\theta')}}\right]\right|_{\theta'=\theta}\\ &= \Delta_\theta^\top\left.\mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta'}{-\log p(x|\theta')}\right]\right|_{\theta'=\theta}\\ &= -\Delta_\theta^\top\left.\mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta'}{\log p(x|\theta')}\right]\right|_{\theta'=\theta} \\ &= 0\quad\triangleright\text{性质1} \end{align} $$</code></p></div><h1 id=fim作为黎曼度量>FIM作为黎曼度量<a hidden class=anchor aria-hidden=true href=#fim作为黎曼度量>#</a></h1><p>在建立了FIM可用于KL散度近似的结论之后，我们将其放到一个新的框架里来审视——这种近似关系实际上定义了一种黎曼度量。</p><p>在黎曼几何中，度量张量（metric tensor）提供了一种将坐标值转化为距离（或内积）的工具。假设在流形上的给定点<code>$p$</code>的切空间<code>$T_pM$</code>上定义了一组基向量<code>$\{e_1,\cdots,e_n\}$</code>，黎曼度量可以定义为基向量两两之间的内积：
<code>$$ g_{ij} = \langle \boldsymbol{e}_i, \boldsymbol{e}_j\rangle $$</code></p><p>对于任意切向量<code>$\boldsymbol{u},\boldsymbol{v}\in T_pM$</code>，可以利用这个黎曼度量来定义切向量的内积：</p><p><code>$$ \begin{align} \langle \boldsymbol{u}, \boldsymbol{v}\rangle _p &= \langle \sum_i u_i\boldsymbol{e}_i, \sum_j v_j\boldsymbol{e}_j\rangle _p\\ &=\sum_{ij}u_i v_j \langle \boldsymbol{e}_i, \boldsymbol{e}_j\rangle\\ &=\sum_{ij}u_i v_j g_{ij}\\ &= \boldsymbol{u}^\top \boldsymbol{G}\boldsymbol{v} \end{align} $$</code></p><p>其中<code>$(\boldsymbol{G})_{ij}=g_{ij}$</code>，即将度量组织成一个矩阵的形式。基于这个内积可以衍生出<code>$p$</code>点附近的微小距离（线元）的计算，考虑<code>$p$</code>点上的一个微小位移<code>$\boldsymbol{\delta}$</code>，则对应的线元为：
<code>$$ |\boldsymbol{\delta}|^2 = \boldsymbol{\delta}^\top\boldsymbol{G}\boldsymbol{\delta} $$</code></p><p>作为一个特例，考虑常见的欧氏空间，由于基向量都是规范正交的，任意两个不同基向量的内积是0，且基向量与自身的内积为1，因此对应的度量张量是<code>$\boldsymbol{G}=\boldsymbol{I}$</code>，这一点对于任意点都成立。因此欧氏空间对应的内积总是<code>$\boldsymbol{u}^\top \boldsymbol{v}$</code>，线元总是<code>$|\boldsymbol{\delta}|^2 = \boldsymbol{\delta}^\top\boldsymbol{\delta}$</code>。</p><p>回顾上一节推导得到的性质4（<code>$D_{\text{KL}}\left(p(x|\theta)\|p(x|\theta+\Delta_\theta)\right)\approx \frac{1}{2}\Delta_\theta^\top\boldsymbol{F}(\theta)\Delta_\theta$</code>），通过对比上述的线元的计算，可以发现二者的形式是一致的。</p><p>现在，如果我们将不同参数<code>$\theta$</code>实例化的分布族<code>$\mathcal{F}=\{p_\theta:\theta\in \Theta\}$</code>拓展为一个流形<code>$\mathcal{M}$</code>，使得流形<code>$\mathcal{M}$</code>上的点与分布族<code>$\mathcal{F}$</code>中的分布构成双射（一一对应关系）。这样，<strong>分布中的参数<code>$\theta$</code>可以看做流形上的坐标</strong>。如果我们将参数的距离定义为参数引导的概率分布的KL散度<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>，则由性质4的近似，<code>$\boldsymbol{F}(\theta)$</code>可以看做点<code>$\theta$</code>的黎曼度量张量。</p><p>在<a href=https://nil9.net/posts/natural-gradient-descent/>下篇文章</a>中，我们会通过泛化标准梯度下降的距离约束条件为黎曼距离，从而得到自然梯度方法的更新公式，并给出这种新的约束下对应的一些性质。</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>这里的距离是广义而言的，因为KL散度不满足距离的约定。&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://nil9.net/tags/optimization/>Optimization</a></li></ul><nav class=paginav><a class=prev href=https://nil9.net/posts/natural-gradient-descent/><span class=title>« Prev</span><br><span>自然梯度（二）：黎曼距离下的最速下降</span>
</a><a class=next href=https://nil9.net/posts/tied-embeddings-in-lm/><span class=title>Next »</span><br><span>关于语言建模中的Tied Embeddings的一点探讨</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 自然梯度（一）：Fisher信息矩阵作为黎曼度量 on x" href="https://x.com/intent/tweet/?text=%e8%87%aa%e7%84%b6%e6%a2%af%e5%ba%a6%ef%bc%88%e4%b8%80%ef%bc%89%ef%bc%9aFisher%e4%bf%a1%e6%81%af%e7%9f%a9%e9%98%b5%e4%bd%9c%e4%b8%ba%e9%bb%8e%e6%9b%bc%e5%ba%a6%e9%87%8f&amp;url=https%3a%2f%2fnil9.net%2fposts%2ffisher-info-matrix%2f&amp;hashtags=Optimization"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 自然梯度（一）：Fisher信息矩阵作为黎曼度量 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fnil9.net%2fposts%2ffisher-info-matrix%2f&amp;title=%e8%87%aa%e7%84%b6%e6%a2%af%e5%ba%a6%ef%bc%88%e4%b8%80%ef%bc%89%ef%bc%9aFisher%e4%bf%a1%e6%81%af%e7%9f%a9%e9%98%b5%e4%bd%9c%e4%b8%ba%e9%bb%8e%e6%9b%bc%e5%ba%a6%e9%87%8f&amp;summary=%e8%87%aa%e7%84%b6%e6%a2%af%e5%ba%a6%ef%bc%88%e4%b8%80%ef%bc%89%ef%bc%9aFisher%e4%bf%a1%e6%81%af%e7%9f%a9%e9%98%b5%e4%bd%9c%e4%b8%ba%e9%bb%8e%e6%9b%bc%e5%ba%a6%e9%87%8f&amp;source=https%3a%2f%2fnil9.net%2fposts%2ffisher-info-matrix%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 自然梯度（一）：Fisher信息矩阵作为黎曼度量 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnil9.net%2fposts%2ffisher-info-matrix%2f&title=%e8%87%aa%e7%84%b6%e6%a2%af%e5%ba%a6%ef%bc%88%e4%b8%80%ef%bc%89%ef%bc%9aFisher%e4%bf%a1%e6%81%af%e7%9f%a9%e9%98%b5%e4%bd%9c%e4%b8%ba%e9%bb%8e%e6%9b%bc%e5%ba%a6%e9%87%8f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 自然梯度（一）：Fisher信息矩阵作为黎曼度量 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnil9.net%2fposts%2ffisher-info-matrix%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 自然梯度（一）：Fisher信息矩阵作为黎曼度量 on whatsapp" href="https://api.whatsapp.com/send?text=%e8%87%aa%e7%84%b6%e6%a2%af%e5%ba%a6%ef%bc%88%e4%b8%80%ef%bc%89%ef%bc%9aFisher%e4%bf%a1%e6%81%af%e7%9f%a9%e9%98%b5%e4%bd%9c%e4%b8%ba%e9%bb%8e%e6%9b%bc%e5%ba%a6%e9%87%8f%20-%20https%3a%2f%2fnil9.net%2fposts%2ffisher-info-matrix%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 自然梯度（一）：Fisher信息矩阵作为黎曼度量 on telegram" href="https://telegram.me/share/url?text=%e8%87%aa%e7%84%b6%e6%a2%af%e5%ba%a6%ef%bc%88%e4%b8%80%ef%bc%89%ef%bc%9aFisher%e4%bf%a1%e6%81%af%e7%9f%a9%e9%98%b5%e4%bd%9c%e4%b8%ba%e9%bb%8e%e6%9b%bc%e5%ba%a6%e9%87%8f&amp;url=https%3a%2f%2fnil9.net%2fposts%2ffisher-info-matrix%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://nil9.net/>nil9.net</a></span> ·
All rights reserved ·</footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>[...document.getElementsByTagName("code")].forEach(e=>{if(e.parentNode.tagName==="PRE"||e.childElementCount>0||e.classList.contains("nolatex"))return;let t=e.textContent;/^\$[^$]/.test(t)&&/[^$]\$$/.test(t)&&(t=t.replace(/^\$/,"\\(").replace(/\$$/,"\\)"),e.textContent=t),(/^\\\((.|\s)+\\\)$/.test(t)||/^\\\[(.|\s)+\\\]$/.test(t)||/^\$(.|\s)+\$$/.test(t)||/^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(t))&&(e.outerHTML=e.innerHTML)})</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("td .highlight-marker");e.forEach(e=>{const t=e.parentElement;e.remove(),t.classList.add("highlight")})})</script><script>hljs.highlightAll()</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>