<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>自然梯度（二）：黎曼距离下的最速下降 | nil9.net</title>
<meta name=keywords content="Optimization"><meta name=description content="上篇文章中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在本篇文章中，我们利用上篇得到的结论，推导自然梯度中为何引入FIM来修正梯度方向，并介绍自然梯度的一些性质。
梯度下降：欧氏距离下的最速下降
考虑一个最优化任务（$f:\Theta\to\mathbb{R}$）：
$$ \underset{\theta}{\operatorname{min}} f(\theta) $$
最常见的一阶优化方法是梯度下降/steepest descent：
$$ \theta^+=\theta-\eta\nabla_\theta f $$
其中$\eta$是学习率。这里的「steepest」指的是在约束欧氏距离定义下的步长在极小范围内时，选取梯度的负方向能最大化一步之内目标函数下降的程度。
$$ \lim_{\epsilon\to 0}\frac{1}{\epsilon}\left(\underset{\delta:\|\delta\|\le \epsilon}{\operatorname{argmin}}  f(\theta+\delta)\right)=-\frac{\nabla_\theta f}{\|\nabla_\theta f\|}\tag{1} $$

    
        
            
        proof"><meta name=author content="Tianyang Lin"><link rel=canonical href=https://nil9.net/posts/natural-gradient-descent/><link crossorigin=anonymous href=/assets/css/stylesheet.71aa4c4f8e8b273084883bf486ff4976c30116bece69acc9b3d1b1e97b7d13ee.css integrity="sha256-capMT46LJzCEiDv0hv9JdsMBFr7OaazJs9Gx6Xt9E+4=" rel="preload stylesheet" as=style><link rel=icon href=https://nil9.net/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nil9.net/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nil9.net/favicon-32x32.png><link rel=apple-touch-icon href=https://nil9.net/apple-touch-icon.png><link rel=mask-icon href=https://nil9.net/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=manifest href=https://nil9.net/site.webmanifest><link rel=alternate hreflang=en href=https://nil9.net/posts/natural-gradient-descent/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script src=//cdn.jsdelivr.net/npm/@xiee/utils/js/math-code.min.js defer></script><script defer src=https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js></script><script>function adjustMathJaxFontSize(){document.querySelectorAll(".mjx-svg, .mjx-chtml").forEach(function(e){const t=e.parentElement;let n=t.offsetWidth;t.classList.contains("isplay")&&(n=t.parentElement.offsetWidth);const s=e.offsetWidth;if(s>n){const t=n/s*100;e.style.fontSize=`${t}%`}})}MathJax.startup.promise.then(()=>{adjustMathJaxFontSize()}),window.addEventListener("resize",()=>{adjustMathJaxFontSize()})</script><script defer src=https://vercount.one/js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js></script><meta property="og:url" content="https://nil9.net/posts/natural-gradient-descent/"><meta property="og:site_name" content="nil9.net"><meta property="og:title" content="自然梯度（二）：黎曼距离下的最速下降"><meta property="og:description" content="上篇文章中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在本篇文章中，我们利用上篇得到的结论，推导自然梯度中为何引入FIM来修正梯度方向，并介绍自然梯度的一些性质。
梯度下降：欧氏距离下的最速下降 考虑一个最优化任务（$f:\Theta\to\mathbb{R}$）： $$ \underset{\theta}{\operatorname{min}} f(\theta) $$
最常见的一阶优化方法是梯度下降/steepest descent： $$ \theta^+=\theta-\eta\nabla_\theta f $$
其中$\eta$是学习率。这里的「steepest」指的是在约束欧氏距离定义下的步长在极小范围内时，选取梯度的负方向能最大化一步之内目标函数下降的程度。 $$ \lim_{\epsilon\to 0}\frac{1}{\epsilon}\left(\underset{\delta:\|\delta\|\le \epsilon}{\operatorname{argmin}} f(\theta+\delta)\right)=-\frac{\nabla_\theta f}{\|\nabla_\theta f\|}\tag{1} $$
proof"><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-06T12:00:41+00:00"><meta property="article:modified_time" content="2025-02-06T12:00:41+00:00"><meta property="article:tag" content="Optimization"><meta property="og:image" content="https://nil9.net/images/android-chrome-512x512.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://nil9.net/images/android-chrome-512x512.png"><meta name=twitter:title content="自然梯度（二）：黎曼距离下的最速下降"><meta name=twitter:description content="上篇文章中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在本篇文章中，我们利用上篇得到的结论，推导自然梯度中为何引入FIM来修正梯度方向，并介绍自然梯度的一些性质。
梯度下降：欧氏距离下的最速下降
考虑一个最优化任务（$f:\Theta\to\mathbb{R}$）：
$$ \underset{\theta}{\operatorname{min}} f(\theta) $$
最常见的一阶优化方法是梯度下降/steepest descent：
$$ \theta^+=\theta-\eta\nabla_\theta f $$
其中$\eta$是学习率。这里的「steepest」指的是在约束欧氏距离定义下的步长在极小范围内时，选取梯度的负方向能最大化一步之内目标函数下降的程度。
$$ \lim_{\epsilon\to 0}\frac{1}{\epsilon}\left(\underset{\delta:\|\delta\|\le \epsilon}{\operatorname{argmin}}  f(\theta+\delta)\right)=-\frac{\nabla_\theta f}{\|\nabla_\theta f\|}\tag{1} $$

    
        
            
        proof"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nil9.net/posts/"},{"@type":"ListItem","position":2,"name":"自然梯度（二）：黎曼距离下的最速下降","item":"https://nil9.net/posts/natural-gradient-descent/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"自然梯度（二）：黎曼距离下的最速下降","name":"自然梯度（二）：黎曼距离下的最速下降","description":"上篇文章中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在本篇文章中，我们利用上篇得到的结论，推导自然梯度中为何引入FIM来修正梯度方向，并介绍自然梯度的一些性质。\n梯度下降：欧氏距离下的最速下降 考虑一个最优化任务（$f:\\Theta\\to\\mathbb{R}$）： $$ \\underset{\\theta}{\\operatorname{min}} f(\\theta) $$\n最常见的一阶优化方法是梯度下降/steepest descent： $$ \\theta^+=\\theta-\\eta\\nabla_\\theta f $$\n其中$\\eta$是学习率。这里的「steepest」指的是在约束欧氏距离定义下的步长在极小范围内时，选取梯度的负方向能最大化一步之内目标函数下降的程度。 $$ \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta:\\|\\delta\\|\\le \\epsilon}{\\operatorname{argmin}} f(\\theta+\\delta)\\right)=-\\frac{\\nabla_\\theta f}{\\|\\nabla_\\theta f\\|}\\tag{1} $$\nproof\n","keywords":["Optimization"],"articleBody":"上篇文章中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在本篇文章中，我们利用上篇得到的结论，推导自然梯度中为何引入FIM来修正梯度方向，并介绍自然梯度的一些性质。\n梯度下降：欧氏距离下的最速下降 考虑一个最优化任务（$f:\\Theta\\to\\mathbb{R}$）： $$ \\underset{\\theta}{\\operatorname{min}} f(\\theta) $$\n最常见的一阶优化方法是梯度下降/steepest descent： $$ \\theta^+=\\theta-\\eta\\nabla_\\theta f $$\n其中$\\eta$是学习率。这里的「steepest」指的是在约束欧氏距离定义下的步长在极小范围内时，选取梯度的负方向能最大化一步之内目标函数下降的程度。 $$ \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta:\\|\\delta\\|\\le \\epsilon}{\\operatorname{argmin}} f(\\theta+\\delta)\\right)=-\\frac{\\nabla_\\theta f}{\\|\\nabla_\\theta f\\|}\\tag{1} $$\nproof\n对极限内的目标函数做一阶泰勒展开： $$ \\begin{align} \\underset{\\delta:\\|\\delta\\|\\le \\epsilon}{\\operatorname{argmin}} f(\\theta+\\delta) \u0026\\approx \\underset{\\delta:\\|\\delta\\|\\le \\epsilon}{\\operatorname{argmin}} f(\\theta) + \\nabla_\\theta f^\\top\\delta\\\\ \u0026=\\underset{\\delta:\\|\\delta\\|\\le \\epsilon}{\\operatorname{argmin}} \\nabla_\\theta f^\\top\\delta \\end{align} $$\n我们将约束条件稍加改写： $$ \\underset{\\delta}{\\min} \\nabla_\\theta f^\\top\\delta\\quad\\text{s.t. }\\|\\delta\\|^2\\le \\epsilon^2 $$\n定义拉格朗日函数 $$ \\mathcal{L}(\\delta, \\lambda):= \\nabla_\\theta f^\\top\\delta + \\lambda (\\|\\delta\\|^2-\\epsilon^2) $$\n根据KKT条件： $$ \\begin{align} \u0026\\nabla_\\delta\\mathcal{L}(\\delta, \\lambda) = 0 \u0026\\triangleright\\text{Stationarity}\\\\ \u0026\\lambda(\\|\\delta\\|^2-\\epsilon^2)=0 \u0026\\triangleright\\text{Complementary slackness}\\\\ \u0026\\|\\delta\\|^2-\\epsilon^2 \\le0\u0026\\triangleright\\text{Primal feasibility}\\\\ \u0026\\lambda\\ge 0\u0026\\triangleright\\text{Dual feasibility}\\\\ \\end{align} $$\n根据驻点条件得到 $$ \\begin{align} \u0026\\nabla_\\theta f+2\\lambda\\delta=0\\\\ \u0026\\delta = -\\frac{1}{2\\lambda}\\nabla_\\theta f\\\\ \\end{align} $$\n代入互补松弛条件（这里$\\lambda=0$可以排除，因为会造成$\\delta$为unbounded）： $$ \\begin{align} \\lambda\u0026\\left(\\left\\|-\\frac{1}{2\\lambda}\\nabla_\\theta f\\right\\|^2-\\epsilon^2\\right)=0\\\\ \u0026\\lambda = \\frac{1}{2\\epsilon}\\|\\nabla_\\theta f\\|\\\\ \\end{align} $$\n带回驻点条件： $$ \\delta^* = -\\epsilon \\frac{\\nabla_\\theta f}{\\|\\nabla_\\theta f\\|} $$\n将$\\delta^*$代入原极限表达式：\n$$ \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta:\\|\\delta\\|\\le \\epsilon}{\\operatorname{argmin}} f(\\theta+\\delta)\\right)=-\\frac{\\nabla_\\theta f}{\\|\\nabla_\\theta f\\|} $$\n注意到在上述的「steepest」的求解中，对步长的约束条件$\\|\\delta\\|\\le\\epsilon$基于欧几里得距离，这里隐含了如下假设：（1）参数空间是标准欧几里得空间；（2）参数构成一组正交归一（orthonormal）的坐标系统。当这个假设不能很好满足的时候，梯度下降的最速性质可能会大打折扣。\n为了说明这个问题，我们考虑一个二维的二次型函数$f(\\boldsymbol{x})=\\boldsymbol{x}^\\top\\boldsymbol{Ax}$，我们令 $$ \\boldsymbol{A} = \\left[\\begin{matrix}1 \u0026 0.5\\\\0.5 \u0026 2\\end{matrix}\\right] $$\n这个函数的等高线图是一个典型的椭圆型，在这个例子中，参数空间不是标准欧氏空间，而是被$\\boldsymbol{A}$定义的椭球几何所支配。如下图所示，使用标准的梯度下降时，由于梯度方向并不指向最低点，因此优化路径是一条曲线，或者（当学习率过大时）呈Z字形。图片右侧是使用自然梯度的优化路径，后面我们会推导自然梯度的形式。\n使用梯度下降与自然梯度在一个「椭球型」二次型函数上的优化路径\n自然梯度：黎曼距离下的最速下降 在揭示了梯度下降的可能问题之后，我们将公式$(1)$中的约束做如下的泛化（差异处我们用蓝色做了区分）：\n$$ \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} f(\\theta+\\delta)\\right)=?\\tag{2} $$\n这里我们引入了局部的度量张量$G(\\theta)$，上篇文章已经简要介绍过黎曼度量和其定义的线元（局部微小变化的长度）\n$$ |\\delta|^2 = \\delta^\\top G(\\theta) \\delta $$\n对$(2)$式做推导，得到的结果就是自然梯度的方向. $$ \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} f(\\theta+\\delta)\\right)=-CG(\\theta)^{-1}\\nabla_\\theta f \\tag{3} $$\n其中$C$是某个常数，可以被吸收到学习率中。这里的证明框架与梯度下降是基本一致的，只不过对约束条件做了一定修改（高亮为蓝色）：\nproof\n对极限内的目标函数做一阶泰勒展开： $$ \\begin{align} \\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} f(\\theta+\\delta) \u0026\\approx \\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} f(\\theta) + \\nabla_\\theta f^\\top\\delta\\\\ \u0026=\\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} \\nabla_\\theta f^\\top\\delta \\end{align} $$\n定义拉格朗日函数 $$ \\mathcal{L}(\\delta, \\lambda):= \\nabla_\\theta f^\\top\\delta + \\lambda ({\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta} -\\epsilon^2) $$\n根据KKT条件： $$ \\begin{align} \u0026\\nabla_\\delta\\mathcal{L}(\\delta, \\lambda) = 0 \u0026\\triangleright\\text{Stationarity}\\\\ \u0026\\lambda ({\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta} -\\epsilon^2)=0 \u0026\\triangleright\\text{Complementary slackness}\\\\ \u0026{\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta} -\\epsilon^2 \\le0\u0026\\triangleright\\text{Primal feasibility}\\\\ \u0026\\lambda\\ge 0\u0026\\triangleright\\text{Dual feasibility}\\\\ \\end{align} $$\n根据驻点条件得到 $$ \\begin{align} \u0026\\nabla_\\theta f+2\\lambda {\\color[rgb]{0, 0.5, 0.8}G(\\theta)}\\delta=0\\\\ \u0026\\delta = -\\frac{1}{2\\lambda} {\\color[rgb]{0, 0.5, 0.8}G(\\theta)^{-1}}\\nabla_\\theta f\\\\ \\end{align} $$\n代入互补松弛条件（$\\lambda=0$可以排除，因为会造成$\\delta$为unbounded）： $$ \\begin{align} {\\color[rgb]{0, 0.5, 0.8}\\left(-\\frac{1}{2\\lambda} G(\\theta)^{-1}\\nabla_\\theta f\\right)^\\top }\u0026{\\color[rgb]{0, 0.5, 0.8}G(\\theta)\\left(-\\frac{1}{2\\lambda} G(\\theta)^{-1}\\nabla_\\theta f\\right)}-\\epsilon^2=0\\\\ \\lambda \u0026= \\frac{1}{2\\epsilon}{\\color[rgb]{0, 0.5, 0.8}\\sqrt{ \\nabla_\\theta f^\\top G(\\theta)^{-1}\\nabla_\\theta f }}\\\\ \\end{align} $$\n带回驻点条件： $$ \\delta^* = -\\epsilon\\frac{{\\color[rgb]{0, 0.5, 0.8}G(\\theta)^{-1}}\\nabla_\\theta f}{{\\color[rgb]{0, 0.5, 0.8}\\sqrt{ \\nabla_\\theta f^\\top G(\\theta)^{-1}\\nabla_\\theta f }}} $$\n将$\\delta^*$代入原极限表达式：\n$$ \\begin{align} \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top G(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} f(\\theta+\\delta)\\right)\u0026=-\\frac{{\\color[rgb]{0, 0.5, 0.8}G(\\theta)^{-1}}\\nabla_\\theta f}{{\\color[rgb]{0, 0.5, 0.8}\\sqrt{ \\nabla_\\theta f^\\top G(\\theta)^{-1}\\nabla_\\theta f }}}\\\\ \u0026=-C{\\color[rgb]{0, 0.5, 0.8}G(\\theta)^{-1}}\\nabla_\\theta f\\\\ \\end{align} $$\n在上述结论中，我们实质上是对标准的梯度下降方向应用了度量张量的逆$G(\\theta)^{-1}$，从而修正了梯度的方向（可以将这个矩阵叫做pre-conditioner）。\n在机器学习中，我们常关注的是概率模型的最大似然优化问题，在自然梯度（一）：Fisher信息矩阵作为黎曼度量中，我们已经建立了Fisher信息矩阵是给定概率分布族的参数空间的黎曼度量张量这一结论。如果我们需要优化的函数是一个概率似然函数$\\ell(\\theta):=\\log p(x|\\theta)$，则自然梯度可以直接由Fisher信息矩阵作为pre-conditioner $$ \\begin{align} \\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}D_{\\text{KL}}\\left(p(x|\\theta)\\|p(x|\\theta+\\delta)\\right) \\le \\epsilon^2}}{\\operatorname{argmin}} \\ell(\\theta+\\delta)\\right)\u0026\\approx\\lim_{\\epsilon\\to 0}\\frac{1}{\\epsilon}\\left(\\underset{\\delta: {\\color[rgb]{0, 0.5, 0.8}\\delta^\\top F(\\theta) \\delta \\le \\epsilon^2}}{\\operatorname{argmin}} \\ell(\\theta+\\delta)\\right)\\\\ \u0026=-CF(\\theta)^{-1}\\nabla_\\theta \\ell \\end{align} $$\n对应的参数更新公式为\n$$ \\theta^+=\\theta-\\eta F(\\theta)^{-1}\\nabla_\\theta \\ell(\\theta|x) $$\n拓展到判别模型 在常见的监督学习设定下，我们学习的是一个判别模型，优化目标是一系列条件概率的联合对数似然函数，其中每个输入$\\boldsymbol{x}^{(i)}$对应一个条件概率分布 $$ \\ell(\\theta)=-\\frac{1}{n} \\sum_i^n \\left[\\log p_\\theta(y^{(i)}|\\boldsymbol{x}^{(i)})\\right] $$\n相应地，约束条件需要更改为在每个条件概率的KL散度的期望 $$ \\mathbb{E}_{x\\sim\\tilde{q}(x)}\\left[D_{\\text{KL}}\\left(p(y|x;\\theta)\\|p(y|x;\\theta+\\delta)\\right)\\right] \\le \\epsilon^2 $$\n这里的$\\tilde{q}(x)$是输入数据的真实分布或替代分布（与真实分布接近）。这个约束条件对应的Fisher信息矩阵的形式为\n$$ F(\\theta)=\\mathbb{E}_{x\\sim\\tilde{q}(x)}\\left[ \\mathbb{E}_{y\\sim p(y|x;\\theta)}\\left[ \\nabla_\\theta \\log p(y|x;\\theta)\\nabla_\\theta \\log p(y|x;\\theta)^\\top \\right] \\right]\\tag{4} $$\n自然梯度的特性 与二阶优化的联系与区别\n自然梯度的一般形式中，使用$G(\\theta)^{-1}$作为梯度的pre-conditioner。如果把自然梯度看做一个一般的框架（而不仅仅考虑概率模型），那么当优化目标满足一定条件（e.g.,凸函数）时，二阶优化可以看做是选取Hessian作为自然梯度的度量张量。\n对于常见的概率模型框架（优化对数似然损失），我们选取FIM作为度量张量，可以带来与二阶优化类似的性质，例如，在函数流形的局部曲率比较小的时候（plateau），自然梯度会将更新步长拉得比较大，从而可能有助于快速离开plateau。不过也需要注意，这里的曲率定义在模型的函数流形上，而不是最终的损失函数定义的函数流形上。\nFIM相比Hessian具有一些比较好的特性。一方面，FIM是一个协方差矩阵，它总是半正定的，而Hessian则不然（非正定矩阵的逆是不稳定的）。另一方面，我们观察$(4)$中定义的FIM，注意到内层的期望是定义在模型分布$p(y|x;\\theta)$上的，也就是说，估计一个FIM只需要输入分布和模型分布，而不需要知道标签的真实分布，这在mini-batch特别小（e.g., online learning）的时候非常方便——我们可以在一个无标注的数据集上估计FIM，然后将得到的统计量与一个有标注的batch数据计算得到的梯度结合更新模型参数。\n另外，在特定条件下，自然梯度可以等价于广义-高斯牛顿方法，而后者一般被认为是一个二阶优化方法，可以参考Martens 2020.。\n模型KL约束\n使用FIM的自然梯度通过约束模型在一步更新前后的KL散度得到的「最优」方向，这种约束与模型的参数化方式无关——无论什么样的模型，一步更新的结果都是恒定的KL散度变化约束。在模型的分布距离约束下，优化过程中每一步更新后，模型的分布都不会有非常剧烈的变化，这构成了一种「平滑」的效应，Pascanu \u0026 Bengio 2014.认为这一定程度上可以防止过拟合。\n应用限制：复杂度考虑\n到目前为止，自然梯度仍然没有在深度学习中得到广泛应用。自然梯度需要计算FIM，对于包含$M$个参数的模型而言，FIM的空间复杂度为$\\mathcal{O}(M^2)$，对于现在的神经网络而言，这是一个不小的负担——一阶优化方法只需要$\\mathcal{O}(M)$的优化器状态。另外，对一个大矩阵求逆也需要比较大的计算复杂度。将自然梯度推广到大模型中需要引入FIM的结构假设（e.g., 分块对角）。\n总结 在自然梯度的两篇文章中，我们从Fisher信息矩阵（FIM）的定义出发，将FIM与概率模型的参数空间的黎曼度量建立联系。在此基础上，我们推导了自然梯度中为何引入FIM来修正梯度方向，并讨论了自然梯度的特性、与二阶优化的联系与区别、以及应用的限制。\n参考阅读 Amari 1998. Natural Gradient Works Efficiently in Learning Amari. Information Geometry of Neural Networks Pascanu \u0026 Bengio 2014. Revisiting Natural Gradient for Deep Networks Martens 2020. New Insights and Perspectives on the Natural Gradient Method ","wordCount":"454","inLanguage":"en","image":"https://nil9.net/images/android-chrome-512x512.png","datePublished":"2025-02-06T12:00:41Z","dateModified":"2025-02-06T12:00:41Z","author":{"@type":"Person","name":"Tianyang Lin"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nil9.net/posts/natural-gradient-descent/"},"publisher":{"@type":"Organization","name":"nil9.net","logo":{"@type":"ImageObject","url":"https://nil9.net/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nil9.net/ accesskey=h title="nil9.net (Alt + H)">nil9.net</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nil9.net/archives/ title=archives><span>archives</span></a></li><li><a href=https://nil9.net/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://nil9.net/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nil9.net/>Home</a>&nbsp;»&nbsp;<a href=https://nil9.net/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">自然梯度（二）：黎曼距离下的最速下降</h1><div class=post-meta><span title='2025-02-06 12:00:41 +0000 UTC'>2025-02-06</span>&nbsp;·&nbsp;Tianyang Lin<div class=meta-item>&nbsp·&nbsp
        <svg t="1737096748489" class="icon" style="vertical-align:middle" viewBox="0 0 1024 1024" p-id="2420" width="1em" height="1em"><path d="M512 928c-17.7.0-32-14.3-32-32V192c0-17.7 14.3-32 32-32s32 14.3 32 32v704c0 17.7-14.3 32-32 32z" fill="#243154" p-id="2421"/><path d="M893.2 928H130.8C76.3 928 32 883.7 32 829.2V194.8C32 140.3 76.3 96 130.8 96h247.5c54.8.0 103.5 26.8 133.7 67.9C542.2 122.8 590.9 96 645.7 96h247.5c54.5.0 98.8 44.3 98.8 98.8v634.3c0 54.6-44.3 98.9-98.8 98.9zM130.8 160c-19.2.0-34.8 15.6-34.8 34.8v634.3c0 19.2 15.6 34.8 34.8 34.8h762.3c19.2.0 34.8-15.6 34.8-34.8V194.8c0-19.2-15.6-34.8-34.8-34.8H645.7C589.6 160 544 205.6 544 261.7c0 17.7-14.3 32-32 32s-32-14.3-32-32c0-56.1-45.6-101.7-101.7-101.7H130.8z" fill="var(--secondary)" p-id="2422"/></svg>
&#8201;<span id=vercount_value_page_pv>-</span></div><div class=meta-item>&nbsp·&nbsp
                <a href=https://nil9.net/tags/optimization/>Optimization</a></div></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%ac%a7%e6%b0%8f%e8%b7%9d%e7%a6%bb%e4%b8%8b%e7%9a%84%e6%9c%80%e9%80%9f%e4%b8%8b%e9%99%8d aria-label=梯度下降：欧氏距离下的最速下降>梯度下降：欧氏距离下的最速下降</a></li><li><a href=#%e8%87%aa%e7%84%b6%e6%a2%af%e5%ba%a6%e9%bb%8e%e6%9b%bc%e8%b7%9d%e7%a6%bb%e4%b8%8b%e7%9a%84%e6%9c%80%e9%80%9f%e4%b8%8b%e9%99%8d aria-label=自然梯度：黎曼距离下的最速下降>自然梯度：黎曼距离下的最速下降</a></li><li><a href=#%e6%8b%93%e5%b1%95%e5%88%b0%e5%88%a4%e5%88%ab%e6%a8%a1%e5%9e%8b aria-label=拓展到判别模型>拓展到判别模型</a></li><li><a href=#%e8%87%aa%e7%84%b6%e6%a2%af%e5%ba%a6%e7%9a%84%e7%89%b9%e6%80%a7 aria-label=自然梯度的特性>自然梯度的特性</a></li><li><a href=#%e6%80%bb%e7%bb%93 aria-label=总结>总结</a></li><li><a href=#%e5%8f%82%e8%80%83%e9%98%85%e8%af%bb aria-label=参考阅读>参考阅读</a></li></ul></div></details></div><div class=post-content><p><a href=https://nil9.net/posts/fisher-info-matrix/>上篇文章</a>中，我们从Fisher信息矩阵（FIM）的定义出发，推导出Fisher矩阵与KL散度的关系，并建立如下结论：FIM可以作为概率模型的参数空间的一种黎曼度量。在本篇文章中，我们利用上篇得到的结论，推导自然梯度中为何引入FIM来修正梯度方向，并介绍自然梯度的一些性质。</p><h1 id=梯度下降欧氏距离下的最速下降>梯度下降：欧氏距离下的最速下降<a hidden class=anchor aria-hidden=true href=#梯度下降欧氏距离下的最速下降>#</a></h1><p>考虑一个最优化任务（<code>$f:\Theta\to\mathbb{R}$</code>）：
<code>$$ \underset{\theta}{\operatorname{min}} f(\theta) $$</code></p><p>最常见的一阶优化方法是梯度下降/steepest descent：
<code>$$ \theta^+=\theta-\eta\nabla_\theta f $$</code></p><p>其中<code>$\eta$</code>是学习率。这里的「steepest」指的是在约束欧氏距离定义下的步长在极小范围内时，选取梯度的负方向能最大化一步之内目标函数下降的程度。
<code>$$ \lim_{\epsilon\to 0}\frac{1}{\epsilon}\left(\underset{\delta:\|\delta\|\le \epsilon}{\operatorname{argmin}} f(\theta+\delta)\right)=-\frac{\nabla_\theta f}{\|\nabla_\theta f\|}\tag{1} $$</code></p><style type=text/css>.notice{--title-color:#fff;--title-background-color:#6be;--content-color:#444;--content-background-color:#e7f2fa}.notice.proof{--title-background-color:rgb(130, 130, 130);--content-background-color:#f7f7f7}.notice.info{--title-background-color:#fb7;--content-background-color:#fec}.notice.tip{--title-background-color:#5a5;--content-background-color:#efe}.notice.warning{--title-background-color:#c33;--content-background-color:#fee}body.dark .notice{--title-color:#fff;--title-background-color:#069;--content-color:#ddd;--content-background-color:#023}body.dark .notice.proof{--title-background-color:rgb(129, 129, 129);--content-background-color:rgb(41, 41, 41)}body.dark .notice.info{--title-background-color:#a50;--content-background-color:#420}body.dark .notice.tip{--title-background-color:#363;--content-background-color:#121}body.dark .notice.warning{--title-background-color:#800;--content-background-color:#400}.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:var(--content-color);background:var(--content-background-color)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background-color)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative}</style><div class="notice proof"><p class=notice-title><span class="icon-notice baseline"></span>proof</p><p>对极限内的目标函数做一阶泰勒展开：
<code>$$ \begin{align} \underset{\delta:\|\delta\|\le \epsilon}{\operatorname{argmin}} f(\theta+\delta) &\approx \underset{\delta:\|\delta\|\le \epsilon}{\operatorname{argmin}} f(\theta) + \nabla_\theta f^\top\delta\\ &=\underset{\delta:\|\delta\|\le \epsilon}{\operatorname{argmin}} \nabla_\theta f^\top\delta \end{align} $$</code></p><p>我们将约束条件稍加改写：
<code>$$ \underset{\delta}{\min} \nabla_\theta f^\top\delta\quad\text{s.t. }\|\delta\|^2\le \epsilon^2 $$</code></p><p>定义拉格朗日函数
<code>$$ \mathcal{L}(\delta, \lambda):= \nabla_\theta f^\top\delta + \lambda (\|\delta\|^2-\epsilon^2) $$</code></p><p>根据KKT条件：
<code>$$ \begin{align} &\nabla_\delta\mathcal{L}(\delta, \lambda) = 0 &\triangleright\text{Stationarity}\\ &\lambda(\|\delta\|^2-\epsilon^2)=0 &\triangleright\text{Complementary slackness}\\ &\|\delta\|^2-\epsilon^2 \le0&\triangleright\text{Primal feasibility}\\ &\lambda\ge 0&\triangleright\text{Dual feasibility}\\ \end{align} $$</code></p><p>根据驻点条件得到
<code>$$ \begin{align} &\nabla_\theta f+2\lambda\delta=0\\ &\delta = -\frac{1}{2\lambda}\nabla_\theta f\\ \end{align} $$</code></p><p>代入互补松弛条件（这里<code>$\lambda=0$</code>可以排除，因为会造成<code>$\delta$</code>为unbounded）：
<code>$$ \begin{align} \lambda&\left(\left\|-\frac{1}{2\lambda}\nabla_\theta f\right\|^2-\epsilon^2\right)=0\\ &\lambda = \frac{1}{2\epsilon}\|\nabla_\theta f\|\\ \end{align} $$</code></p><p>带回驻点条件：
<code>$$ \delta^* = -\epsilon \frac{\nabla_\theta f}{\|\nabla_\theta f\|} $$</code></p><p>将<code>$\delta^*$</code>代入原极限表达式：</p><p><code>$$ \lim_{\epsilon\to 0}\frac{1}{\epsilon}\left(\underset{\delta:\|\delta\|\le \epsilon}{\operatorname{argmin}} f(\theta+\delta)\right)=-\frac{\nabla_\theta f}{\|\nabla_\theta f\|} $$</code></p></div><p>注意到在上述的「steepest」的求解中，对步长的约束条件<code>$\|\delta\|\le\epsilon$</code>基于欧几里得距离，这里隐含了如下假设：（1）参数空间是标准欧几里得空间；（2）参数构成一组正交归一（orthonormal）的坐标系统。当这个假设不能很好满足的时候，梯度下降的最速性质可能会大打折扣。</p><p>为了说明这个问题，我们考虑一个二维的二次型函数<code>$f(\boldsymbol{x})=\boldsymbol{x}^\top\boldsymbol{Ax}$</code>，我们令
<code>$$ \boldsymbol{A} = \left[\begin{matrix}1 & 0.5\\0.5 & 2\end{matrix}\right] $$</code></p><p>这个函数的等高线图是一个典型的椭圆型，在这个例子中，参数空间不是标准欧氏空间，而是被<code>$\boldsymbol{A}$</code>定义的椭球几何所支配。如下图所示，使用标准的梯度下降时，由于梯度方向并不指向最低点，因此优化路径是一条曲线，或者（当学习率过大时）呈Z字形。图片右侧是使用自然梯度的优化路径，后面我们会推导自然梯度的形式。</p><p><figure><img loading=lazy src=https://nil9.net/images/fisher-ngd/ngd-simple-exmpl.png title=使用梯度下降与自然梯度在一个「椭球型」二次型函数上的优化路径><figcaption class=image-caption>使用梯度下降与自然梯度在一个「椭球型」二次型函数上的优化路径</figcaption></figure></p><h1 id=自然梯度黎曼距离下的最速下降>自然梯度：黎曼距离下的最速下降<a hidden class=anchor aria-hidden=true href=#自然梯度黎曼距离下的最速下降>#</a></h1><p>在揭示了梯度下降的可能问题之后，我们将公式<code>$(1)$</code>中的约束做如下的泛化（差异处我们用蓝色做了区分）：</p><p><code>$$ \lim_{\epsilon\to 0}\frac{1}{\epsilon}\left(\underset{\delta: {\color[rgb]{0, 0.5, 0.8}\delta^\top G(\theta) \delta \le \epsilon^2}}{\operatorname{argmin}} f(\theta+\delta)\right)=?\tag{2} $$</code></p><p>这里我们引入了局部的度量张量<code>$G(\theta)$</code>，<a href=https://nil9.net/posts/fisher-info-matrix/>上篇文章</a>已经简要介绍过黎曼度量和其定义的线元（局部微小变化的长度）</p><p><code>$$ |\delta|^2 = \delta^\top G(\theta) \delta $$</code></p><p>对<code>$(2)$</code>式做推导，得到的结果就是自然梯度的方向.
<code>$$ \lim_{\epsilon\to 0}\frac{1}{\epsilon}\left(\underset{\delta: {\color[rgb]{0, 0.5, 0.8}\delta^\top G(\theta) \delta \le \epsilon^2}}{\operatorname{argmin}} f(\theta+\delta)\right)=-CG(\theta)^{-1}\nabla_\theta f \tag{3} $$</code></p><p>其中<code>$C$</code>是某个常数，可以被吸收到学习率中。这里的证明框架与梯度下降是基本一致的，只不过对约束条件做了一定修改（高亮为蓝色）：</p><div class="notice proof"><p class=notice-title><span class="icon-notice baseline"></span>proof</p><p>对极限内的目标函数做一阶泰勒展开：
<code>$$ \begin{align} \underset{\delta: {\color[rgb]{0, 0.5, 0.8}\delta^\top G(\theta) \delta \le \epsilon^2}}{\operatorname{argmin}} f(\theta+\delta) &\approx \underset{\delta: {\color[rgb]{0, 0.5, 0.8}\delta^\top G(\theta) \delta \le \epsilon^2}}{\operatorname{argmin}} f(\theta) + \nabla_\theta f^\top\delta\\ &=\underset{\delta: {\color[rgb]{0, 0.5, 0.8}\delta^\top G(\theta) \delta \le \epsilon^2}}{\operatorname{argmin}} \nabla_\theta f^\top\delta \end{align} $$</code></p><p>定义拉格朗日函数
<code>$$ \mathcal{L}(\delta, \lambda):= \nabla_\theta f^\top\delta + \lambda ({\color[rgb]{0, 0.5, 0.8}\delta^\top G(\theta) \delta} -\epsilon^2) $$</code></p><p>根据KKT条件：
<code>$$ \begin{align} &\nabla_\delta\mathcal{L}(\delta, \lambda) = 0 &\triangleright\text{Stationarity}\\ &\lambda ({\color[rgb]{0, 0.5, 0.8}\delta^\top G(\theta) \delta} -\epsilon^2)=0 &\triangleright\text{Complementary slackness}\\ &{\color[rgb]{0, 0.5, 0.8}\delta^\top G(\theta) \delta} -\epsilon^2 \le0&\triangleright\text{Primal feasibility}\\ &\lambda\ge 0&\triangleright\text{Dual feasibility}\\ \end{align} $$</code></p><p>根据驻点条件得到
<code>$$ \begin{align} &\nabla_\theta f+2\lambda {\color[rgb]{0, 0.5, 0.8}G(\theta)}\delta=0\\ &\delta = -\frac{1}{2\lambda} {\color[rgb]{0, 0.5, 0.8}G(\theta)^{-1}}\nabla_\theta f\\ \end{align} $$</code></p><p>代入互补松弛条件（<code>$\lambda=0$</code>可以排除，因为会造成<code>$\delta$</code>为unbounded）：
<code>$$ \begin{align} {\color[rgb]{0, 0.5, 0.8}\left(-\frac{1}{2\lambda} G(\theta)^{-1}\nabla_\theta f\right)^\top }&{\color[rgb]{0, 0.5, 0.8}G(\theta)\left(-\frac{1}{2\lambda} G(\theta)^{-1}\nabla_\theta f\right)}-\epsilon^2=0\\ \lambda &= \frac{1}{2\epsilon}{\color[rgb]{0, 0.5, 0.8}\sqrt{ \nabla_\theta f^\top G(\theta)^{-1}\nabla_\theta f }}\\ \end{align} $$</code></p><p>带回驻点条件：
<code>$$ \delta^* = -\epsilon\frac{{\color[rgb]{0, 0.5, 0.8}G(\theta)^{-1}}\nabla_\theta f}{{\color[rgb]{0, 0.5, 0.8}\sqrt{ \nabla_\theta f^\top G(\theta)^{-1}\nabla_\theta f }}} $$</code></p><p>将<code>$\delta^*$</code>代入原极限表达式：</p><p><code>$$ \begin{align} \lim_{\epsilon\to 0}\frac{1}{\epsilon}\left(\underset{\delta: {\color[rgb]{0, 0.5, 0.8}\delta^\top G(\theta) \delta \le \epsilon^2}}{\operatorname{argmin}} f(\theta+\delta)\right)&=-\frac{{\color[rgb]{0, 0.5, 0.8}G(\theta)^{-1}}\nabla_\theta f}{{\color[rgb]{0, 0.5, 0.8}\sqrt{ \nabla_\theta f^\top G(\theta)^{-1}\nabla_\theta f }}}\\ &=-C{\color[rgb]{0, 0.5, 0.8}G(\theta)^{-1}}\nabla_\theta f\\ \end{align} $$</code></p></div><p>在上述结论中，我们实质上是对标准的梯度下降方向应用了度量张量的逆<code>$G(\theta)^{-1}$</code>，从而修正了梯度的方向（可以将这个矩阵叫做pre-conditioner）。</p><p>在机器学习中，我们常关注的是概率模型的最大似然优化问题，在<a href=https://nil9.net/posts/fisher-info-matrix/>自然梯度（一）：Fisher信息矩阵作为黎曼度量</a>中，我们已经建立了Fisher信息矩阵是给定概率分布族的参数空间的黎曼度量张量这一结论。如果我们需要优化的函数是一个概率似然函数<code>$\ell(\theta):=\log p(x|\theta)$</code>，则自然梯度可以直接由Fisher信息矩阵作为pre-conditioner
<code>$$ \begin{align} \lim_{\epsilon\to 0}\frac{1}{\epsilon}\left(\underset{\delta: {\color[rgb]{0, 0.5, 0.8}D_{\text{KL}}\left(p(x|\theta)\|p(x|\theta+\delta)\right) \le \epsilon^2}}{\operatorname{argmin}} \ell(\theta+\delta)\right)&\approx\lim_{\epsilon\to 0}\frac{1}{\epsilon}\left(\underset{\delta: {\color[rgb]{0, 0.5, 0.8}\delta^\top F(\theta) \delta \le \epsilon^2}}{\operatorname{argmin}} \ell(\theta+\delta)\right)\\ &=-CF(\theta)^{-1}\nabla_\theta \ell \end{align} $$</code></p><p>对应的参数更新公式为</p><p><code>$$ \theta^+=\theta-\eta F(\theta)^{-1}\nabla_\theta \ell(\theta|x) $$</code></p><h1 id=拓展到判别模型>拓展到判别模型<a hidden class=anchor aria-hidden=true href=#拓展到判别模型>#</a></h1><p>在常见的监督学习设定下，我们学习的是一个判别模型，优化目标是一系列条件概率的联合对数似然函数，其中每个输入<code>$\boldsymbol{x}^{(i)}$</code>对应一个条件概率分布
<code>$$ \ell(\theta)=-\frac{1}{n} \sum_i^n \left[\log p_\theta(y^{(i)}|\boldsymbol{x}^{(i)})\right] $$</code></p><p>相应地，约束条件需要更改为在每个条件概率的KL散度的期望
<code>$$ \mathbb{E}_{x\sim\tilde{q}(x)}\left[D_{\text{KL}}\left(p(y|x;\theta)\|p(y|x;\theta+\delta)\right)\right] \le \epsilon^2 $$</code></p><p>这里的<code>$\tilde{q}(x)$</code>是输入数据的真实分布或替代分布（与真实分布接近）。这个约束条件对应的Fisher信息矩阵的形式为</p><p><code>$$ F(\theta)=\mathbb{E}_{x\sim\tilde{q}(x)}\left[ \mathbb{E}_{y\sim p(y|x;\theta)}\left[ \nabla_\theta \log p(y|x;\theta)\nabla_\theta \log p(y|x;\theta)^\top \right] \right]\tag{4} $$</code></p><h1 id=自然梯度的特性>自然梯度的特性<a hidden class=anchor aria-hidden=true href=#自然梯度的特性>#</a></h1><p><strong>与二阶优化的联系与区别</strong></p><p>自然梯度的一般形式中，使用<code>$G(\theta)^{-1}$</code>作为梯度的pre-conditioner。如果把自然梯度看做一个一般的框架（而不仅仅考虑概率模型），那么当优化目标满足一定条件（e.g.,凸函数）时，二阶优化可以看做是选取Hessian作为自然梯度的度量张量。</p><p>对于常见的概率模型框架（优化对数似然损失），我们选取FIM作为度量张量，可以带来与二阶优化类似的性质，例如，在函数流形的局部曲率比较小的时候（plateau），自然梯度会将更新步长拉得比较大，从而可能有助于快速离开plateau。不过也需要注意，这里的曲率定义在模型的函数流形上，而不是最终的损失函数定义的函数流形上。</p><p>FIM相比Hessian具有一些比较好的特性。一方面，FIM是一个协方差矩阵，它总是半正定的，而Hessian则不然（非正定矩阵的逆是不稳定的）。另一方面，我们观察<code>$(4)$</code>中定义的FIM，注意到内层的期望是定义在模型分布<code>$p(y|x;\theta)$</code>上的，也就是说，估计一个FIM只需要输入分布和模型分布，而不需要知道标签的真实分布，这在mini-batch特别小（e.g., online learning）的时候非常方便——我们可以在一个无标注的数据集上估计FIM，然后将得到的统计量与一个有标注的batch数据计算得到的梯度结合更新模型参数。</p><p>另外，在特定条件下，自然梯度可以等价于广义-高斯牛顿方法，而后者一般被认为是一个二阶优化方法，可以参考<a href=https://arxiv.org/pdf/1412.1193>Martens 2020.</a>。</p><p><strong>模型KL约束</strong></p><p>使用FIM的自然梯度通过约束模型在一步更新前后的KL散度得到的「最优」方向，这种约束与模型的参数化方式无关——无论什么样的模型，一步更新的结果都是恒定的KL散度变化约束。在模型的分布距离约束下，优化过程中每一步更新后，模型的分布都不会有非常剧烈的变化，这构成了一种「平滑」的效应，<a href=https://arxiv.org/pdf/1301.3584>Pascanu & Bengio 2014.</a>认为这一定程度上可以防止过拟合。</p><p><strong>应用限制：复杂度考虑</strong></p><p>到目前为止，自然梯度仍然没有在深度学习中得到广泛应用。自然梯度需要计算FIM，对于包含<code>$M$</code>个参数的模型而言，FIM的空间复杂度为<code>$\mathcal{O}(M^2)$</code>，对于现在的神经网络而言，这是一个不小的负担——一阶优化方法只需要<code>$\mathcal{O}(M)$</code>的优化器状态。另外，对一个大矩阵求逆也需要比较大的计算复杂度。将自然梯度推广到大模型中需要引入FIM的结构假设（e.g., 分块对角）。</p><h1 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h1><p>在自然梯度的两篇文章中，我们从Fisher信息矩阵（FIM）的定义出发，将FIM与概率模型的参数空间的黎曼度量建立联系。在此基础上，我们推导了自然梯度中为何引入FIM来修正梯度方向，并讨论了自然梯度的特性、与二阶优化的联系与区别、以及应用的限制。</p><h1 id=参考阅读>参考阅读<a hidden class=anchor aria-hidden=true href=#参考阅读>#</a></h1><ul><li><a href=https://ieeexplore.ieee.org/document/6790500/>Amari 1998. Natural Gradient Works Efficiently in Learning</a></li><li><a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=829d357b1b8b71e352be14de85b01aef75cfdb22">Amari. Information Geometry of Neural Networks</a></li><li><a href=https://arxiv.org/abs/1301.3584>Pascanu & Bengio 2014. Revisiting Natural Gradient for Deep Networks</a></li><li><a href=https://arxiv.org/pdf/1412.1193>Martens 2020. New Insights and Perspectives on the Natural Gradient Method</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://nil9.net/tags/optimization/>Optimization</a></li></ul><nav class=paginav><a class=prev href=https://nil9.net/posts/tensor-product-attention/><span class=title>« Prev</span><br><span>Tensor Product Attention (TPA) 导读</span>
</a><a class=next href=https://nil9.net/posts/fisher-info-matrix/><span class=title>Next »</span><br><span>自然梯度（一）：Fisher信息矩阵作为黎曼度量</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 自然梯度（二）：黎曼距离下的最速下降 on x" href="https://x.com/intent/tweet/?text=%e8%87%aa%e7%84%b6%e6%a2%af%e5%ba%a6%ef%bc%88%e4%ba%8c%ef%bc%89%ef%bc%9a%e9%bb%8e%e6%9b%bc%e8%b7%9d%e7%a6%bb%e4%b8%8b%e7%9a%84%e6%9c%80%e9%80%9f%e4%b8%8b%e9%99%8d&amp;url=https%3a%2f%2fnil9.net%2fposts%2fnatural-gradient-descent%2f&amp;hashtags=Optimization"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 自然梯度（二）：黎曼距离下的最速下降 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fnil9.net%2fposts%2fnatural-gradient-descent%2f&amp;title=%e8%87%aa%e7%84%b6%e6%a2%af%e5%ba%a6%ef%bc%88%e4%ba%8c%ef%bc%89%ef%bc%9a%e9%bb%8e%e6%9b%bc%e8%b7%9d%e7%a6%bb%e4%b8%8b%e7%9a%84%e6%9c%80%e9%80%9f%e4%b8%8b%e9%99%8d&amp;summary=%e8%87%aa%e7%84%b6%e6%a2%af%e5%ba%a6%ef%bc%88%e4%ba%8c%ef%bc%89%ef%bc%9a%e9%bb%8e%e6%9b%bc%e8%b7%9d%e7%a6%bb%e4%b8%8b%e7%9a%84%e6%9c%80%e9%80%9f%e4%b8%8b%e9%99%8d&amp;source=https%3a%2f%2fnil9.net%2fposts%2fnatural-gradient-descent%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 自然梯度（二）：黎曼距离下的最速下降 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnil9.net%2fposts%2fnatural-gradient-descent%2f&title=%e8%87%aa%e7%84%b6%e6%a2%af%e5%ba%a6%ef%bc%88%e4%ba%8c%ef%bc%89%ef%bc%9a%e9%bb%8e%e6%9b%bc%e8%b7%9d%e7%a6%bb%e4%b8%8b%e7%9a%84%e6%9c%80%e9%80%9f%e4%b8%8b%e9%99%8d"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 自然梯度（二）：黎曼距离下的最速下降 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnil9.net%2fposts%2fnatural-gradient-descent%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 自然梯度（二）：黎曼距离下的最速下降 on whatsapp" href="https://api.whatsapp.com/send?text=%e8%87%aa%e7%84%b6%e6%a2%af%e5%ba%a6%ef%bc%88%e4%ba%8c%ef%bc%89%ef%bc%9a%e9%bb%8e%e6%9b%bc%e8%b7%9d%e7%a6%bb%e4%b8%8b%e7%9a%84%e6%9c%80%e9%80%9f%e4%b8%8b%e9%99%8d%20-%20https%3a%2f%2fnil9.net%2fposts%2fnatural-gradient-descent%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 自然梯度（二）：黎曼距离下的最速下降 on telegram" href="https://telegram.me/share/url?text=%e8%87%aa%e7%84%b6%e6%a2%af%e5%ba%a6%ef%bc%88%e4%ba%8c%ef%bc%89%ef%bc%9a%e9%bb%8e%e6%9b%bc%e8%b7%9d%e7%a6%bb%e4%b8%8b%e7%9a%84%e6%9c%80%e9%80%9f%e4%b8%8b%e9%99%8d&amp;url=https%3a%2f%2fnil9.net%2fposts%2fnatural-gradient-descent%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://nil9.net/>nil9.net</a></span> ·
All rights reserved ·</footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>[...document.getElementsByTagName("code")].forEach(e=>{if(e.parentNode.tagName==="PRE"||e.childElementCount>0||e.classList.contains("nolatex"))return;let t=e.textContent;/^\$[^$]/.test(t)&&/[^$]\$$/.test(t)&&(t=t.replace(/^\$/,"\\(").replace(/\$$/,"\\)"),e.textContent=t),(/^\\\((.|\s)+\\\)$/.test(t)||/^\\\[(.|\s)+\\\]$/.test(t)||/^\$(.|\s)+\$$/.test(t)||/^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(t))&&(e.outerHTML=e.innerHTML)})</script><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll("td .highlight-marker");e.forEach(e=>{const t=e.parentElement;e.remove(),t.classList.add("highlight")})})</script><script>hljs.highlightAll()</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>